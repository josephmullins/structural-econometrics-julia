[
  {
    "objectID": "lectures/extremum_intro_examples.html",
    "href": "lectures/extremum_intro_examples.html",
    "title": "12  Introducing the Estimators with Examples",
    "section": "",
    "text": "12.1 The Generalized Roy Model\nBefore diving into statistical theory, we will introduce the estimators by proposing estimation methods for each of our prototype models.\nThe three workhorse methods are:\nEach of these approaches is an extremum estimator: any estimator that can be characterized as the solution to a maximization or minimization problem.\nJust to clarify where we are going, it helps to reiterate what the key properties are that we would like to establish for each approach.\nTo discuss estimation of this model, let’s assume a linear form for the selection and outcomes equations:\n\\[ D = \\mathbf{1}\\{\\gamma_0 + \\gamma_1X + \\gamma_2Z - V \\geq0\\} \\] \\[ Y_{D} = \\beta_{D,0} + \\beta_{D,1}X + U_D \\] with \\(V\\sim\\mathcal{N}(0,1)\\).\nOur identification argument suggested a two-step estimator for the Generalized Roy Model, which we implemented in Example 7.1:\nNote that this is a two-step estimator. The second stage relies on parameters estimated in the first stage. We will need to develop theory for this!",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing the Estimators with Examples</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_intro_examples.html#the-generalized-roy-model",
    "href": "lectures/extremum_intro_examples.html#the-generalized-roy-model",
    "title": "12  Introducing the Estimators with Examples",
    "section": "",
    "text": "Estimate the selection equation by maximum likelihood: \\[ \\hat{\\gamma} = \\arg\\max_{\\gamma}\\frac{1}{N}\\sum_{n=1}^{N}D_{n}\\log(\\Phi(\\mathbf{w}_{n}\\gamma)) + (1-D_n)\\log(1-\\Phi(\\mathbf{w}_{n}\\gamma)) \\] where \\(\\mathbf{w}_{n} = [1,\\ X_{n},\\ Z_{n}]\\).\nEstimate the outcome equations with OLS using a selection correction.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing the Estimators with Examples</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_intro_examples.html#the-search-model",
    "href": "lectures/extremum_intro_examples.html#the-search-model",
    "title": "12  Introducing the Estimators with Examples",
    "section": "12.2 The Search Model",
    "text": "12.2 The Search Model\nFor this example, let’s assume that we observe wages with some small amount of known measurement error:\n\\[ \\log(W^{o}_{n,t}) = \\log(W_{n,t}) + \\zeta_{n,t}\\]\nwhere \\(\\zeta_{n,t} \\sim \\mathcal{N}(0,\\sigma^2_\\zeta)\\) and \\(\\sigma_\\zeta = 0.05\\).\nRecall that without further variation, we must make a parametric assumption on the wage distribution, and so we assume that \\(W\\) is log-normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2_{W}\\).\nOur strategy here is to estimate the parameters\n\\[ \\theta = (\\mu,\\sigma^2_{W},h,\\delta,w^*) \\]\nand invert out \\(\\lambda\\) and \\(b\\) (the latter using the reservation wage equation). Let \\(X_{n} = (W^o,t_u,E)\\) indicate the data. The log-likelihood of a single observation is:\n\\[ l(X;\\theta) = E \\times \\int f_{W|W&gt;w^*}(\\log(W^{o})-\\zeta)\\phi(\\zeta;\\sigma_\\zeta)d\\zeta + (1-E)\\times[\\log(h) + t_u\\log(1-h)]  \\]\nwhere, according to our parametric specifications:\n\\[ f_{W|W&gt;w^*}(w) = \\frac{\\phi(w;\\sigma_{W})}{1-\\Phi(w^*/\\sigma_{W})}.\\]\n\\(\\phi(\\cdot;\\sigma)\\) is the pdf of a normal with standard deviation \\(\\sigma\\) and \\(\\Phi\\) is the cdf of a standard normal.\nThe maximum likelihood estimator is:\n\\[ \\hat{\\theta} = \\arg\\max_\\theta \\frac{1}{N}\\sum_{n}l(X_n;\\theta) \\] while the MLE estimates of \\(\\lambda\\) and \\(b\\) are:\n\\[ \\hat{\\lambda} = \\hat{h} / (1 - \\widehat{F}_{W|W&gt;w^*}(\\hat{w}^*) \\]\n\\[ \\hat{b} = w^* - \\frac{\\hat{\\lambda}}{1 - \\beta(1-\\hat{\\delta})}\\int_{\\hat{w}^*}(1-\\widehat{F}_{W|W&gt;w^*}(w))dw \\]\nWhen we get to the theory we will consider the asymptotic properties of not just \\(\\hat{\\theta}\\) but also the derived estimates \\(\\hat{b}\\) and \\(\\hat{\\lambda}\\).\n\n\n\n\n\n\nExample: Coding the Log-Likelihood\n\n\n\n\nExample 12.1 First, let’s load the routines that we previously wrote to solve the model and take numerical integrals with quadrature. These are identical to what we’ve seen before and are available on the course github.\n\ninclude(\"../scripts/search_model.jl\")\n\nsolve_res_wage (generic function with 1 method)\n\n\nBefore writing the likelihood, let’s load the data, clean, and create the data frame.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\ndata = @chain data begin\n    @transform :E = :EMPSTAT.&lt;21\n    @transform @byrow :wage = begin\n        if :PAIDHOUR==0\n            return missing\n        elseif :PAIDHOUR==2\n            if :HOURWAGE&lt;99.99 && :HOURWAGE&gt;0\n                return :HOURWAGE\n            else\n                return missing\n            end\n        elseif :PAIDHOUR==1\n            if :EARNWEEK&gt;0 && :UHRSWORKT&lt;997 && :UHRSWORKT&gt;0\n                return :EARNWEEK / :UHRSWORKT\n            else\n                return missing\n            end\n        end\n    end\n    @subset :MONTH.==1\n    @select :AGE :SEX :RACE :EDUC :wage :E :DURUNEMP\n    @transform begin\n        :bachelors = :EDUC.&gt;=111\n        :nonwhite = :RACE.!=100 \n        :female = :SEX.==2\n        :DURUNEMP = round.(:DURUNEMP .* 12/52)\n    end\nend\n\n# the whole dataset in a named tuple\nwage_missing = ismissing.(data.wage)\nwage = coalesce.(data.wage,1.)\nN = length(data.AGE)\n# create a named tuple with all variables to conveniently pass to the log-likelihood:\ndat = (;logwage = log.(wage),wage_missing,E = data.E,tU = data.DURUNEMP) \n\n(logwage = [0.0, 0.0, 0.0, 3.0368742168851663, 2.302585092994046, 3.2188758248682006, 2.2512917986064953, 0.0, 0.0, 0.0  …  0.0, 0.0, 2.4849066497880004, 3.056356895370426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 0, 0, 0, 0, 1, 1, 1  …  1, 1, 0, 0, 1, 1, 1, 1, 1, 1], E = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1], tU = [231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0  …  231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0])\n\n\nNow, let’s write the log-likelihood as above.\n\nusing Distributions, Optim\n\nϕ(x,μ,σ) = pdf(Normal(μ,σ),x)\nΦ(x,μ,σ) = cdf(Normal(μ,σ),x)\n\n# a function for the log-likelihood of observed wages (integrating out measurement error)\nfunction logwage_likelihood(logwage,F,σζ,wres)\n    f(x) = pdf(F,x) / (1-cdf(F,wres)) * ϕ(logwage,log(x),σζ)\n    ub = quantile(F,0.9999)\n    return integrateGL(f,wres,ub)\nend\n\n# a function to get the log-likelihood of a single observation\n# note this function assumes that data holds vectors \n# E, tU, and logwage\nfunction log_likelihood(n,data,pars)\n    (;h,δ,wres,F,σζ) = pars\n    ll = 0.\n    if data.E[n]\n        ll += log(h) - log(h + δ)\n        if !data.wage_missing[n]\n            ll += logwage_likelihood(data.logwage[n],F,σζ,wres)\n        end\n    else\n        ll += log(δ) - log(h + δ)\n        ll += log(h) + data.tU[n] * log(1-h)\n    end\n    return ll\nend\n\n# a function to iterate over all observations\nfunction log_likelihood_obj(x,pars,data)\n    pars = update(pars,x)\n    ll = 0.\n    for n in eachindex(data.E)\n        ll += log_likelihood(n,data,pars)\n    end\n    return ll / length(data.E)\nend\n\nlog_likelihood_obj (generic function with 1 method)\n\n\nFinally, since routines like Optim optimize over vectors, we want to write an update routine that takes a vector x and maps it to new parameters. Here we are going to use transformation functions to ensure that parameters obey their bound constraints. There are other ways to ensure this, but this is one way that works.\n\nlogit(x) = exp(x) / (1+exp(x))\nlogit_inv(x) = log(x/(1-x))\n\nfunction update(pars,x)\n    h = logit(x[1])\n    δ = logit(x[2])\n    μ = x[3]\n    σ = exp(x[4])\n    wres = exp(x[5])\n    F = LogNormal(μ,σ)\n    return (;pars...,h,δ,μ,σ,wres,F)\nend\n\nupdate (generic function with 1 method)\n\n\nNow we can finally test our likelihood to see how it runs:\n\nx0 = [logit_inv(0.5),logit_inv(0.03),2.,log(1.),log(5.)]\npars = (;σζ = 0.05, β = 0.995)\nlog_likelihood_obj(x0,pars,dat) #&lt;- test.\nres = optimize(x-&gt;-log_likelihood_obj(x,pars,dat),x0,Newton(),Optim.Options(show_trace=true))\n\nIter     Function value   Gradient norm \n     0     2.428210e-01     2.348428e-01\n * time: 5.984306335449219e-5\n     1     2.051862e-01     1.072566e-01\n * time: 1.8515889644622803\n     2     1.827395e-01     4.626301e-01\n * time: 3.292236804962158\n     3     1.758193e-01     4.441528e-01\n * time: 4.586717844009399\n     4     1.692587e-01     3.168138e-02\n * time: 6.080646991729736\n     5     1.668869e-01     1.483080e-01\n * time: 7.363166809082031\n     6     1.647839e-01     2.507384e-01\n * time: 8.843937873840332\n     7     1.638445e-01     1.648957e-01\n * time: 10.333063840866089\n     8     1.633820e-01     2.338710e-02\n * time: 11.627697944641113\n     9     1.633356e-01     2.095668e-02\n * time: 13.11549687385559\n    10     1.633224e-01     1.774668e-03\n * time: 14.597818851470947\n    11     1.633213e-01     3.628638e-03\n * time: 16.073489904403687\n    12     1.633211e-01     1.280346e-04\n * time: 17.559399843215942\n    13     1.633210e-01     3.719531e-04\n * time: 19.04760980606079\n    14     1.633210e-01     5.299485e-06\n * time: 20.525203943252563\n    15     1.633210e-01     3.008006e-05\n * time: 22.201759815216064\n    16     1.633210e-01     8.243085e-08\n * time: 23.69115400314331\n    17     1.633210e-01     6.359010e-08\n * time: 24.976842880249023\n    18     1.633210e-01     2.488281e-07\n * time: 26.455304861068726\n    19     1.633210e-01     2.588340e-08\n * time: 27.74436092376709\n    20     1.633210e-01     1.472700e-08\n * time: 29.030247926712036\n    21     1.633210e-01     2.956400e-09\n * time: 30.599798917770386\n\n\n * Status: success (objective increased between iterations)\n\n * Candidate solution\n    Final objective value:     1.633210e-01\n\n * Found with\n    Algorithm:     Newton's Method\n\n * Convergence measures\n    |x - x'|               = 6.08e-06 ≰ 0.0e+00\n    |x - x'|/|x'|          = 2.83e-07 ≰ 0.0e+00\n    |f(x) - f(x')|         = 1.60e-13 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 9.82e-13 ≰ 0.0e+00\n    |g(x)|                 = 2.96e-09 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   31  (vs limit Inf)\n    Iterations:    21\n    f(x) calls:    66\n    ∇f(x) calls:   66\n    ∇²f(x) calls:  21\n\n\nHere we tell Optim to make use of automatic differentiation with ForwardDiff. Let’s take a peek at the parameter estimates:\n\nDataFrame(;update(pars,res.minimizer)...)\n\n1×8 DataFrame\n\n\n\nRow\nσζ\nβ\nh\nδ\nμ\nσ\nwres\nF\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nLogNorma…\n\n\n\n\n1\n0.05\n0.995\n0.184915\n0.00764741\n3.47993\n1.09258\n4.80711e-10\nLogNormal{Float64}(μ=3.47993, σ=1.09258)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance Note\n\n\n\nNote that in Example 12.1 we created a NamedTuple called dat from the data frame, which cements the type of each vector of data into dat.\nThis is important for performance! Working with DataFrame types directly can dramatically slow down your code because the columns of these data frames are not typed concretely. See the performance tips for more discussion.\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 12.1 Extend the code above to\n\nAdditionally estimate the parameters \\(b\\) and \\(\\lambda\\).\nEstimate the search model separately for men with and without a bachelor’s degree.\nReport and comment on your estimates.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing the Estimators with Examples</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_intro_examples.html#the-labor-supply-model",
    "href": "lectures/extremum_intro_examples.html#the-labor-supply-model",
    "title": "12  Introducing the Estimators with Examples",
    "section": "12.3 The Labor Supply Model",
    "text": "12.3 The Labor Supply Model\nSuppose that we have a vector of instruments \\(\\mathbf{z}_{n}\\) that we hope will jointly move consumption and labor supply, with a single cross-section of observations:\n\\[ (W_{n},H_{n},C_{n},\\mathbf{z}_{n}).\\]\nWe write the labor supply equation as\n\\[ \\log(H) = \\mu - \\psi\\log(W) - \\psi\\sigma\\log(C) + \\epsilon \\]\nwhere we assume that \\(\\mathbb{E}[\\epsilon\\ |\\mathbf{z}] = 0\\), implying the moment condition:\n\\[ \\mathbb{E}[\\epsilon \\mathbf{z}] = 0.\\]\nLet \\(\\theta  = (\\mu,\\sigma,\\psi)\\) and define the sample moment:\n\\[g_{N}(\\theta) = \\frac{1}{N}\\sum_{N}\\left(\\log(H_{n})-\\mu-\\psi\\log(W)-\\psi\\sigma\\log(C)\\right)\\mathbf{z}_{n}.\\]\nThe GMM estimator is:\n\\[ \\hat{\\theta} = \\arg\\min_{\\theta} g_{N}(\\theta)^\\prime \\mathbf{W}_{N} g_{N}(\\theta) \\]\nwhere \\(\\mathbf{W}_{N}\\) is a symmetric, positive definite weighting matrix. Since we have a linear system, this becomes a quadratic minimization problem with a known solution,1 but the theory we develop will be more general.\nRelevant questions for GMM are:\n\nWhat value of \\(\\mathbf{W}\\) will give us the “best” performing estimator in the population? (And yes, we also have to define what “best” means).\nCan we implement optimal weighting in finite sample?",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing the Estimators with Examples</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_intro_examples.html#the-savings-model",
    "href": "lectures/extremum_intro_examples.html#the-savings-model",
    "title": "12  Introducing the Estimators with Examples",
    "section": "12.4 The Savings Model",
    "text": "12.4 The Savings Model\nLet’s consider estimation of the income process for this model and save estimation of the preference parameters for our chapter on simulation. Recall from our discussion of identification and from Exercise 10.1 that we can identify the parameters of this process by matching implied variances and covariances. Supposing that we have more of these moments than we do parameters (i.e. that the parameters are over-identified by the moments), we can estimate the income process by minimum distance.\nRecall the income process: \\[ \\varepsilon_{n,t+1} = \\rho \\varepsilon_{n,t} + \\eta_{n,t},\\qquad \\eta_{n,t}\\sim\\mathcal{N}(0,\\sigma^2_\\eta) \\]\nand consider the extended specification with permanent heterogeneity: \\[ \\log(y_{n,t}) = \\mu_t + \\alpha_n + \\varepsilon_{n,t} \\]\nwhere \\(\\alpha_n \\sim (0,\\sigma^2_\\alpha)\\) is an individual fixed effect. Let us further assume that in the first period, \\(\\varepsilon_{0} = 0\\). This gives us that\n\\[\\varepsilon_{t} = \\sum_{s=1}^{t}\\rho^{t-s}\\eta_{s}.\\]\nDefine \\(\\theta = (\\rho, \\sigma^2_\\alpha, \\sigma^2_\\eta)\\) as the parameters we wish to estimate.\n\n12.4.1 The Minimum Distance Estimator\nIn Example 10.1, we considered identification of the income process by examining covariance restrictions in panel data. Here we’ll consider this approach as well as an alternative. Define\n\\[\\epsilon = \\log(y) - \\mu_{t} = \\alpha + \\varepsilon \\]\nLet’s begin by noting the following generic relationships:\n\\[ \\mathbb{V}[\\epsilon_{t}] = \\sigma^2_{\\alpha} + \\frac{(1-\\rho^{2(t-1)})}{1-\\rho^2}\\sigma^2_{\\eta}\\]\nand\n\\[ \\mathbb{V}[\\epsilon_{t+1}] = \\sigma^2_{\\alpha} + \\rho^2\\mathbb{V}[\\varepsilon_{t}] + \\sigma^2_{\\eta} \\]\n\\[ \\mathbb{C}(\\epsilon_{t},\\epsilon_{t+s}) = \\sigma^2_{\\alpha} + \\rho^{s}\\mathbb{V}[\\epsilon_{t}] \\]\nWe’ll consider two potential vectors of moments to match. The first vector consists of the variance of \\(\\epsilon\\) at each \\(t\\):\n\\[ \\mathbf{v} = [\\mathbb{V}[\\epsilon_{1}],\\ \\mathbb{V}[\\epsilon_{2}],\\ ...,\\ \\mathbb{V}[\\epsilon_{T}]]^\\prime \\]\nwhile the second takes two variances and \\(K\\) covariances:\n\\[\\mathbf{c} = [\\mathbb{V}[\\epsilon_{t}],\\ \\mathbb{V}[\\epsilon_{t+1}],\\ \\mathbb{C}(\\epsilon_{t},\\epsilon_{t+1}),\\ ...,\\ \\mathbb{C}(\\epsilon_{t},\\epsilon_{t+K})]^\\prime \\]\nLet \\(\\mathbf{v}(\\theta)\\) and \\(\\mathbf{c}(\\theta)\\) be the model-implied values of these moments, given by the expressions above. The minimum distance estimator is\n\\[ \\hat{\\theta} = \\arg\\min_\\theta \\left(\\hat{\\mathbf{v}} - \\mathbf{v}(\\theta)\\right)^\\prime \\mathbf{W} \\left(\\hat{\\mathbf{v}} - \\mathbf{v}(\\theta)\\right) \\]\nwhere \\(\\mathbf{W}\\) is a positive definite weighting matrix. An estimator is equivalently defined for the second set of moments \\(\\mathbf{c}\\).\n\n\n\n\n\n\nExample: Minimum Distance Estimation of the Income Process\n\n\n\n\nExample 12.2 Building on Example 10.1, let’s implement a minimum distance estimator for the income process parameters. First, we compute sample moments from the PSID data.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics, Optim, Plots\n\n# Load and prepare data\ndata = @chain begin\n    CSV.read(\"../data/abb_aea_data.csv\",DataFrame,missingstring = \"NA\")\n    @select :person :y :tot_assets1 :asset :age :year\n    @subset :age.&gt;=25 :age.&lt;=64\nend\n\n# Calculate the variance of log income at each age\nm_hat = @chain data begin\n    groupby(:age)\n    @combine :var_logy = var(log.(:y))\n    @orderby :age\n    _.var_logy\nend\n\n40-element Vector{Float64}:\n 0.2832928560329017\n 0.31385672611280657\n 0.3481614098261801\n 0.49042885748190596\n 0.7186945866590636\n 0.8145871761041581\n 0.3899819595842863\n 0.4310734870614536\n 1.0048586189529876\n 0.8330684076119356\n 0.6927461795744777\n 0.4530783605245448\n 0.7647115466410289\n ⋮\n 0.7856911636904547\n 0.8002257744890358\n 0.6119942670835942\n 0.9594333073919417\n 0.6036241679859082\n 1.0296983130781643\n 0.6008575915783718\n 1.1495148217769573\n 0.7851363933479678\n 1.6514958037883842\n 0.5559690181469094\n 1.1855708905092428\n\n\nNow we define the model-implied moments. Since PSID is biennial, we adjust for two-year gaps:\n\nfunction model_moments(θ, T)\n    ρ, σ2_α, σ2_η = θ\n    # Variance of transitory component (assuming stationarity for simplicity)\n    var_eps = σ2_η / (1 - ρ^2)\n    m = [σ2_α + (1-ρ^(2(t-1)))/(1-ρ^2) * σ2_η for t in 1:T]\n    return m\nend\n\n# Minimum distance objective (identity weighting matrix)\nfunction md_objective(x, m_hat)\n    # Transform to ensure constraints: ρ ∈ (-1,1), σ² &gt; 0\n    ρ = tanh(x[1])\n    σ2_α = exp(x[2])\n    σ2_η = exp(x[3])\n    θ = (ρ, σ2_α, σ2_η)\n    T = length(m_hat)\n    m_model = model_moments(θ, T)\n    diff = m_hat .- m_model\n    return diff' * diff  # Identity weighting\nend\n\nmd_objective (generic function with 1 method)\n\n\nFinally, we estimate the parameters:\n\n# Initial values\nx0 = [0.5, log(0.1), log(0.05)]\n\n# Optimize\nres = optimize(x -&gt; md_objective(x, m_hat), x0, Newton(),autodiff = :forward)\n\n# Extract estimates\nx_hat = res.minimizer\nρ_hat = tanh(x_hat[1])\nσ2_α_hat = exp(x_hat[2])\nσ2_η_hat = exp(x_hat[3])\nθ_hat = (ρ_hat, σ2_α_hat, σ2_η_hat)\nprintln(\"Minimum Distance Estimates:\")\nprintln(\"  ρ = $(round(ρ_hat, digits=3))\")\nprintln(\"  σ²_α = $(round(σ2_α_hat, digits=3))\")\nprintln(\"  σ²_η = $(round(σ2_η_hat, digits=3))\")\n# and finally a plot of model fit\nT = length(m_hat)\nscatter(1:T,m_hat,label = \"data\",title = \"Model Fit of Targeted Moments\")\nplot!(1:T,model_moments(θ_hat,length(m_hat)),label = \"model fit\")\nxlabel!(\"Model Periods (Age)\")\n\nMinimum Distance Estimates:\n  ρ = 0.918\n  σ²_α = 0.279\n  σ²_η = 0.085\n\n\n\n\n\n\n\n\nAs with the case of GMM, we would like to know how our choice of \\(\\mathbf{W}\\) affects the sampling distribution (i.e. precision) of our estimator and if there is an “optimal” choice.\n\n\n\n\n\n\nDiscussion: Whether vs How\n\n\n\nLet’s think about more about how we approached identification here: by matching the growth in the variance of log income with age.\nEssentially, we are attributing all of this growth to income risk. Suppose we use the model to evaluate the welfare gains from redistributive taxes and transfers. Are you comfortable with how we’ve identified the extent of income risk? How important will those parameters be for how agents value social insurance?",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing the Estimators with Examples</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_intro_examples.html#the-entry-exit-model",
    "href": "lectures/extremum_intro_examples.html#the-entry-exit-model",
    "title": "12  Introducing the Estimators with Examples",
    "section": "12.5 The Entry-Exit Model",
    "text": "12.5 The Entry-Exit Model\nConsider two alternative estimators of the entry-exit model. The key insight from our identification discussion is that the choice probability \\(p(x,a,a')\\) is directly observable in the data and encodes information about the underlying payoff parameters.\nRecall the payoff specification: \\[ u_{1}(x,a,d^{\\prime}) = \\phi_{0} + \\phi_{1}x - \\phi_{2}d^\\prime - \\phi_{3}(1-a) \\] \\[ u_{0}(x,a) = \\phi_{4}a \\]\nand let \\(\\phi = (\\phi_0, \\phi_1, \\phi_2, \\phi_3, \\phi_4)\\) denote the vector of payoff parameters.\n\n12.5.1 Estimation by Minimum Distance\nThe minimum distance approach directly exploits the mapping between parameters and choice probabilities. For each market-state combination \\((x,a,a')\\), the model implies a choice probability:\n\\[ p(x,a,a';\\phi,\\beta) = \\frac{\\exp(v_1(x,a,a';\\phi,\\beta))}{\\exp(v_0(x,a,a';\\phi,\\beta)) + \\exp(v_1(x,a,a';\\phi,\\beta))} \\]\nwhere \\(v_0\\) and \\(v_1\\) are the choice-specific value functions that depend on the equilibrium solution.\nSuppose we have a cross-section of data:\n\\[ (X_{m},D_{m},A_{m},A'_{m})_{m=1}^{M} \\]\nfor each of \\(M\\) markets. Further assume that \\(X\\) is a variable that takes one of a discrete number of values in the support \\(\\mathcal{X}\\).\nFor each unique state \\((x,a,a')\\) in the data, we can compute the empirical choice frequency:\n\\[ \\hat{p}(x,a,a') = \\frac{\\sum_{m} D_{m}\\mathbf{1}\\{X_m = x, A_{m} = a, A'_{m} = a'\\}}{\\sum_{m} \\mathbf{1}\\{X_m = x, A_{m} = a, A'_{m} = a'\\}} \\]\nThe minimum distance estimator minimizes the weighted sum of squared deviations between observed and predicted choice probabilities. Let \\(\\mathbf{p}(\\theta)\\) be the vector of choice probabilities across the state space \\(\\mathcal{X}\\times\\{0,1\\}^2\\) and let \\(\\widehat{\\mathbf{p}}\\) be the equivalent frequency estimate. The minimum distance estimator is:\n\\[ \\hat{\\theta} = \\arg\\min_\\theta (\\widehat{\\mathbf{p}}-\\mathbf{p}(\\theta))^\\prime \\mathbf{W}_{N}(\\widehat{\\mathbf{p}}-\\mathbf{p}(\\theta))\\]\nwhere \\(\\mathbf{W}_{N}\\) is once again a positive definite weighting matrix.\n\n\n12.5.2 Estimation by GMM\nAn alternative approach uses the Generalized Method of Moments. The key insight is that choice probabilities satisfy certain orthogonality conditions that can be expressed as moment restrictions.\nGiven the discrete choice structure, the residual: \\[ \\xi_{m} = D_{m} - p(X_m, A_{m}, A'_{m}; \\phi, \\beta) \\]\nhas the property that \\(\\mathbb{E}[\\xi_{m} | X_m, A_{m}, A'_{m}] = 0\\) when evaluated at the true parameters. This suggests the moment conditions:\n\\[ \\mathbb{E}\\left[(D_{m} - p(X_m, A_{m}, A'_{m}; \\phi, \\beta)) \\cdot \\mathbf{z}_{m}\\right] = 0 \\]\nwhere \\(\\mathbf{z}_{m}\\) is a vector of instruments. Natural choices include functions of \\((X_m, A_{m}, A'_{m})\\) themselves, such as:\n\\[ \\mathbf{z}_{m} = [1,\\ X_m,\\ A_{m},\\ A'_{m},\\ X_m \\cdot A_{m}]^\\prime \\]\nThe GMM estimator minimizes:\n\\[ \\hat{\\phi} = \\arg\\min_\\phi g_M(\\phi)^\\prime \\mathbf{W}_M g_M(\\phi) \\]\nwhere the sample moment is:\n\\[ g_M(\\phi) = \\frac{1}{M}\\sum_{m}\\left(D_{m} - p(X_m, A_{m}, A'_{m}; \\phi, \\beta)\\right) \\mathbf{z}_{m} \\]",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing the Estimators with Examples</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_intro_examples.html#footnotes",
    "href": "lectures/extremum_intro_examples.html#footnotes",
    "title": "12  Introducing the Estimators with Examples",
    "section": "",
    "text": "Specifically, let \\(\\beta = [\\mu,\\ \\psi, \\psi\\sigma]^\\prime\\), we know that: \\(\\hat{\\beta} = (\\mathbf{X}^\\prime\\mathbf{Z}\\mathbf{W}_{N}\\mathbf{Z}^\\prime\\mathbf{X})^{-1}(\\mathbf{X}^\\prime\\mathbf{Z}\\mathbf{W}_{N}\\mathbf{Z}^\\prime\\mathbf{Y}\\) where \\(\\mathbf{X}\\), \\(\\mathbf{Z}\\), \\(\\mathbf{Y}\\) are appropriately stacked vectors of regressors, instruments, and outcomes (log hours).↩︎",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducing the Estimators with Examples</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aguirregabiria, Victor, and Pedro Mira. 2002. “Swapping the Nested\nFixed Point Algorithm: A Class of Estimators for Discrete Markov\nDecision Models.” Econometrica 70 (4): 1519–43.\n\n\n———. 2007. “Sequential Estimation of Dynamic Discrete\nGames.” Econometrica 75 (1): 1–53.\n\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2010. “The\nCredibility Revolution in Empirical Economics: How Better Research\nDesign Is Taking the Con Out of Econometrics.” Journal of\nEconomic Perspectives 24 (2).\n\n\nArellano, Manuel, Richard Blundell, and Stephane Bonhomme. 2018.\n“Nonlinear Persistence and Partial Insurance: Income and\nConsumption Dynamics in the PSID.” AEA Papers and\nProceedings 108 (May): 281–86. https://doi.org/10.1257/pandp.20181049.\n\n\nBlundell, Richard, and Ian Walker. 1986. “A Life-Cycle Consistent\nEmpirical Model of Family Labour Supply Using Cross-Section\nData.” The Review of Economic Studies 53 (4): 539–58.\n\n\nCarneiro, P., J. J. Heckman, and E. J. Vytlacil. 2011. “Estimating\nMarginal Returns to Education.” American Economic Review\n101 (October): 2754–81. http://www.nber.org/papers/w16474.\n\n\nChernozhukov, Victor, and Christian Hansen. 2005. “An IV Model of\nQuantile Treatment Effects.” Econometrica 73 (1):\n245–61.\n\n\nChesher, Andrew. 2010. “Instrumental Variable Models for Discrete\nOutcomes.” Econometrica 78 (2): 575–601.\n\n\nCunha, Flavio, James Heckman, and Susanne Schennach. 2010.\n“Estimating the Technology of Cognitive and Noncognitive Skill\nFormation.” Econometrica 78 (3): 883–931.\n\n\nDe Nardi, Mariacristina. 2004. “Wealth Inequality and\nIntergenerational Links.” The Review of Economic Studies\n71 (3): 743–68. https://doi.org/10.1111/j.1467-937X.2004.00302.x.\n\n\nDearing, Adam, and Jason R. Blevins. 2024. “Efficient and\nConvergent Sequential Pseudo-Likelihood Estimation of Dynamic Discrete\nGames.” Review of Economic Studies.\n\n\nEricson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry\nDynamics: A Framework for Empirical Work.” The Review of\nEconomic Studies 62 (1): 53–82. https://doi.org/10.2307/2297841.\n\n\nFlinn, Christopher, and James Heckman. 1982. “New Methods for\nAnalyzing Structural Models of Labor Force Dynamics.” Journal\nof Econometrics 18 (1): 115–68.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with\nVariation in Treatment Timing.” Journal of Econometrics\n225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nGorman, W. M. 1959. “Separable Utility and Aggregation.”\nEconometrica 27 (3): 469–81. http://www.jstor.org/stable/1909472.\n\n\nGourinchas, Pierre-Olivier, and Jonathan A. Parker. 2002.\n“Consumption over the Life Cycle.” Econometrica 70\n(1): 47–89. https://doi.org/10.1111/1468-0262.00269.\n\n\nHarberger, Arnold C. 1954. “Monopoly and Resource\nAllocation.” The American Economic Review 44 (2): 77–87.\nhttp://www.jstor.org/stable/1818325.\n\n\nHeckman, James J, and Bo E Honore. 1990. “The Empirical Content of\nthe Roy Model.” Econometrica: Journal of the Econometric\nSociety, 1121–49.\n\n\nHeckman, James, and Burton Singer. 1984. “A Method for Minimizing\nthe Impact of Distributional Assumptions in Econometric Models for\nDuration Data.” Econometrica: Journal of the Econometric\nSociety, 271–320.\n\n\nHeckman, James, and Edward Vytlacil. 2005. “Structural equations, treatment effects, and econometric\npolicy evaluation.” Econometrica 73 (3): 669–738.\n\n\n———. 2007. “Chapter 70 Econometric Evaluation of Social Programs,\nPart i: Causal Models, Structural Models and Econometric Policy\nEvaluation.” In, edited by James J. Heckman and Edward E. Leamer,\n6:4779–874. Handbook of Econometrics. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(07)06070-9.\n\n\nImbens, Guido W., and Joshua D. Angrist. 1994. “Identification and\nEstimation of Local Average Treatment Effects.”\nEconometrica 62 (2): 467–75. http://www.jstor.org/stable/2951620.\n\n\nKasahara, Hiroyuki, and Katsumi Shimotsu. 2009. “Nonparametric\nIdentification of Finite Mixture Models of Dynamic Discrete\nChoices.” Econometrica 77 (1): 135–75.\n\n\nKleven, Henrik J. 2021. “Sufficient Statistics Revisited.”\nJournal Article. Annual Review of Economics 13 (Volume 13,\n2021): 515–38. https://doi.org/https://doi.org/10.1146/annurev-economics-060220-023547.\n\n\nLewbel, Arthur. 2019. “The Identification Zoo: Meanings of\nIdentification in Econometrics.” Journal of Economic\nLiterature 57 (4): 835–903. https://doi.org/10.1257/jel.20181361.\n\n\nLucas Jr, Robert E. 1976. “Econometric Policy Evaluation: A\nCritique.” In Carnegie-Rochester Conference Series on Public\nPolicy, 1:19–46. North-Holland.\n\n\nMaCurdy, Thomas E. 1981. “An Empirical Model of Labor Supply in a\nLife-Cycle Setting.” Journal of Political Economy 89\n(6): 1059–85.\n\n\nMagnac, Thierry, and David Thesmar. 2002. “Identifying Dynamic\nDiscrete Decision Processes.” Econometrica 70 (2):\n801–16. https://doi.org/https://doi.org/10.1111/1468-0262.00306.\n\n\nMarschak, Jacob. 1953. “Economic Measurements for Policy and\nPrediction.” In Studies in Econometric Method, edited by\nW. Hood and C. Koopmans. John Wiley & Sons.\n\n\nMcCall, John Joseph. 1970. “Economics of Information and Job\nSearch.” The Quarterly Journal of Economics 84 (1):\n113–26.\n\n\nMullins, Joseph. 2026. “A Structural Meta-Analysis of Welfare\nReform Experiments and Their Impacts on Children.” Journal of\nPolitical Economy 134 (1): 435–77. https://doi.org/10.1086/738482.\n\n\nRoy, A. D. 1951. “Some Thoughts on the Distribution of\nEarnings.” Oxford Economic Papers 3 (2): 135–46. http://www.jstor.org/stable/2662082.\n\n\nVytlacil, Edward J. 2002. “Independence ,\nMonotonicity and Latent Index Models : An Equivalence\nResult.” Econometrica 70 (1): 331–41.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendices/data.html",
    "href": "appendices/data.html",
    "title": "Appendix A — Data",
    "section": "",
    "text": "A.1 A Disclaimer for IPUMS CPS data\nThese data are a subsample of the IPUMS CPS data available from cps.ipums.org. Any use of these data should be cited as follows:\nSarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D030.V11.0\nThe CPS data file is intended only for exercises as part of ECON8208. Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests. For all other uses of these data, please access data directly via cps.ipums.org.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "appendices/data.html#a-disclaimer-for-ipums-cps-data",
    "href": "appendices/data.html#a-disclaimer-for-ipums-cps-data",
    "title": "Appendix A — Data",
    "section": "",
    "text": "Arellano, Manuel, Richard Blundell, and Stephane Bonhomme. 2018. “Nonlinear Persistence and Partial Insurance: Income and Consumption Dynamics in the PSID.” AEA Papers and Proceedings 108 (May): 281–86. https://doi.org/10.1257/pandp.20181049.\n\n\nDearing, Adam, and Jason R. Blevins. 2024. “Efficient and Convergent Sequential Pseudo-Likelihood Estimation of Dynamic Discrete Games.” Review of Economic Studies.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html",
    "href": "appendices/autodiff.html",
    "title": "Appendix B — Automatic Differentiation",
    "section": "",
    "text": "B.1 Why AD Matters for Structural Estimation\nIn structural econometrics, we frequently need gradients for:\nHand-coding derivatives is tedious and error-prone. Finite differences are slow (requiring \\(O(n)\\) function evaluations for an \\(n\\)-dimensional gradient) and can be numerically unstable. AD provides exact gradients efficiently.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#why-ad-matters-for-structural-estimation",
    "href": "appendices/autodiff.html#why-ad-matters-for-structural-estimation",
    "title": "Appendix B — Automatic Differentiation",
    "section": "",
    "text": "Optimization (MLE, GMM, minimum distance)\n\nGradient-free methods such as Nelder-Mead are popular, of course, but are less efficient\n\nComputing standard errors\nSolving models with equilibrium conditions (using Newton’s method, for example)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#forward-mode-vs-reverse-mode",
    "href": "appendices/autodiff.html#forward-mode-vs-reverse-mode",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.2 Forward Mode vs Reverse Mode",
    "text": "B.2 Forward Mode vs Reverse Mode\nAD comes in two flavors:\nForward mode propagates derivatives forward through the computation. For a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), computing the full Jacobian requires \\(n\\) forward passes. This is efficient when \\(n \\ll m\\).\nReverse mode propagates derivatives backward (like backpropagation in neural networks). Computing the full Jacobian requires \\(m\\) reverse passes. This is efficient when \\(m \\ll n\\).\nFor most estimation problems, we have a scalar objective (\\(m = 1\\)) and many parameters (\\(n\\) large), so reverse mode is typically preferred. In my experience however, I have had more success writing code that is compatible with forward differencing. You will learn from experience that these tools can be fussy.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#ad-in-julia",
    "href": "appendices/autodiff.html#ad-in-julia",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.3 AD in Julia",
    "text": "B.3 AD in Julia\nJulia’s AD ecosystem is excellent. The main packages are:\n\nB.3.1 ForwardDiff.jl\nForward-mode AD. Simple and robust, works out-of-the-box for most pure Julia code.\n\nusing ForwardDiff\n\nf(x) = sum(x.^2)\nx = [1.0, 2.0, 3.0]\n\n# Gradient\nForwardDiff.gradient(f, x)\n\n3-element Vector{Float64}:\n 2.0\n 4.0\n 6.0\n\n\n\n# Hessian\nForwardDiff.hessian(f, x)\n\n3×3 Matrix{Float64}:\n 2.0  0.0  0.0\n 0.0  2.0  0.0\n 0.0  0.0  2.0\n\n\n\n\nB.3.2 Enzyme.jl\nA high-performance AD engine that works at the LLVM level. Supports both forward and reverse mode. Often the fastest option, especially for code with loops and mutations.\n\nusing Enzyme\n\nf(x) = x[1]^2 + sin(x[2])\n\nx = [1.0, 2.0]\ndx = zeros(2)\n\n# Reverse mode gradient\nEnzyme.autodiff(Reverse, f, Active, Duplicated(x, dx))\ndx\n\n2-element Vector{Float64}:\n  2.0\n -0.4161468365471424\n\n\n\n\nB.3.3 Zygote.jl\nA source-to-source reverse-mode AD system. Popular in machine learning (used by Flux.jl). Works well for array-heavy code but may struggle with control flow.\n\nusing Zygote\n\nf(x) = sum(x.^2)\nx = [1.0, 2.0, 3.0]\n\nZygote.gradient(f, x)\n\n([2.0, 4.0, 6.0],)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#practical-recommendations",
    "href": "appendices/autodiff.html#practical-recommendations",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.4 Practical Recommendations",
    "text": "B.4 Practical Recommendations\n\nStart with ForwardDiff for problems with few parameters (&lt; 100). It’s the most reliable.\nUse Enzyme for performance-critical code, especially if you have loops or in-place mutations.\nBe aware of limitations: AD systems can fail on code that uses certain constructs (try-catch, foreign function calls, some global variables). When in doubt, test that your gradients match finite differences:\n\n\nusing ForwardDiff, FiniteDiff\n\nf(x) = log(1 + exp(x[1] * x[2])) + x[3]^2\nx = [1.0, 2.0, 3.0]\n\nad_grad = ForwardDiff.gradient(f, x)\nfd_grad = FiniteDiff.finite_difference_gradient(f, x)\n\nmaximum(abs.(ad_grad .- fd_grad))\n\n5.3512749786932545e-12",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#integration-with-optimization",
    "href": "appendices/autodiff.html#integration-with-optimization",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.5 Integration with Optimization",
    "text": "B.5 Integration with Optimization\nMost Julia optimization packages accept AD gradients. Here’s an example with Optim.jl:\n\nusing Optim, ForwardDiff\n\nrosenbrock(x) = (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2\nx0 = [0.0, 0.0]\n\n# With automatic gradients via ForwardDiff\nresult = optimize(rosenbrock, x0, LBFGS(); autodiff = :forward)\nresult.minimizer\n\n2-element Vector{Float64}:\n 0.999999999999928\n 0.9999999999998559",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#example-maximum-likelihood-with-optim.jl",
    "href": "appendices/autodiff.html#example-maximum-likelihood-with-optim.jl",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.6 Example: Maximum Likelihood with Optim.jl",
    "text": "B.6 Example: Maximum Likelihood with Optim.jl\nConsider a simple probit model:\n\\[ D = \\mathbf{1}\\{X\\beta - \\nu \\geq 0\\},\\qquad \\nu \\sim \\mathcal{N}(0,1) \\]\nHere is code to simulate data for this model:\n\nusing Random, Distributions\n\nfunction sim_data(X ; γ)\n    N = size(X,1)\n    ν = rand(Normal(),N)\n    D = (X * γ .- ν) .&gt; 0\n    return D\nend\n\n# a quick test of the function:\nN = 1000\nX = [ones(N) 2*rand(Normal(),N)]\nγ = [0.1, 0.5]\nD = sim_data(X ; γ);\n\nConsider the problem of estimating \\(\\gamma\\) using maximum likelihood. We will establish the properties of this estimator in class. Here let’s just focus on numerically how to attack the minimization problem. The log-likelihood of the data D given X is:\n\\[ \\mathcal{L}(\\gamma) = \\sum_{n}l(D_{n}; X_{n},\\gamma) = \\sum_{n=1}^{N}D_{n}\\log(\\Phi(X\\gamma)) + (1-D_{n})\\log(1-\\Phi(X\\gamma)) \\]\nLet’s write up this likelihood function.\n\nfunction log_likelihood(D,X,γ)\n    ll = 0.\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            ll += log(cdf(Normal(),xg))\n        else\n            ll += log(1-cdf(Normal(),xg))\n        end\n    end\n    return ll\nend\nlog_likelihood(D,X,[0.,0.])\n\n-693.1471805599322\n\n\n\nB.6.1 Numerical Optimization\nOptimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses \\(\\gamma_{k+1}\\) given \\(\\gamma_{k}\\) as:\n\\[ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} \\]\nwhere \\(Q\\) is the function being maximized (or minimized). \\(A_{k}\\) defines a direction in which to search (providing weights on the derivatives) and \\(\\lambda_{k}\\) is a scalar variable known as a step-size which is often calculated optimally in each iteration \\(k\\). For Newton’s method, the matrix \\(A_{k}\\) is the inverse of the Hessian of the objective function \\(Q\\). Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\nSince we have a simple model, we can calculate derivatives relatively easily. Below we’ll compare a hard-coded derivative to this automatic differentiation.\n\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\ndx = zeros(2)\n# forward mode\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x-&gt;log_likelihood(D,X,x),γ)\n# reverse mode\nauto_deriv2_ll(D,X,γ,dx) = Enzyme.autodiff(Reverse, x-&gt;log_likelihood(D,X,x), Active, Duplicated(γ, dx))\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\nauto_deriv2_ll(D,X,γ,dx)\n[d1 d2 dx]\n\n2×3 Matrix{Float64}:\n 18.585   18.585   18.585\n 36.7039  36.7039  36.7039\n\n\nOk so we’re confident that these functions work as intended, but how do they compare in performance?\n\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n@time auto_deriv2_ll(D,X,γ,dx);\n\n  0.000021 seconds (2 allocations: 80 bytes)\n  0.000035 seconds (7 allocations: 304 bytes)\n  0.000033 seconds\n\n\nAll are quite quick and you can see that we’re not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions.\nSo now let’s try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton’s Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shannon (BFGS) algorithm (which updates search direction using changes in the first derivative).\nWhile Newton’s method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let’s test them.\n\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #&lt;- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     8.794794e+02\n * time: 0.011163949966430664\n     1     5.038529e+02     1.424353e+02\n * time: 0.3925299644470215\n     2     4.896962e+02     5.718137e+00\n * time: 0.39277005195617676\n     3     4.896776e+02     6.554352e-04\n * time: 0.39298009872436523\n     4     4.896776e+02     2.812263e-10\n * time: 0.3931429386138916\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     8.794794e+02\n * time: 5.507469177246094e-5\n     1     5.022184e+02     1.277309e+02\n * time: 0.007002115249633789\n     2     4.993056e+02     1.210926e+02\n * time: 0.007193088531494141\n     3     4.898180e+02     1.538296e+01\n * time: 0.007416963577270508\n     4     4.896776e+02     7.292141e-02\n * time: 0.007581949234008789\n     5     4.896776e+02     2.642307e-03\n * time: 0.007750034332275391\n     6     4.896776e+02     6.079906e-08\n * time: 0.007950067520141602\n     7     4.896776e+02     3.867739e-14\n * time: 0.008134126663208008\n\n\n2×3 Matrix{Float64}:\n 0.143143  0.143143  0.1\n 0.540119  0.540119  0.5",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#further-reading",
    "href": "appendices/autodiff.html#further-reading",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.7 Further Reading",
    "text": "B.7 Further Reading\n\nJuliaDiff documentation\nForwardDiff.jl docs\nEnzyme.jl docs",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/performance.html",
    "href": "appendices/performance.html",
    "title": "Appendix C — Performance Tips",
    "section": "",
    "text": "C.1 The Golden Rules",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "appendices/performance.html#the-golden-rules",
    "href": "appendices/performance.html#the-golden-rules",
    "title": "Appendix C — Performance Tips",
    "section": "",
    "text": "C.1.1 1. Avoid Global Variables\nGlobal variables with non-constant types force the compiler to generate slow, generic code.\n# Bad\ndata = [1.0, 2.0, 3.0]\nf() = sum(data)  # `data` could change type\n\n# Good: use const\nconst DATA = [1.0, 2.0, 3.0]\nf() = sum(DATA)\n\n# Good: pass as argument\nf(data) = sum(data)\n\n\nC.1.2 2. Write Type-Stable Functions\nA function is type-stable if the output type can be inferred from the input types. Type instability forces runtime dispatch.\n# Bad: returns Int or Float64 depending on value\nfunction unstable(x)\n    if x &gt; 0\n        return 1\n    else\n        return 0.0\n    end\nend\n\n# Good: consistent return type\nfunction stable(x)\n    if x &gt; 0\n        return 1.0\n    else\n        return 0.0\n    end\nend\nUse @code_warntype to check for type instabilities (look for red Any or Union types).\n\n\nC.1.3 3. Pre-allocate Arrays\nAvoid creating arrays inside loops. Pre-allocate and use in-place operations.\n# Bad: allocates on every iteration\nfunction bad_example(n)\n    result = 0.0\n    for i in 1:n\n        v = zeros(100)  # allocation!\n        v .= rand(100)\n        result += sum(v)\n    end\n    result\nend\n\n# Good: pre-allocate\nfunction good_example(n)\n    result = 0.0\n    v = zeros(100)\n    for i in 1:n\n        rand!(v)  # in-place\n        result += sum(v)\n    end\n    result\nend\n\n\nC.1.4 4. Use @views for Array Slices\nArray slices create copies by default. Use @views or view() to avoid allocation.\nA = rand(1000, 1000)\n\n# Bad: creates a copy\nf(A[1:100, :])\n\n# Good: creates a view\nf(@views A[1:100, :])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "appendices/performance.html#quick-profiling",
    "href": "appendices/performance.html#quick-profiling",
    "title": "Appendix C — Performance Tips",
    "section": "C.2 Quick Profiling",
    "text": "C.2 Quick Profiling\nUse @time for basic timing (run twice—first call includes compilation):\n@time my_function(args)  # compile\n@time my_function(args)  # actual timing\nFor more detailed analysis: - BenchmarkTools.jl: Accurate microbenchmarks with @btime - Profile (stdlib): Sampling profiler - ProfileView.jl: Flame graph visualization",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "appendices/performance.html#further-resources",
    "href": "appendices/performance.html#further-resources",
    "title": "Appendix C — Performance Tips",
    "section": "C.3 Further Resources",
    "text": "C.3 Further Resources\n\nJulia Performance Tips — the official guide, essential reading\nJulia Academy performance course — free video tutorials\nBenchmarkTools.jl documentation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "lectures/identification_savings.html",
    "href": "lectures/identification_savings.html",
    "title": "10  The Life-Cycle Savings Model",
    "section": "",
    "text": "10.1 Identification of the Income Process\nWe’ll consider identification of the savings model with the following income process:\n\\[ \\log(y_{n,t}) = \\mu_{t} + \\varepsilon_{n,t}\\]\nwhere\n\\[ \\varepsilon_{n,t+1} = \\rho \\varepsilon_{n,t} + \\eta_{n,t},\\qquad \\eta_{m,t}\\sim\\mathcal{N}(0,\\sigma^2_\\eta) \\].\nCollecting parameters, we want to identify\n\\[ (\\mu,\\rho,\\sigma_\\eta),\\qquad (\\beta,\\sigma,\\psi) \\]\nwhere the first block indicates parameters of the income process, and the second block determines preferences.\nAssume that our data has a panel dimension, so that we see \\((y_t,C_t,t)_{t=\\tau_0}^{\\tau_1}\\) for some pair \\((\\tau_0,\\tau_1)\\). Remember that \\(t\\) indexes age in the model, so it is quite plausible that \\(\\tau_0\\) and \\(\\tau_1\\) may themselves be random variables (this will be true in the data… we see panels of individuals at different ages and for different lengths of time).\nFirst, as long as the support of the random variables (\\(\\tau_0,\\tau_1\\)) covers \\(t=1\\) through to \\(T\\), \\(\\mu\\) is identified as the mean of log income at each age, \\(\\mu_t = \\mathbb{E}[\\log(y_{t})]\\). We can then residualize log income to get \\(\\varepsilon_{t} = \\log(y_t)-\\mu_t\\).\nSecond, consider the following variances and covariances (remember that the \\(\\eta\\) terms are iid):\n\\[\\begin{eqnarray}\n\\mathbb{V}[\\varepsilon_{t+1}] = \\rho^2\\mathbb{V}[\\varepsilon_{t}] + \\sigma^2_{\\eta} \\\\\n\\mathbb{C}(\\varepsilon_{t},\\varepsilon_{t+1}) = \\rho\\mathbb{V}[\\varepsilon_{t}] \\\\\n\\mathbb{C}(\\varepsilon_{t},\\varepsilon_{t+2}) = \\rho^2\\mathbb{V}[\\varepsilon_t]\n\\end{eqnarray}\\]\nmeaning that we can identify \\(\\rho\\) and \\(\\sigma_\\eta\\) from this system of simultaneous equations.",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "lectures/identification_savings.html#identification-of-the-income-process",
    "href": "lectures/identification_savings.html#identification-of-the-income-process",
    "title": "10  The Life-Cycle Savings Model",
    "section": "",
    "text": "Note: Whether vs How\n\n\n\nNote that, since these moments can be calculated at any age \\(t\\) and can extend to arbitrary lags, this model is over-identified. Which moments should we use in practice? Thinking inside the model, we will address this topic when we get to discussing minimum distance estimation. You might also like to think outside the model and think about (1) how real income processes might deviate from your stylized model and (2) what features of the data you most want the parameters \\(\\rho\\) and \\(\\sigma^2\\) to capture.\n\n\n\n\n\n\n\n\nExample: Data from PSID\n\n\n\n\nExample 10.1 In this example we’ll load psid data from Arellano, Blundell, and Bonhomme (2018) and show how sample equivalents to the above moments might be calculated.\nTo begin, let’s load the data and pull out the variables we are interested in using. These are person identifiers (person), year, total income (y), savings (tot_assets1) and age. You should bear in mind that it is by no means trivial to measure total income and total assets in these data. The variables we are looking at are the product of a lot of data cleaning and careful choices by the authors.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics\ndata = @chain begin \n    CSV.read(\"../data/abb_aea_data.csv\",DataFrame,missingstring = \"NA\")\n    @select :person :y :tot_assets1 :asset :age :year\nend\n\n19317×6 DataFrame19292 rows omitted\n\n\n\nRow\nperson\ny\ntot_assets1\nasset\nage\nyear\n\n\n\nInt64\nInt64\nInt64\nFloat64\nInt64\nInt64\n\n\n\n\n1\n12061\n173100\n605000\n15500.0\n65\n98\n\n\n2\n17118\n54000\n60000\n0.0\n49\n98\n\n\n3\n12630\n61283\n224000\n39283.0\n59\n98\n\n\n4\n12647\n42300\n28240\n0.0\n38\n98\n\n\n5\n5239\n82275\n7500\n0.0\n56\n98\n\n\n6\n2671\n69501\n48000\n3600.0\n35\n98\n\n\n7\n13027\n68000\n148000\n20000.0\n49\n98\n\n\n8\n6791\n93758\n80000\n160.0\n41\n98\n\n\n9\n6475\n26581\n23300\n0.0\n35\n98\n\n\n10\n18332\n33785\n0\n0.0\n42\n98\n\n\n11\n3856\n55300\n311000\n5300.0\n33\n98\n\n\n12\n19326\n40200\n105250\n0.0\n40\n98\n\n\n13\n21818\n42500\n13000\n0.0\n36\n98\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n19306\n6617\n115887\n241000\n21346.0\n62\n108\n\n\n19307\n626\n128600\n98000\n0.0\n46\n108\n\n\n19308\n4795\n105000\n-68000\n0.0\n34\n108\n\n\n19309\n3223\n120000\n132000\n0.0\n47\n108\n\n\n19310\n8098\n26527\n4700\n0.0\n37\n108\n\n\n19311\n8954\n144026\n220000\n25.0\n46\n108\n\n\n19312\n12990\n122665\n220000\n0.0\n53\n108\n\n\n19313\n8782\n55000\n69000\n0.0\n31\n108\n\n\n19314\n13059\n42728\n-10000\n0.0\n26\n108\n\n\n19315\n13535\n57000\n0\n0.0\n26\n108\n\n\n19316\n3806\n87000\n74200\n0.0\n26\n108\n\n\n19317\n11085\n74000\n-50000\n0.0\n31\n108\n\n\n\n\n\n\nTo map to the model, assume that agents begin (\\(t=1\\)) when aged 25 and live for 40 years (so the “terminal” period is at age 64). Thus, we should filter the data to look at only these ages.\n\n@subset!(data,:age.&gt;=25,:age.&lt;=64);\n\nNow let’s residualize log wages by age, to get our estimate of \\(\\varepsilon_{n,t}\\):\n\ndata = @chain data begin\n    groupby(:age)\n    @transform :eps = log.(:y) .- mean(log.(:y))\nend;\n\nNext, here is a simple way of creating lagged variables (by mutating the year, renaming, and merging).\n\nd1 = @chain data begin\n    @select :year :person :eps\n    @transform :year = :year .- 2\n    @rename :epslag1 = :eps\nend\n\nd2 = @chain data begin\n    @select :year :person :eps\n    @transform :year = :year .- 4\n    @rename :epslag2 = :eps\nend\n\ndata = @chain data begin\n    innerjoin(d1 , on=[:person,:year])\n    innerjoin(d2 , on=[:person,:year])\nend;\n\nAn example of calculating covariances:\n\n@chain data begin\n    @combine begin \n        :c1 = cov(:eps,:epslag1) \n        :c2 = cov(:eps,:epslag2)\n    end\nend;\n\nSince the psid interviews are only every two years, we have to adjust our estimate of \\(\\rho\\) slightly by taking the square root of the covariance ratio:\n\nrho_est = sqrt(ans.c2[1] / ans.c1[1])\nprintln(\"The estimate of rho is $(round(rho_est,digits=2))\")\n\nThe estimate of rho is 0.98\n\n\nWhen it comes to the identification of this income process, let’s consider its ability to fit the life-cycle profile in the variance of income:\n\nusing Plots\nd = @chain begin data\n    groupby(:age)\n    @combine :var_income = var(log.(:y))\nend\nscatter(d.age,d.var_income,smooth = true,label = false)\n\n\n\n\nThe variance of log income seems to grow linearly with age, so this would be hard for our income process to fit if either\n\nWe assume that \\(\\varepsilon\\) is initially in its stationary distribution; or\n\\(\\rho\\) is far from 1, since it implies a concave path for the variance.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nExercise 10.1 Suppose that income processes also feature permanent differences in productivity among individuals, so that:\n\\[ \\log(y_{n,t}) = \\mu_t + \\alpha_n + \\varepsilon_{n,t} \\]\nwhere \\(\\varepsilon_{n,t}\\) is defined as before, and \\(\\alpha_n\\) is the individual fixed effect in wages. Assume that \\(\\alpha\\perp \\varepsilon_1\\), \\(\\alpha \\perp \\eta_t\\) for all \\(t\\), and define \\(\\sigma^2_\\alpha = \\mathbb{V}[\\alpha]\\).\n\nShow that you can identify this income process using additional covariances.\nEstimate the parameters \\((\\rho,\\sigma^2_\\alpha,\\sigma^2_\\eta)\\) by following your identification argument using the psid data from Example 10.1.\nHow do your estimates compare to Example 10.1 where we ignored permanent individual heterogeneity?",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "lectures/identification_savings.html#identification-of-preference-parameters",
    "href": "lectures/identification_savings.html#identification-of-preference-parameters",
    "title": "10  The Life-Cycle Savings Model",
    "section": "10.2 Identification of Preference Parameters",
    "text": "10.2 Identification of Preference Parameters\nThis is an interesting case because the problem will likely more closely reflect how you will approach identification in your own research.\nIn previous examples, we typically made use of analytical (i.e. “pencil and paper”) representations of optimal behavior as they relate to deeper parameters, and we used this for identification. That is harder to do here since we know we must solve for savings policies numerically.\n\n\n\n\n\n\nIdentification of more complicated models\n\n\n\nHere are some steps to help you think through identification of your model.\n\nCould you obtain identification if you had a “perfect” data set or experiment? Do your data allow a second-best approximation that harnesses the intuition of this perfect alternative? Remember that to show identification you can dispense with the practical considerations of finite samples and zoom in on very specific comparisons within the population distribution.\nWhat kind of variation is in the data that you do have and how do the parameters determine individuals’ response to that variation? If necessary, you can play with numerical solutions of the model to develop your intuition here.\nCan you simplify your model in a way that highlights some of the key forces of identification?\n\n\n\nThese approaches all differ in their level of precision. Your main goal is to provide your audience and yourself with some credible and sensible intuition. For the savings model, let’s use some combination of strategies (2) and (3). First, note that when \\(\\psi=0\\), individuals would run their assets down to zero in the final period. Thus, \\(\\psi\\) is very clearly identified by average bequests at the end of the life-cycle.\nNow, suppose we remove uncertainty from the model and impose the natural borrowing constraint, such that the Euler equation becomes:\n\\[ \\beta(1+r)\\left(\\frac{C_{t+1}}{C_{t}}\\right)^{-\\sigma}=1 \\]\nNotice that \\(\\sigma\\) determines the intertemporal elasticity of substitution: how individuals would substitute consumption across periods when there is variation in the price of doing so (\\(r\\)). What sources of variation do we have in this model? Only the income shocks \\(\\eta\\). Without uncertainty, individuals then choose a consumption profile based on the effect of that shock on the net present value of income. The resultant path depends on \\(\\beta\\), \\(r\\), and \\(\\sigma\\), but importantly \\(\\beta\\) and \\(\\sigma\\) are not separately identified. Thus, uncertainty and borrowing constraints hold the key for separately identifying parameters in our setting.\nNow, we know from experimenting with this model that as individuals accumulate assets, the risk of hitting the borrowing constraint diminishes and their behavior begins to more closely reflect the case without uncertainty: consumption responses are very close to linear with respect to cash in hand. Thus, to identify \\(\\beta\\) and \\(\\sigma\\) separately, we need to focus on potential nonlinearities in consumption behavior closer to the borrowing constraint. One example of a set of identifying moments would be:\n\nMean assets at each age; and\nThe covariance of changes in log consumption with log income conditional on different asset levels.\n\nWhile the first set of moments should pin down \\(\\beta\\) and \\(\\psi\\) jointly by effectively matching average consumption profiles, the second set attempts to pin down the nonlinear effect that \\(\\sigma\\) has on consumption at different wealth levels.\nAs you can see, this is a sensible intuitive approach to identification that does not offer an exact mapping between data and parameters.\nSince \\(\\sigma\\) determines both risk aversion and the intertemporal elasticity of substitution, some ideal data settings that would identify \\(\\sigma\\) would include:\n\nThe risk profile of asset portfolio choices (we don’t have this).\nVariation in the income risk faced by individuals (we don’t have this).\nVariation in the returns to saving either through \\(r_t\\) or through policy intervention (we don’t have this).\n\nIn general then there are many ways to identify \\(\\sigma\\), but most are missing in our simple model, so we will have to be more careful with the moments we choose.\n\n\n\n\n\n\nWhether vs How\n\n\n\nSuppose you wanted to use this model to evaluate the effect of a pension program reform on savings behavior. Would you be comfortable forecasting counterfactuals with this kind of identification approach? What kind of variation in the data would help you feel that your identification approach was more credible?\n\n\n\n\n\n\nArellano, Manuel, Richard Blundell, and Stephane Bonhomme. 2018. “Nonlinear Persistence and Partial Insurance: Income and Consumption Dynamics in the PSID.” AEA Papers and Proceedings 108 (May): 281–86. https://doi.org/10.1257/pandp.20181049.",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_theory.html",
    "href": "lectures/extremum_theory.html",
    "title": "13  Asymptotic Theory",
    "section": "",
    "text": "13.1 Definitions\nIn the previous chapter, we introduced three classes of extremum estimators — maximum likelihood, GMM, and minimum distance — with examples from each of our prototype models. Now we turn to the statistical theory that governs these estimators. Recall the two key properties we set out to establish:\nThe results in this chapter apply broadly to all extremum estimators, and then we specialize to maximum likelihood, minimum distance, and GMM in turn. Throughout, we use our prototype models to illustrate how the theory translates into practice.\nWe begin by formally defining the classes of estimators we will study. The broadest class is the extremum estimator.\nThis is a very broad definition. All of the estimators we encountered in the previous chapter fall into this class. What distinguishes them is the structure of \\(Q_{N}\\).",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Asymptotic Theory</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_theory.html#consistency",
    "href": "lectures/extremum_theory.html#consistency",
    "title": "13  Asymptotic Theory",
    "section": "13.2 Consistency",
    "text": "13.2 Consistency\nAn extremum estimator solves \\(\\hat{\\theta} = \\arg\\max_{\\theta\\in\\Theta}Q_{N}(\\theta)\\). Let \\(Q_{0}(\\theta)\\) denote the population analogue: the probability limit of \\(Q_{N}(\\theta)\\).\nWhen can we guarantee that \\(\\hat{\\theta}\\rightarrow_{p}\\theta_{0}\\)? Intuitively, two conditions are needed:\n\nIdentification: The population objective \\(Q_{0}(\\theta)\\) must be uniquely maximized at \\(\\theta_{0}\\). If there were multiple maximizers, convergence of \\(Q_{N}\\) to \\(Q_{0}\\) would not pin down which one \\(\\hat{\\theta}\\) approaches.\nConvergence: \\(Q_{N}(\\theta)\\) must converge to \\(Q_{0}(\\theta)\\) in a sufficiently strong sense that the maximizer of \\(Q_{N}\\) tracks the maximizer of \\(Q_{0}\\).\n\nThese two conditions are the backbone of every consistency argument. The precise form of the convergence condition depends on the structure of the problem.\n\nTheorem 13.1 (Consistency with Compactness) Suppose the following conditions hold:\n\n\\(\\Theta\\) is a compact subset of \\(\\mathbb{R}^{p}\\)\n\\(Q_{N}(\\theta)\\) is continuous in \\(\\theta\\) for all realizations of the data\n\\(Q_{N}(\\theta)\\) is a measurable function of the data for all \\(\\theta\\in\\Theta\\)\n\nand additionally:\n\nIdentification: \\(Q_{0}(\\theta)\\) is uniquely maximized at \\(\\theta_{0}\\in\\Theta\\)\nUniform Convergence: \\(\\sup_{\\theta\\in\\Theta}|Q_{N}(\\theta)-Q_{0}(\\theta)|\\rightarrow_{p}0\\)\n\nThen \\(\\hat{\\theta}\\rightarrow_{p}\\theta_{0}\\).\n\n\n\n\n\n\n\nProof Sketch\n\n\n\n\n\nThe idea is straightforward. Pick any open neighborhood \\(\\mathcal{N}\\) around \\(\\theta_{0}\\). We want to show that \\(\\hat{\\theta}\\in\\mathcal{N}\\) with probability approaching 1.\nSince \\(\\theta_{0}\\) uniquely maximizes \\(Q_{0}\\) and \\(\\Theta\\setminus\\mathcal{N}\\) is compact (closed subset of a compact set), there exists a gap: \\[\\varepsilon = Q_{0}(\\theta_{0}) - \\sup_{\\theta\\in\\Theta\\setminus\\mathcal{N}}Q_{0}(\\theta) &gt; 0\\]\nNow, by uniform convergence: \\[Q_{N}(\\hat{\\theta}) \\geq Q_{N}(\\theta_{0}) \\geq Q_{0}(\\theta_{0}) - \\varepsilon/2\\] with probability approaching 1. At the same time, for any \\(\\theta\\in\\Theta\\setminus\\mathcal{N}\\): \\[Q_{N}(\\theta) \\leq Q_{0}(\\theta) + \\varepsilon/2 \\leq Q_{0}(\\theta_{0}) - \\varepsilon/2\\] also with probability approaching 1. Thus \\(\\hat{\\theta}\\) cannot lie outside \\(\\mathcal{N}\\), and since \\(\\mathcal{N}\\) was arbitrary, \\(\\hat{\\theta}\\rightarrow_{p}\\theta_{0}\\).\n\n\n\nCompactness is a strong assumption. Many parameter spaces of interest are not bounded (e.g. regression coefficients). The following result relaxes compactness at the cost of requiring concavity.\n\nTheorem 13.2 (Consistency without Compactness) Suppose the following conditions hold:\n\n\\(\\theta_{0}\\in\\text{int}(\\Theta)\\)\n\\(Q_{N}(\\theta)\\) is concave in \\(\\theta\\) for all realizations of the data\n\\(Q_{N}(\\theta)\\) is a measurable function of the data for all \\(\\theta\\in\\Theta\\)\n\nand additionally:\n\nIdentification: \\(Q_{0}(\\theta)\\) is uniquely maximized at \\(\\theta_{0}\\in\\Theta\\)\nPointwise Convergence: \\(Q_{N}(\\theta)\\rightarrow_{p}Q_{0}(\\theta)\\) for all \\(\\theta\\in\\Theta\\)\n\nThen \\(\\hat{\\theta}\\rightarrow_{p}\\theta_{0}\\).\n\nThe key insight is that concavity turns pointwise convergence into uniform convergence on compact subsets (this follows from a result in convex analysis due to Rockafellar, 1970). Combined with the fact that \\(\\theta_{0}\\) is an interior point, one can construct a compact set around \\(\\theta_{0}\\) that traps \\(\\hat{\\theta}\\) with probability approaching 1 and then apply the logic of Theorem 13.1.\n\n13.2.1 Uniform Convergence\nCondition (b) of Theorem 13.1 requires uniform convergence of \\(Q_{N}\\) to \\(Q_{0}\\). This is stronger than pointwise convergence and deserves some attention. For M-estimators of the form \\(Q_{N}(\\theta)=\\frac{1}{N}\\sum_{n=1}^{N}m(\\mathbf{w}_{n},\\theta)\\), the question reduces to asking for a uniform law of large numbers. The following result provides simple sufficient conditions.\n\nTheorem 13.3 (Uniform Law of Large Numbers) Suppose that \\(\\{\\mathbf{w}_{n}\\}_{n=1}^{N}\\) is an ergodic stationary sequence and:\n\n\\(\\Theta\\) is compact\n\\(m(\\mathbf{w},\\theta)\\) is continuous in \\(\\theta\\) for all \\(\\mathbf{w}\\)\n\\(m(\\mathbf{w},\\theta)\\) is measurable in \\(\\mathbf{w}\\) for all \\(\\theta\\)\nThere exists \\(d(\\mathbf{w})\\) with \\(|m(\\mathbf{w},\\theta)|\\leq d(\\mathbf{w})\\) for all \\(\\theta\\in\\Theta\\) and \\(\\mathbb{E}[d(\\mathbf{w})]&lt;\\infty\\)\n\nThen:\n\n\\(\\sup_{\\theta\\in\\Theta}|Q_{N}(\\theta)-Q_{0}(\\theta)|\\rightarrow_{p}0\\); and\n\\(Q_{0}(\\theta) = \\mathbb{E}[m(\\mathbf{w},\\theta)]\\) is continuous in \\(\\theta\\).\n\n\nIn practice, the dominance condition (4) is verified by checking \\(\\mathbb{E}[\\sup_{\\theta\\in\\Theta}|m(\\mathbf{w},\\theta)|]&lt;\\infty\\). This is straightforward for many common estimators. See Newey and McFadden (1994) for a comprehensive treatment of these results.\n\n\n13.2.2 Consistency of Maximum Likelihood\nFor maximum likelihood, the objective is \\(Q_{N}(\\theta)=\\frac{1}{N}\\sum_{n}^{N}\\log f(\\mathbf{w}_{n};\\theta)\\), and the population analogue is \\(Q_{0}(\\theta) = \\mathbb{E}_{\\theta_{0}}[\\log f(\\mathbf{w};\\theta)]\\). Identification for MLE has an elegant justification through the Kullback-Leibler inequality: for any two densities \\(g\\) and \\(h\\),\n\\[\\mathbb{E}_{g}\\left[\\log\\frac{g(\\mathbf{w})}{h(\\mathbf{w})}\\right]\\geq 0\\]\nwith equality if and only if \\(g=h\\) almost everywhere. Applying this with \\(g(\\cdot) = f(\\cdot;\\theta_{0})\\) and \\(h(\\cdot) = f(\\cdot;\\theta)\\) gives:\n\\[\\mathbb{E}_{\\theta_{0}}[\\log f(\\mathbf{w};\\theta_{0})] \\geq \\mathbb{E}_{\\theta_{0}}[\\log f(\\mathbf{w};\\theta)]\\]\nwith equality if and only if \\(f(\\cdot;\\theta)=f(\\cdot;\\theta_{0})\\) almost everywhere. Thus, as long as different values of \\(\\theta\\) imply different densities (a natural notion of identification for parametric models), the population log-likelihood is uniquely maximized at \\(\\theta_{0}\\).\n\nTheorem 13.4 (Consistency of Maximum Likelihood) Suppose that \\(\\{\\mathbf{w}_{n}\\}\\) is ergodic stationary with density \\(f(\\mathbf{w};\\theta_{0})\\) and that \\(\\theta_{0}\\in\\Theta\\). If:\n\n\\(\\Theta\\) is compact\n\\(\\log f(\\mathbf{w};\\theta)\\) is continuous in \\(\\theta\\)\n\\(f(\\mathbf{w};\\theta_{0})\\neq f(\\mathbf{w};\\theta)\\) with positive probability for all \\(\\theta\\neq\\theta_{0}\\) (identification)\n\\(\\mathbb{E}[\\sup_{\\theta\\in\\Theta}|\\log f(\\mathbf{w};\\theta)|]&lt;\\infty\\) (dominance)\n\nThen \\(\\hat{\\theta}_{ML}\\rightarrow_{p}\\theta_{0}\\).\n\nNotice that identification here takes a model-specific form: we need different parameter values to imply different distributions for the data. This is a consequence of the fact that MLE relies on a fully specified parametric model.\nAn analogous result holds without compactness when the log-likelihood is concave in \\(\\theta\\) (as is often the case for exponential family models), replacing (1) with \\(\\theta_{0}\\in\\text{int}(\\Theta)\\) and (4) with pointwise moment conditions.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Asymptotic Theory</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_theory.html#asymptotic-normality",
    "href": "lectures/extremum_theory.html#asymptotic-normality",
    "title": "13  Asymptotic Theory",
    "section": "13.3 Asymptotic Normality",
    "text": "13.3 Asymptotic Normality\nHaving established when \\(\\hat{\\theta}\\rightarrow_{p}\\theta_{0}\\), we now turn to characterizing the rate and distribution of \\(\\hat{\\theta}\\) around \\(\\theta_{0}\\). The answer will justify the standard errors and confidence intervals that we routinely compute in applied work.\nConsider an M-estimator: \\(Q_{N}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N}m(\\mathbf{w}_{n},\\theta)\\). Define the score (gradient) and Hessian of \\(m\\):\n\\[\\mathbf{s}(\\mathbf{w},\\theta) = \\frac{\\partial m(\\mathbf{w},\\theta)}{\\partial\\theta}\\qquad(p\\times 1)\\]\n\\[\\mathbf{H}(\\mathbf{w},\\theta) = \\frac{\\partial^{2}m(\\mathbf{w},\\theta)}{\\partial\\theta\\partial\\theta'}\\qquad(p\\times p)\\]\n\n13.3.1 Derivation via the Mean Value Theorem\nSince \\(\\hat{\\theta}\\) maximizes \\(Q_{N}\\), the first-order condition gives: \\[\\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{s}(\\mathbf{w}_{n},\\hat{\\theta}) = \\mathbf{0}\\]\nA mean value expansion around \\(\\theta_{0}\\) yields: \\[\\mathbf{0} = \\frac{1}{N}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\theta_{0}) + \\left[\\frac{1}{N}\\sum_{n}\\mathbf{H}(\\mathbf{w}_{n},\\bar{\\theta})\\right](\\hat{\\theta}-\\theta_{0})\\]\nwhere \\(\\bar{\\theta}\\) lies between \\(\\hat{\\theta}\\) and \\(\\theta_{0}\\) (applied row-by-row). Rearranging: \\[\\sqrt{N}(\\hat{\\theta}-\\theta_{0}) = -\\left[\\frac{1}{N}\\sum_{n}\\mathbf{H}(\\mathbf{w}_{n},\\bar{\\theta})\\right]^{-1}\\frac{1}{\\sqrt{N}}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\theta_{0})\\]\nNow apply two standard arguments:\n\nBy the Central Limit Theorem: \\(\\frac{1}{\\sqrt{N}}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Sigma)\\) where \\(\\Sigma = \\mathbb{E}[\\mathbf{s}(\\mathbf{w},\\theta_{0})\\mathbf{s}(\\mathbf{w},\\theta_{0})']\\).\nBy a Law of Large Numbers and continuity: \\(\\frac{1}{N}\\sum_{n}\\mathbf{H}(\\mathbf{w}_{n},\\bar{\\theta})\\rightarrow_{p}\\mathbb{E}[\\mathbf{H}(\\mathbf{w},\\theta_{0})]\\), using that \\(\\bar{\\theta}\\rightarrow_{p}\\theta_{0}\\).\n\nCombining these via Slutsky’s theorem gives the result.\n\nTheorem 13.5 (Asymptotic Normality for M-estimators) Suppose that the consistency conditions hold and additionally:\n\n\\(\\theta_{0}\\in\\text{int}(\\Theta)\\)\n\\(m(\\mathbf{w},\\theta)\\) is twice continuously differentiable in \\(\\theta\\)\n\\(\\frac{1}{\\sqrt{N}}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Sigma)\\) with \\(\\Sigma\\) positive definite\n\\(\\mathbb{E}[\\sup_{\\theta\\in\\mathcal{N}(\\theta_{0})}\\|\\mathbf{H}(\\mathbf{w},\\theta)\\|]&lt;\\infty\\) for some neighborhood \\(\\mathcal{N}(\\theta_{0})\\)\n\\(\\mathbb{E}[\\mathbf{H}(\\mathbf{w},\\theta_{0})]\\) is nonsingular\n\nThen: \\[\\sqrt{N}(\\hat{\\theta}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}\\left(\\mathbf{0},\\ \\mathbb{E}[\\mathbf{H}]^{-1}\\Sigma\\mathbb{E}[\\mathbf{H}]^{-1}\\right)\\]\nwhere \\(\\mathbb{E}[\\mathbf{H}]=\\mathbb{E}[\\mathbf{H}(\\mathbf{w},\\theta_{0})]\\) and \\(\\Sigma = \\mathbb{E}[\\mathbf{s}(\\mathbf{w},\\theta_{0})\\mathbf{s}(\\mathbf{w},\\theta_{0})']\\).\n\nThe asymptotic variance \\(\\mathbb{E}[\\mathbf{H}]^{-1}\\Sigma\\mathbb{E}[\\mathbf{H}]^{-1}\\) is often called the sandwich formula. In practice, we replace the population expectations with sample analogues: \\[\\widehat{\\mathbb{V}}[\\hat{\\theta}] = \\hat{H}^{-1}\\hat{\\Sigma}\\hat{H}^{-1}/N\\] where \\(\\hat{H} = \\frac{1}{N}\\sum_{n}\\mathbf{H}(\\mathbf{w}_{n},\\hat{\\theta})\\) and \\(\\hat{\\Sigma} = \\frac{1}{N}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\hat{\\theta})\\mathbf{s}(\\mathbf{w}_{n},\\hat{\\theta})'\\).\n\n\n13.3.2 The Information Matrix Equality\nFor maximum likelihood, \\(m(\\mathbf{w},\\theta)=\\log f(\\mathbf{w};\\theta)\\), and a remarkable simplification occurs. Under standard regularity conditions, the information matrix equality holds:\n\\[\\mathcal{I}(\\theta_{0}) \\equiv \\mathbb{E}\\left[\\mathbf{s}(\\mathbf{w},\\theta_{0})\\mathbf{s}(\\mathbf{w},\\theta_{0})'\\right] = -\\mathbb{E}\\left[\\mathbf{H}(\\mathbf{w},\\theta_{0})\\right]\\]\n\n\n\n\n\n\nWhy does this hold?\n\n\n\n\n\nSince \\(\\int f(\\mathbf{w};\\theta)d\\mathbf{w}=1\\) for all \\(\\theta\\), differentiating under the integral sign with respect to \\(\\theta\\) gives: \\[\\int\\frac{\\partial f(\\mathbf{w};\\theta)}{\\partial\\theta}d\\mathbf{w} = 0\\] which is \\(\\mathbb{E}_{\\theta}[\\mathbf{s}(\\mathbf{w},\\theta)]=0\\). Differentiating again: \\[\\int\\frac{\\partial^{2}\\log f}{\\partial\\theta\\partial\\theta'}f\\ d\\mathbf{w} + \\int\\frac{\\partial\\log f}{\\partial\\theta}\\frac{\\partial\\log f}{\\partial\\theta'}f\\ d\\mathbf{w} = 0\\] which yields \\(\\mathbb{E}[\\mathbf{H}]+\\mathbb{E}[\\mathbf{s}\\mathbf{s}'] = 0\\), i.e. \\(\\Sigma = -\\mathbb{E}[\\mathbf{H}]\\).\n\n\n\nThis means that for MLE, the sandwich formula collapses to: \\[\\sqrt{N}(\\hat{\\theta}_{ML}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}\\left(\\mathbf{0},\\ \\mathcal{I}(\\theta_{0})^{-1}\\right)\\]\nThe matrix \\(\\mathcal{I}(\\theta)\\) is called the Fisher information matrix. The MLE variance can be estimated using either the Hessian or the outer product of the score — or indeed the sandwich (which is robust to certain forms of misspecification).\n\n\n\n\n\n\nExample: Probit Standard Errors\n\n\n\n\n\n\nExample 13.1 Let’s illustrate the asymptotic variance formula for the probit model from the Generalized Roy Model (Example 7.1). The probit log-likelihood for a single observation is:\n\\[m(\\mathbf{w}_{n},\\gamma) = D_{n}\\log\\Phi(\\mathbf{w}_{n}\\gamma) + (1-D_{n})\\log(1-\\Phi(\\mathbf{w}_{n}\\gamma))\\]\nwhere \\(\\mathbf{w}_{n} = [1,X_{n},Z_{n}]\\). Rather than deriving the Hessian and score analytically, we can use automatic differentiation — a tool covered in more detail in the appendix.\n\nusing Distributions, Optim, Random, ForwardDiff, LinearAlgebra, Plots\nfunction sim_data(γ,β0,β1,N ; ρ_0 = 0.3, ρ_1 = -0.3)\n    X = rand(Normal(),N)\n    Z = rand(Normal(),N)\n    v = rand(Normal(),N)\n    U0 = rand(Normal(),N) .+ ρ_0.*v\n    U1 = rand(Normal(),N) .+ ρ_1.*v\n    D = (γ[1] .+ γ[2]*X .+ γ[3]*Z .- v) .&gt; 0\n    Y = β0[1] .+ β0[2].*X .+ U0\n    Y1 = β1[1] .+ β1[2].*X .+ U1\n    Y[D.==1] .= Y1[D.==1]\n    return (;X,Z,Y,D)\nend\n\nfunction log_likelihood(γ,data)\n    (;D,X,Z) = data\n    ll = 0.\n    Fv = Normal()\n    for n in eachindex(D)\n        xg = γ[1] + γ[2]*X[n] + γ[3]*Z[n]\n        if D[n] == 1\n            ll += log(cdf(Fv,xg))\n        else\n            ll += log(1-cdf(Fv,xg))\n        end\n    end\n    return ll / length(D)\nend\n\nfunction estimate_probit(data)\n    res = optimize(x-&gt;-log_likelihood(x,data),zeros(3),Newton(),autodiff=:forward)\n    return res.minimizer\nend\n\nestimate_probit (generic function with 1 method)\n\n\nNow let’s estimate the probit model on a single dataset and compute standard errors using the information matrix. We use ForwardDiff to compute the score and Hessian numerically.\n\ngamma = [0., 0.5, 0.5]\nbeta0 = [0., 0.3]\nbeta1 = [0., 0.5]\nRandom.seed!(123)\ndata = sim_data(gamma, beta0, beta1, 5000)\nγ_hat = estimate_probit(data)\n\n# Compute score for each observation\nfunction score_n(n, γ, data)\n    function ll_n(g)\n        xg = g[1] + g[2]*data.X[n] + g[3]*data.Z[n]\n        if data.D[n] == 1\n            return log(cdf(Normal(),xg))\n        else\n            return log(1-cdf(Normal(),xg))\n        end\n    end\n    return ForwardDiff.gradient(ll_n, γ)\nend\n\nN = length(data.D)\n\n# Outer product of scores (estimate of Fisher information)\nΣ_hat = zeros(3,3)\nfor n in 1:N\n    s = score_n(n, γ_hat, data)\n    Σ_hat += s * s'\nend\nΣ_hat ./= N\n\n# Hessian of average log-likelihood\nH_hat = ForwardDiff.hessian(g -&gt; log_likelihood(g, data), γ_hat)\n\n# Asymptotic variance (three ways)\nV_sandwich = inv(H_hat) * Σ_hat * inv(H_hat) / N\nV_hessian = -inv(H_hat) / N\nV_opg = inv(Σ_hat) / N\n\nse_sandwich = sqrt.(diag(V_sandwich))\nse_hessian = sqrt.(diag(V_hessian))\nse_opg = sqrt.(diag(V_opg))\nprintln(\"Estimates: $(round.(γ_hat,digits=3))\")\nprintln(\"SE (sandwich): $(round.(se_sandwich,digits=4))\")\nprintln(\"SE (Hessian):  $(round.(se_hessian,digits=4))\")\nprintln(\"SE (OPG):      $(round.(se_opg,digits=4))\")\n\nEstimates: [-0.007, 0.473, 0.475]\nSE (sandwich): [0.019, 0.0212, 0.0209]\nSE (Hessian):  [0.019, 0.0209, 0.0207]\nSE (OPG):      [0.0191, 0.0205, 0.0206]\n\n\nLet’s verify these asymptotic standard errors against a Monte Carlo simulation. If the asymptotic theory is working, the standard deviation of the Monte Carlo estimates should be close to the asymptotic SE.\n\ngamma_mc = mapreduce(vcat, 1:500) do b\n    d = sim_data(gamma, beta0, beta1, 5000)\n    estimate_probit(d)'\nend\n\nse_mc = std.(eachcol(gamma_mc))\nprintln(\"SE (Monte Carlo): $(round.(se_mc,digits=4))\")\nprintln(\"SE (asymptotic):  $(round.(se_hessian,digits=4))\")\n\nSE (Monte Carlo): [0.0209, 0.0204, 0.0207]\nSE (asymptotic):  [0.019, 0.0209, 0.0207]\n\n\nWe can also plot the Monte Carlo distribution against the asymptotic normal approximation.\n\npl = [begin\n    histogram(gamma_mc[:,j], normalize=:pdf, label=false, alpha=0.5)\n    xgrid = range(extrema(gamma_mc[:,j])..., length=100)\n    plot!(xgrid, pdf.(Normal(gamma[j], se_hessian[j]), xgrid),\n          linewidth=2, label=\"Asymptotic Normal\", color=\"red\")\n    plot!(title = \"γ_$(j)\")\nend for j in 1:3]\nplot(pl..., layout=(1,3), size=(900,300))\n\n\n\n\n\n\n\n\n\n\n13.3.3 The Delta Method\nIn many settings, the parameters of direct interest are not \\(\\theta\\) itself but some smooth function \\(g(\\theta)\\). For instance, in Example 12.1 the search model is parameterized by \\(\\theta=(h,\\delta,\\mu,\\sigma,w^{*})\\) but the economically meaningful objects include \\(\\lambda\\) and \\(b\\), which are nonlinear functions of \\(\\theta\\). The delta method provides the asymptotic distribution of such transformed parameters.\n\nTheorem 13.6 (Delta Method) If \\(\\sqrt{N}(\\hat{\\theta}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},V)\\) and \\(g:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}^{k}\\) is continuously differentiable at \\(\\theta_{0}\\) with Jacobian \\(\\nabla g(\\theta_{0})\\) of full row rank, then:\n\\[\\sqrt{N}(g(\\hat{\\theta})-g(\\theta_{0}))\\rightarrow_{d}\\mathcal{N}\\left(\\mathbf{0},\\ \\nabla g(\\theta_{0})\\ V\\ \\nabla g(\\theta_{0})'\\right)\\]\n\nThe proof is a direct application of the continuous mapping theorem to the first-order Taylor expansion \\(g(\\hat{\\theta})\\approx g(\\theta_{0})+\\nabla g(\\theta_{0})(\\hat{\\theta}-\\theta_{0})\\).\nIn practice, the Jacobian \\(\\nabla g\\) can be computed analytically or by automatic differentiation. This is convenient when \\(g\\) is a complex function — for instance, when it involves solving the model as in the case of the reservation wage \\(w^{*}\\) in the search model.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Asymptotic Theory</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_theory.html#efficiency",
    "href": "lectures/extremum_theory.html#efficiency",
    "title": "13  Asymptotic Theory",
    "section": "13.4 Efficiency",
    "text": "13.4 Efficiency\nGiven two consistent, asymptotically normal estimators \\(\\hat{\\theta}_{1}\\) and \\(\\hat{\\theta}_{2}\\), we say \\(\\hat{\\theta}_{1}\\) is asymptotically efficient relative to \\(\\hat{\\theta}_{2}\\) if \\(V_{2}-V_{1}\\) is positive semi-definite, where \\(V_{j}\\) is the asymptotic variance of \\(\\hat{\\theta}_{j}\\). A natural question is: among the class of estimators we have discussed, is there a “best” one?\n\n13.4.1 Efficiency of Maximum Likelihood\nThe answer is yes, under the assumption that the model is correctly specified. The MLE achieves the Cramér-Rao lower bound: for any consistent, asymptotically normal estimator \\(\\hat{\\theta}\\) based on the likelihood,\n\\[V[\\hat{\\theta}] - \\mathcal{I}(\\theta_{0})^{-1}\\geq 0\\]\nin the positive semi-definite sense. Since the MLE has asymptotic variance \\(\\mathcal{I}(\\theta_{0})^{-1}\\), no other estimator in this class can do better.\nMore concretely, one can show that MLE is efficient relative to any GMM estimator that uses moment conditions implied by the model. The argument proceeds by showing that for any GMM estimator with moments \\(g(\\mathbf{w},\\theta)\\), the difference in asymptotic variances is:\n\\[V_{GMM} - V_{MLE} = \\mathbb{E}[\\mathbf{m}\\mathbf{s}']^{-1}\\mathbb{E}[\\mathbf{U}\\mathbf{U}']\\mathbb{E}[\\mathbf{s}\\mathbf{m}']^{-1}\\geq 0\\]\nwhere \\(\\mathbf{m}\\) is the influence function of the GMM estimator and \\(\\mathbf{U} = \\mathbf{m}-\\mathbb{E}[\\mathbf{m}\\mathbf{s}']\\mathbb{E}[\\mathbf{s}\\mathbf{s}']^{-1}\\mathbf{s}\\) is the projection residual of \\(\\mathbf{m}\\) on \\(\\mathbf{s}\\). This is non-negative by construction, and equals zero only when \\(\\mathbf{m}\\) is a linear function of \\(\\mathbf{s}\\) — i.e. when the GMM estimator fully exploits the likelihood.\n\n\n\n\n\n\nDiscussion: Efficiency vs. Robustness\n\n\n\nThe efficiency of MLE comes at a price: it requires the entire parametric model to be correctly specified. If the density \\(f(\\mathbf{w};\\theta)\\) is wrong — even slightly — the MLE may still converge, but it will converge to a pseudo-true value that minimizes the Kullback-Leibler divergence to the truth, and the information matrix equality will fail. In contrast, GMM only requires the moment conditions to be correct, making it more robust to partial misspecification. The sandwich variance estimator \\(\\hat{H}^{-1}\\hat{\\Sigma}\\hat{H}^{-1}\\) remains valid for MLE even under misspecification, which is why it is sometimes preferred in practice.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Asymptotic Theory</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_theory.html#minimum-distance-estimators",
    "href": "lectures/extremum_theory.html#minimum-distance-estimators",
    "title": "13  Asymptotic Theory",
    "section": "13.5 Minimum Distance Estimators",
    "text": "13.5 Minimum Distance Estimators\nThe minimum distance estimator takes a different approach from the M-estimators discussed above. Instead of directly optimizing an objective over the raw data, it works in two stages: first estimate a reduced-form object \\(\\pi\\), then find the structural parameters \\(\\theta\\) that best fit the model’s implications for \\(\\pi\\).\n\n13.5.1 Setup\nLet \\(\\psi(\\pi,\\theta)\\) be a vector of \\(J\\) model restrictions satisfying \\(\\psi(\\pi_{0},\\theta_{0})=\\mathbf{0}\\). For example, in the savings model from Example 12.2, \\(\\pi\\) consists of the variances of log income at each age, and \\(\\psi(\\pi,\\theta)=\\pi - \\mathbf{v}(\\theta)\\) measures the gap between observed and model-implied moments.\nSuppose we have a first-stage estimator \\(\\hat{\\pi}\\) with: \\[\\sqrt{N}(\\hat{\\pi}-\\pi_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Omega)\\]\nThe minimum distance estimator is: \\[\\hat{\\theta} = \\arg\\min_{\\theta}\\psi(\\hat{\\pi},\\theta)'\\mathbf{W}_{N}\\psi(\\hat{\\pi},\\theta)\\] where \\(\\mathbf{W}_{N}\\) is a positive definite weighting matrix.\n\n\n13.5.2 Asymptotic Distribution\n\nTheorem 13.7 (Asymptotics for Minimum Distance) Suppose:\n\n\\(\\psi(\\pi_{0},\\theta_{0})=\\mathbf{0}\\) and \\(\\psi(\\pi_{0},\\theta)\\neq\\mathbf{0}\\) for all \\(\\theta\\neq\\theta_{0}\\) (identification)\n\\(\\sqrt{N}(\\hat{\\pi}-\\pi_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Omega)\\)\n\\(\\mathbf{W}_{N}\\rightarrow_{p}\\mathbf{W}\\), symmetric and nonsingular\n\\(\\psi\\) is differentiable with \\(\\text{rank}(\\nabla_{\\theta}\\psi_{0})=p\\)\n\nDefine \\(\\nabla_{\\theta}\\psi_{0} = \\frac{\\partial\\psi(\\pi_{0},\\theta_{0})'}{\\partial\\theta}\\) and \\(\\nabla_{\\pi}\\psi_{0}=\\frac{\\partial\\psi(\\pi_{0},\\theta_{0})'}{\\partial\\pi}\\). Then:\n\\[\\sqrt{N}(\\hat{\\theta}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\ V_{MD})\\] where: \\[V_{MD} = \\left(\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\theta}\\psi_{0}'\\right)^{-1}\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\pi}\\psi_{0}'\\Omega\\nabla_{\\pi}\\psi_{0}\\mathbf{W}\\nabla_{\\theta}\\psi_{0}'\\left(\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\theta}\\psi_{0}'\\right)^{-1}\\]\n\n\n\n\n\n\n\nDerivation Sketch\n\n\n\n\n\nThe first-order condition of the minimum distance problem is: \\[\\nabla_{\\theta}\\psi(\\hat{\\pi},\\hat{\\theta})\\mathbf{W}_{N}\\psi(\\hat{\\pi},\\hat{\\theta}) = \\mathbf{0}\\]\nExpanding \\(\\psi(\\hat{\\pi},\\hat{\\theta})\\) around \\((\\pi_{0},\\theta_{0})\\): \\[\\psi(\\hat{\\pi},\\hat{\\theta})\\approx \\nabla_{\\pi}\\psi_{0}'(\\hat{\\pi}-\\pi_{0}) + \\nabla_{\\theta}\\psi_{0}'(\\hat{\\theta}-\\theta_{0})\\]\nSubstituting and solving: \\[\\sqrt{N}(\\hat{\\theta}-\\theta_{0})\\approx -(\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\theta}\\psi_{0}')^{-1}\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\pi}\\psi_{0}'\\sqrt{N}(\\hat{\\pi}-\\pi_{0})\\]\nThe result follows from the asymptotic distribution of \\(\\hat{\\pi}\\).\n\n\n\n\n\n13.5.3 The Optimal Weighting Matrix\nThe variance \\(V_{MD}\\) depends on the choice of \\(\\mathbf{W}\\). The optimal weighting matrix minimizes \\(V_{MD}\\) (in the positive semi-definite sense) and is given by:\n\\[\\mathbf{W}^{*} = \\left(\\nabla_{\\pi}\\psi_{0}'\\Omega\\nabla_{\\pi}\\psi_{0}\\right)^{-1}\\]\nUnder this choice, the variance simplifies to: \\[V_{MD}^{*} = \\left(\\nabla_{\\theta}\\psi_{0}\\left(\\nabla_{\\pi}\\psi_{0}'\\Omega\\nabla_{\\pi}\\psi_{0}\\right)^{-1}\\nabla_{\\theta}\\psi_{0}'\\right)^{-1}\\]\nIn the common case where \\(\\psi(\\pi,\\theta) = \\pi-h(\\theta)\\) for some function \\(h\\), the derivatives simplify: \\(\\nabla_{\\pi}\\psi_{0} = I\\) and \\(\\nabla_{\\theta}\\psi_{0} = -\\nabla_{\\theta}h(\\theta_{0})\\). Then \\(\\mathbf{W}^{*}=\\Omega^{-1}\\) and:\n\\[V_{MD}^{*} = \\left(\\nabla_{\\theta}h_{0}\\Omega^{-1}\\nabla_{\\theta}h_{0}'\\right)^{-1}\\]\nAn important special case arises when the model is just-identified: \\(\\text{dim}(\\psi)=\\text{dim}(\\theta)\\). In this case, one can show using the implicit function theorem that the optimally weighted minimum distance estimator achieves the same asymptotic variance as MLE. Over-identification (more moments than parameters) necessarily introduces some loss relative to MLE but gains robustness.\n\n\n\n\n\n\nExample: Standard Errors for the Income Process\n\n\n\n\n\n\nExample 13.2 Let’s extend the minimum distance estimation from Example 12.2 to compute standard errors for the income process parameters. Recall that we matched the variance of log income at each age to the model-implied variances.\nSince our restrictions take the form \\(\\psi(\\pi,\\theta) = \\pi - \\mathbf{v}(\\theta)\\), we have \\(\\nabla_{\\pi}\\psi=I\\) and \\(\\nabla_{\\theta}\\psi = -\\nabla_{\\theta}\\mathbf{v}\\). Using the identity weighting matrix, the asymptotic variance is:\n\\[V_{MD} = (\\nabla_{\\theta}\\mathbf{v}'\\nabla_{\\theta}\\mathbf{v})^{-1}\\nabla_{\\theta}\\mathbf{v}'\\Omega\\nabla_{\\theta}\\mathbf{v}(\\nabla_{\\theta}\\mathbf{v}'\\nabla_{\\theta}\\mathbf{v})^{-1}\\]\nwhere \\(\\Omega\\) is the variance-covariance matrix of the sample variance estimates \\(\\hat{\\pi}\\).\n\nusing CSV, DataFrames, DataFramesMeta, Statistics, Optim, ForwardDiff, LinearAlgebra, Plots\n\n# Load and prepare data\ndata_psid = @chain begin\n    CSV.read(\"../data/abb_aea_data.csv\",DataFrame,missingstring = \"NA\")\n    @select :person :y :tot_assets1 :asset :age :year\n    @subset :age.&gt;=25 :age.&lt;=64\nend\n\n# Calculate sample variances by age\nmoments_df = @chain data_psid begin\n    groupby(:age)\n    @combine begin\n        :var_logy = var(log.(:y))\n        :n = length(:y)\n    end\n    @orderby :age\nend\nm_hat = moments_df.var_logy\nn_age = moments_df.n\n\n# Model-implied moments\nfunction model_moments(θ, T)\n    ρ, σ2_α, σ2_η = θ\n    m = [σ2_α + (1-ρ^(2(t-1)))/(1-ρ^2) * σ2_η for t in 1:T]\n    return m\nend\n\n# Transform parameters to enforce constraints\nfunction transform(x)\n    ρ = tanh(x[1])\n    σ2_α = exp(x[2])\n    σ2_η = exp(x[3])\n    return [ρ, σ2_α, σ2_η]\nend\n\n# Minimum distance objective (identity weight)\nfunction md_objective(x, m_hat)\n    θ = transform(x)\n    T = length(m_hat)\n    m_model = model_moments(θ, T)\n    diff = m_hat .- m_model\n    return diff' * diff\nend\n\n# Estimate\nx0 = [0.5, log(0.1), log(0.05)]\nres = optimize(x -&gt; md_objective(x, m_hat), x0, Newton(), autodiff = :forward)\nx_hat = res.minimizer\nθ_hat = transform(x_hat)\n\n3-element Vector{Float64}:\n 0.9180604516854505\n 0.27854304267515606\n 0.08522314351469629\n\n\nNow we compute standard errors. We need \\(\\nabla_{\\theta}\\mathbf{v}\\) (the Jacobian of model moments with respect to parameters) and \\(\\Omega\\) (the variance of sample moments). We compute the Jacobian with ForwardDiff and use the delta method to account for the parameter transformation.\n\nT = length(m_hat)\n\n# Jacobian of model moments w.r.t. θ = (ρ, σ²_α, σ²_η)\n∇v = ForwardDiff.jacobian(θ -&gt; model_moments(θ, T), θ_hat)\n\n# Jacobian of transform (for delta method through the transformation)\n∇t = ForwardDiff.jacobian(transform, x_hat)\n\n# Estimate Ω: for variances, Var(σ̂²) ≈ 2σ⁴/(n-1) under normality\n# A simple approximation using sample sizes at each age\nΩ = Diagonal(2 .* m_hat.^2 ./ (n_age .- 1))\n\n# Total sample size (use average n per age as approximation)\nN_eff = Int(round(mean(n_age)))\n\n# Asymptotic variance of θ̂ (identity weighting)\nG = ∇v  # J × p Jacobian\nV_md = inv(G' * G) * G' * Ω * G * inv(G' * G) / N_eff\n\n# Standard errors\nse = sqrt.(diag(V_md))\n\nprintln(\"Minimum Distance Estimates with Standard Errors:\")\nprintln(\"  ρ     = $(round(θ_hat[1], digits=3))  ($(round(se[1], digits=4)))\")\nprintln(\"  σ²_α  = $(round(θ_hat[2], digits=3))  ($(round(se[2], digits=4)))\")\nprintln(\"  σ²_η  = $(round(θ_hat[3], digits=3))  ($(round(se[3], digits=4)))\")\n\nMinimum Distance Estimates with Standard Errors:\n  ρ     = 0.918  (0.0005)\n  σ²_α  = 0.279  (0.0009)\n  σ²_η  = 0.085  (0.0005)\n\n\nWe can also compute the standard errors under the optimal weighting matrix and compare:\n\n# Optimal weighting\nW_opt = inv(Ω)\nV_md_opt = inv(G' * W_opt * G) / N_eff\nse_opt = sqrt.(diag(V_md_opt))\n\n# Re-estimate with optimal weighting\nfunction md_objective_opt(x, m_hat, W)\n    θ = transform(x)\n    T = length(m_hat)\n    m_model = model_moments(θ, T)\n    diff = m_hat .- m_model\n    return diff' * W * diff\nend\n\nres_opt = optimize(x -&gt; md_objective_opt(x, m_hat, W_opt), x0, Newton(), autodiff = :forward)\nθ_hat_opt = transform(res_opt.minimizer)\n\n# Recompute Jacobian at new estimates\n∇v_opt = ForwardDiff.jacobian(θ -&gt; model_moments(θ, T), θ_hat_opt)\nV_md_opt2 = inv(∇v_opt' * W_opt * ∇v_opt) / N_eff\nse_opt2 = sqrt.(diag(V_md_opt2))\n\nprintln(\"\\nOptimally Weighted Estimates:\")\nprintln(\"  ρ     = $(round(θ_hat_opt[1], digits=3))  ($(round(se_opt2[1], digits=4)))\")\nprintln(\"  σ²_α  = $(round(θ_hat_opt[2], digits=3))  ($(round(se_opt2[2], digits=4)))\")\nprintln(\"  σ²_η  = $(round(θ_hat_opt[3], digits=3))  ($(round(se_opt2[3], digits=4)))\")\n\n\nOptimally Weighted Estimates:\n  ρ     = 0.922  (0.0004)\n  σ²_α  = 0.271  (0.0008)\n  σ²_η  = 0.062  (0.0003)\n\n\nThe optimally weighted estimator should be at least as precise, and is often substantially more so when the moment conditions have very different scales or variances.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Asymptotic Theory</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_theory.html#the-generalized-method-of-moments",
    "href": "lectures/extremum_theory.html#the-generalized-method-of-moments",
    "title": "13  Asymptotic Theory",
    "section": "13.6 The Generalized Method of Moments",
    "text": "13.6 The Generalized Method of Moments\nGMM is an extremum estimator with objective: \\[Q_{N}(\\theta) = -\\frac{1}{2}\\mathbf{g}_{N}(\\theta)'\\mathbf{W}_{N}\\mathbf{g}_{N}(\\theta),\\qquad\\mathbf{g}_{N}(\\theta)=\\frac{1}{N}\\sum_{n}g(\\mathbf{w}_{n},\\theta)\\]\nwhere \\(\\mathbb{E}[g(\\mathbf{w},\\theta_{0})]=\\mathbf{0}\\) are the moment conditions. The asymptotic distribution follows from Theorem 13.5 as a special case, but the structure of the problem leads to a particularly clean expression.\n\nTheorem 13.8 (Asymptotic Distribution of GMM) Suppose that the standard regularity conditions hold and \\(\\mathbf{W}_{N}\\rightarrow_{p}\\mathbf{W}\\). Let \\(G=\\mathbb{E}[\\nabla_{\\theta}g(\\mathbf{w},\\theta_{0})']\\) and \\(S=\\mathbb{E}[g(\\mathbf{w},\\theta_{0})g(\\mathbf{w},\\theta_{0})']\\). Then:\n\\[\\sqrt{N}(\\hat{\\theta}_{GMM}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}\\left(\\mathbf{0},\\ (G'\\mathbf{W}G)^{-1}G'\\mathbf{W}S\\mathbf{W}G(G'\\mathbf{W}G)^{-1}\\right)\\]\n\n\n13.6.1 The Optimal Weighting Matrix\nAs with minimum distance, the asymptotic variance depends on the choice of \\(\\mathbf{W}\\). The optimal weighting matrix is:\n\\[\\mathbf{W}^{*}=S^{-1} = \\left(\\mathbb{E}[g(\\mathbf{w},\\theta_{0})g(\\mathbf{w},\\theta_{0})']\\right)^{-1}\\]\nUnder this choice, the variance simplifies to:\n\\[V_{GMM}^{*} = (G'S^{-1}G)^{-1}\\]\nWhen the model is just-identified (number of moments equals number of parameters), the GMM estimator does not depend on \\(\\mathbf{W}\\) at all. This is because the sample moments \\(\\mathbf{g}_{N}(\\hat{\\theta})=\\mathbf{0}\\) are set exactly to zero, regardless of the weighting.\n\n\n13.6.2 Feasible Efficient GMM\nIn practice, \\(S\\) depends on \\(\\theta_{0}\\) and must be estimated. A common approach is two-step GMM:\n\nEstimate \\(\\hat{\\theta}_{1}\\) using some initial weighting matrix (e.g. \\(\\mathbf{W}=I\\)).\nCompute \\(\\hat{S}=\\frac{1}{N}\\sum_{n}g(\\mathbf{w}_{n},\\hat{\\theta}_{1})g(\\mathbf{w}_{n},\\hat{\\theta}_{1})'\\).\nRe-estimate: \\(\\hat{\\theta}_{2} = \\arg\\min_{\\theta}\\mathbf{g}_{N}(\\theta)'\\hat{S}^{-1}\\mathbf{g}_{N}(\\theta)\\).\n\nThe resulting estimator \\(\\hat{\\theta}_{2}\\) is asymptotically efficient. The first-stage estimation of \\(\\hat{S}\\) does not affect the asymptotic variance because \\(\\hat{S}\\rightarrow_{p}S\\) under standard conditions, and the weighting matrix appears in the asymptotic variance only through its probability limit.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Asymptotic Theory</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_theory.html#two-step-estimators",
    "href": "lectures/extremum_theory.html#two-step-estimators",
    "title": "13  Asymptotic Theory",
    "section": "13.7 Two-Step Estimators",
    "text": "13.7 Two-Step Estimators\nMany structural estimators proceed in stages. In Example 7.1, we first estimated the selection equation by MLE and then used these estimates in a second-stage OLS regression. In the search model, we might first estimate the wage distribution and then back out the reservation wage. The theory of two-step estimators formalizes the effect of first-stage estimation uncertainty on second-stage inference.\n\n13.7.1 Setup\nSuppose the estimator is defined by two sets of moment conditions:\n\nFirst step: Estimate \\(\\hat{\\gamma}\\) via \\(\\frac{1}{N}\\sum_{n}g_{1}(\\mathbf{w}_{n},\\hat{\\gamma})=\\mathbf{0}\\)\nSecond step: Estimate \\(\\hat{\\beta}\\) via \\(\\frac{1}{N}\\sum_{n}g_{2}(\\mathbf{w}_{n},\\hat{\\gamma},\\hat{\\beta})=\\mathbf{0}\\)\n\nThe key feature is that the second step depends on the first-step estimates.\n\n\n13.7.2 Asymptotic Distribution\nTo derive the joint distribution, stack the moment conditions. Let \\(\\alpha=(\\gamma',\\beta')'\\) and write the full system as: \\[\\frac{1}{N}\\sum_{n}\\begin{bmatrix}g_{1}(\\mathbf{w}_{n},\\gamma)\\\\ g_{2}(\\mathbf{w}_{n},\\gamma,\\beta)\\end{bmatrix} = \\mathbf{0}\\]\nThe Jacobian of this system has a block-triangular structure: \\[\\Gamma = \\begin{bmatrix}\\Gamma_{1\\gamma} & 0 \\\\ \\Gamma_{2\\gamma} & \\Gamma_{2\\beta}\\end{bmatrix}\\]\nwhere \\(\\Gamma_{1\\gamma}=\\mathbb{E}[\\nabla_{\\gamma}g_{1}']\\), \\(\\Gamma_{2\\gamma}=\\mathbb{E}[\\nabla_{\\gamma}g_{2}']\\), and \\(\\Gamma_{2\\beta}=\\mathbb{E}[\\nabla_{\\beta}g_{2}']\\). The zero in the upper-right block reflects the fact that the first step does not depend on \\(\\beta\\).\nApplying the standard GMM formula, the asymptotic variance of \\(\\hat{\\beta}\\) is:\n\\[V_{\\beta} = \\Gamma_{2\\beta}^{-1}\\mathbb{E}[(g_{2}-\\Gamma_{2\\gamma}\\Gamma_{1\\gamma}^{-1}g_{1})(g_{2}-\\Gamma_{2\\gamma}\\Gamma_{1\\gamma}^{-1}g_{1})']\\Gamma_{2\\beta}^{-1\\prime}\\]\nThe term \\(\\Gamma_{2\\gamma}\\Gamma_{1\\gamma}^{-1}g_{1}\\) captures the correction for first-stage estimation error. If we ignored this term and computed standard errors using only the second-stage moment conditions, we would generally get incorrect inference.\n\n\n\n\n\n\nWhen Does the First Stage Not Matter?\n\n\n\nThere is an important special case: if \\(\\Gamma_{2\\gamma}=\\mathbb{E}[\\nabla_{\\gamma}g_{2}']=0\\), then the correction vanishes and the first-stage estimation has no effect on the second-stage asymptotic variance. Intuitively, this happens when the second-step moment conditions are locally insensitive to the first-step parameters at the true values.\nA classic example is the two-stage IV estimator where the first stage is a probit. Here, the second-stage moment condition takes the form \\(\\mathbb{E}[\\Phi(\\mathbf{x}'\\gamma_{0})\\cdot u]=0\\). One can show that \\(\\mathbb{E}[\\nabla_{\\gamma}g_{2}']=0\\) because the projection of \\(u\\) onto functions of the instruments is zero by construction.\n\n\n\n\n\n\n\n\nExample: Two-Step Roy Model with Standard Errors\n\n\n\n\n\n\nExample 13.3 Let’s revisit the two-step estimator for the Generalized Roy Model, now accounting for first-stage estimation uncertainty. In the first step we estimate the probit, and in the second step we run the selection-corrected OLS regression.\nThe second-step regression for the treated group is: \\[Y = X\\beta_{1} - \\sigma_{V1}\\frac{\\phi(W\\gamma)}{\\Phi(W\\gamma)} + \\text{error}\\]\nSince the selection correction depends on \\(\\hat{\\gamma}\\), we need to account for first-stage variability.\n\nusing Distributions, Optim, Random, ForwardDiff, LinearAlgebra, Plots\n\n# Reuse sim_data and estimate_probit from above\n\n# Two-step estimation with standard errors\nfunction two_step_with_se(data, N)\n    (;X,Z,Y,D) = data\n\n    # Step 1: Probit\n    γ_hat = estimate_probit(data)\n\n    # Compute first-stage information matrix\n    H1 = ForwardDiff.hessian(g -&gt; log_likelihood(g, data), γ_hat)\n    V_gamma = -inv(H1) / N\n\n    # Step 2: Selection-corrected OLS for treated group\n    W = hcat(ones(N), X, Z)\n    idx = W * γ_hat\n    correction1 = pdf.(Normal(), idx) ./ cdf.(Normal(), idx)\n\n    Y1 = Y[D .== 1]\n    X1_mat = hcat(ones(sum(D)), X[D .== 1], correction1[D .== 1])\n    β1_hat = X1_mat \\ Y1\n\n    # For correct standard errors, we need the influence function approach\n    # The second-stage residuals\n    resid1 = Y1 .- X1_mat * β1_hat\n    N1 = sum(D)\n\n    # Naive OLS variance (ignoring first stage)\n    V_naive = inv(X1_mat' * X1_mat) * (X1_mat' * Diagonal(resid1.^2) * X1_mat) * inv(X1_mat' * X1_mat)\n\n    # Correction: how the selection term responds to γ\n    # ∂correction1/∂γ for each observation\n    function correction_grad(n, γ)\n        w = [1., X[n], Z[n]]\n        xg = dot(w, γ)\n        ϕ = pdf(Normal(), xg)\n        Φ = cdf(Normal(), xg)\n        # d/dγ [ϕ(wγ)/Φ(wγ)] = [-xg*ϕ/Φ - ϕ²/Φ²] * w\n        dcorr = (-xg * ϕ / Φ - ϕ^2 / Φ^2) .* w\n        return dcorr\n    end\n\n    # Compute Γ_{2γ} = E[∂g₂/∂γ']\n    # g₂ = X₁'(Y₁ - X₁β₁) where X₁ depends on γ through the correction term\n    # The effect of γ on the moment is through the correction term\n    treated_idx = findall(D .== 1)\n    Gamma_2gamma = zeros(3, 3)\n    for (j, n) in enumerate(treated_idx)\n        dc = correction_grad(n, γ_hat)\n        Gamma_2gamma += -β1_hat[3] * [0. 0. 0.; 0. 0. 0.; dc'] / N1\n        # The correction also affects X1_mat column 3\n        Gamma_2gamma += -[1., X[n], correction1[n]] * dc' * resid1[j] / (N1 * correction1[n]) # approximation\n    end\n\n    return (;γ_hat, β1_hat, se_naive = sqrt.(diag(V_naive)),\n             V_gamma, Gamma_2gamma)\nend\n\ngamma = [0., 0.5, 0.5]\nbeta0 = [0., 0.3]\nbeta1 = [0., 0.5]\nRandom.seed!(42)\ndata = sim_data(gamma, beta0, beta1, 5000)\nresults = two_step_with_se(data, 5000)\nprintln(\"Second-stage estimates: $(round.(results.β1_hat[1:2], digits=3))\")\nprintln(\"Naive SE:              $(round.(results.se_naive[1:2], digits=4))\")\n\nSecond-stage estimates: [-0.106, 0.508]\nNaive SE:              [0.0633, 0.0283]\n\n\nA robust way to check the standard errors is to compare with a Monte Carlo simulation, which we already did for the probit in Example 13.1. The correction matters most when the selection correction term is strongly estimated and when the first-stage parameters are imprecisely estimated.\n\n# Monte Carlo to verify\nests = mapreduce(vcat, 1:500) do b\n    d = sim_data(gamma, beta0, beta1, 2000)\n    γ_hat = estimate_probit(d)\n    W = hcat(ones(2000), d.X, d.Z)\n    idx = W * γ_hat\n    corr = pdf.(Normal(), idx) ./ cdf.(Normal(), idx)\n    Y1 = d.Y[d.D .== 1]\n    X1 = hcat(ones(sum(d.D)), d.X[d.D .== 1], corr[d.D .== 1])\n    β = X1 \\ Y1\n    return β[1:2]'\nend\n\nprintln(\"Monte Carlo SD of β₁: $(round.(std.(eachcol(ests)), digits=4))\")\n\nMonte Carlo SD of β₁: [0.0928, 0.0441]\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion: Bootstrap vs. Analytical Standard Errors\n\n\n\nComputing analytical standard errors for two-step estimators can be tedious, as the example above illustrates. An attractive alternative is the bootstrap: resample the data, re-run the entire two-step procedure on each bootstrap sample, and compute standard errors from the distribution of bootstrap estimates. This automatically accounts for first-stage estimation uncertainty without requiring explicit computation of correction terms. We will discuss the bootstrap formally in the chapter on simulation methods.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Asymptotic Theory</span>"
    ]
  },
  {
    "objectID": "lectures/extremum_theory.html#exercises",
    "href": "lectures/extremum_theory.html#exercises",
    "title": "13  Asymptotic Theory",
    "section": "13.8 Exercises",
    "text": "13.8 Exercises\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 13.1 Extend Example 12.1 to:\n\nCompute asymptotic standard errors for the estimated parameters \\(\\hat{\\theta}=(h,\\delta,\\mu,\\sigma,w^{*})\\) using the information matrix.\nUse the delta method (or ForwardDiff) to compute standard errors for the derived estimates \\(\\hat{\\lambda}\\) and \\(\\hat{b}\\).\nEstimate the search model separately for men with and without a bachelor’s degree.\nReport and compare your estimates. Are the differences across education groups economically meaningful? What does the model imply about the sources of wage differences between these groups?\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 13.2 Blundell, Pistaferri, and Preston (2008) estimate income and consumption dynamics using a model where: \\[\\Delta y_{n,t} = \\zeta_{n,t} + \\Delta\\xi_{n,t}\\] where \\(\\zeta_{n,t}\\) is a permanent shock and \\(\\xi_{n,t}\\) is a transitory shock. Consumption responds differently to each: \\[\\Delta c_{n,t} = \\phi\\zeta_{n,t} + \\psi\\xi_{n,t} + \\epsilon_{n,t}\\] where \\(\\phi\\) and \\(\\psi\\) capture the “insurance coefficients” — the degree to which consumption responds to permanent and transitory income shocks, respectively.\n\nShow that from the second moments of \\((\\Delta y, \\Delta c)\\), one can identify \\(\\sigma^{2}_{\\zeta}\\), \\(\\sigma^{2}_{\\xi}\\), \\(\\phi\\), and \\(\\psi\\). Hint: consider \\(\\mathbb{V}[\\Delta y_{t}]\\), \\(\\mathbb{C}(\\Delta y_{t},\\Delta y_{t-1})\\), \\(\\mathbb{C}(\\Delta c_{t},\\Delta y_{t})\\), and \\(\\mathbb{C}(\\Delta c_{t},\\Delta y_{t-1})\\).\nUsing the data from Example 10.1, implement a minimum distance estimator for \\((\\sigma^{2}_{\\zeta},\\sigma^{2}_{\\xi},\\phi,\\psi)\\) and report standard errors.\nWhat do the estimates of \\(\\phi\\) and \\(\\psi\\) tell us about how well households insure against permanent versus transitory income shocks?\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 13.3 Consider estimation of the entry-exit model by minimum distance, as described in the introduction. Recall that for each state \\((x,a,a')\\), the model implies a choice probability \\(p(x,a,a';\\phi,\\beta)\\) that depends on the payoff parameters through the equilibrium of the game. Data consist of entry decisions across many independent markets.\n\nSimulate data from the entry-exit model for \\(M=500\\) markets at 5 equally spaced values of \\(x\\) and use the empirical choice frequencies as your target moments \\(\\hat{\\mathbf{p}}\\).\nImplement the minimum distance estimator: \\[\\hat{\\phi} = \\arg\\min_{\\phi}(\\hat{\\mathbf{p}}-\\mathbf{p}(\\phi,\\beta))'\\mathbf{W}(\\hat{\\mathbf{p}}-\\mathbf{p}(\\phi,\\beta))\\] using the identity weighting matrix. Note: for each candidate \\(\\phi\\), you must solve for the equilibrium to compute \\(\\mathbf{p}(\\phi,\\beta)\\).\nCompute standard errors for \\(\\hat{\\phi}\\) using the minimum distance variance formula from Theorem 13.7.\nRe-estimate with the optimal weighting matrix and compare your standard errors.\n\n\n\n\n\n\n\n\nBlundell, Richard, Luigi Pistaferri, and Ian Preston. 2008. “Consumption Inequality and Partial Insurance.” American Economic Review 98 (5).\n\n\nNewey, Whitney K, and Daniel McFadden. 1994. “Large Sample Estimation and Hypothesis Testing.” Handbook of Econometrics 4: 2111–2245.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Asymptotic Theory</span>"
    ]
  },
  {
    "objectID": "lectures/extremum-estimators.html",
    "href": "lectures/extremum-estimators.html",
    "title": "Extremum Estimators",
    "section": "",
    "text": "In the previous section we studied identification: the mapping between population distributions and model parameters. We emphasized that it is not just important to establish whether your model is identified, but also how it is identified, given that many quantitative models are heavily parameterized.\nIdentification can lead — often but not always — directly to an estimation strategy. This is particularly true when, in the course of showing identification, you want to show that the key parameters in your study have a particularly credible source of identification.\nIn this part of the course we will move to estimation, first introducing approaches using each of our prototype models and, at times, discussing how estimation relates to the task of “credible” inference.",
    "crumbs": [
      "Extremum Estimators"
    ]
  },
  {
    "objectID": "lectures/extremum_theory.html#definitions",
    "href": "lectures/extremum_theory.html#definitions",
    "title": "13  Asymptotic Theory",
    "section": "",
    "text": "Definition 13.1 (Extremum Estimator) \\(\\hat{\\theta}\\) is an extremum estimator if: \\[\\hat{\\theta} = \\arg\\max_{\\theta\\in\\Theta}Q_{N}(\\theta)\\] where \\(\\Theta\\subset\\mathbb{R}^{p}\\) and \\(Q_{N}(\\cdot)\\) is some objective function that depends on the data.\n\n\n\n13.1.1 M-estimators\nAn important subclass of extremum estimators arises when the objective function is an average over the sample:\n\nDefinition 13.2 (M-estimator) \\(\\hat{\\theta}\\) is an M-estimator if: \\[Q_{N}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N}m(\\mathbf{w}_{n},\\theta)\\] for some known function \\(m\\).\n\nThe “M” stands for “maximum” (or “minimum”). Two of our workhorse estimators are M-estimators:\n\nMaximum Likelihood: \\(m(\\mathbf{w}_{n},\\theta) = \\log f(y_{n}|\\mathbf{x}_{n},\\theta)\\). This is the log-likelihood contribution of observation \\(n\\).\nNonlinear Least Squares: \\(m(\\mathbf{w}_{n},\\theta) = -(y_{n}-\\varphi(\\mathbf{x}_{n},\\theta))^{2}\\). Here \\(\\varphi(\\mathbf{x},\\theta)\\) is a regression function and the objective penalizes deviations of \\(y\\) from its conditional mean.\n\n\n\n13.1.2 GMM Estimator\nThe GMM estimator is defined by a set of moment conditions \\(\\mathbb{E}[g(\\mathbf{w},\\theta_{0})]=\\mathbf{0}\\):\n\nDefinition 13.3 (GMM Estimator) \\[Q_{N}(\\theta) = -\\frac{1}{2}\\mathbf{g}_{N}(\\theta)'\\hat{\\mathbf{W}}\\mathbf{g}_{N}(\\theta),\\qquad\\mathbf{g}_{N}(\\theta)=\\frac{1}{N}\\sum_{n}g(\\mathbf{w}_{n},\\theta)\\] where \\(\\hat{\\mathbf{W}}\\) is a positive definite weighting matrix.\n\nNote that GMM is itself an M-estimator with \\(m(\\mathbf{w}_{n},\\theta)=-\\frac{1}{2}g(\\mathbf{w}_{n},\\theta)'\\hat{\\mathbf{W}}g(\\mathbf{w}_{n},\\theta)\\) (after expanding the quadratic form). We will return to the specific properties of GMM in a later section.\n\n\n13.1.3 Minimum Distance Estimator\nThe minimum distance estimator works with a first-stage reduced-form estimate \\(\\hat{\\pi}\\) and model restrictions \\(\\psi(\\pi,\\theta)\\):\n\nDefinition 13.4 (Minimum Distance Estimator) \\[Q_{N}(\\theta) = -\\frac{1}{2}\\psi(\\hat{\\pi}_{N},\\theta)'\\hat{\\mathbf{W}}\\psi(\\hat{\\pi}_{N},\\theta)\\] where \\(\\psi(\\pi_{0},\\theta_{0})=\\mathbf{0}\\) and \\(\\sqrt{N}(\\hat{\\pi}_{N}-\\pi_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Omega)\\).\n\nThe minimum distance estimator differs from GMM in that the objective depends on the data only through the first-stage statistic \\(\\hat{\\pi}\\), rather than through the individual observations directly. We will study its asymptotic properties in a dedicated section.",
    "crumbs": [
      "Extremum Estimators",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Asymptotic Theory</span>"
    ]
  },
  {
    "objectID": "lectures/simulation-methods.html",
    "href": "lectures/simulation-methods.html",
    "title": "14  Simulation Methods",
    "section": "",
    "text": "14.1 Integration by Simulation\nIn many structural models, the likelihood or moment conditions cannot be computed in closed form because they involve high-dimensional integrals over unobserved heterogeneity, latent states, or expectations over future shocks. The savings model is a good example: even if we knew the true parameters, computing the likelihood of an observed consumption path requires integrating over sequences of income shocks. In these settings, simulation provides a way forward.\nThis chapter covers three topics. First, we discuss simulation-based estimators — the method of simulated moments (MSM) and indirect inference — which replace intractable analytical objects with simulation approximations. Second, we introduce the bootstrap, a general resampling-based approach to inference that is particularly useful when analytical standard errors are difficult to derive. Third, we bring these tools together with our prototype models.\nAt the heart of simulation-based estimation is the problem of computing integrals. Suppose we need to evaluate:\n\\[f(y|\\mathbf{x},\\theta) = \\int h(y|\\mathbf{x},\\theta,\\mathbf{u})g(\\mathbf{u})d\\mathbf{u}\\]\nwhere \\(\\mathbf{u}\\) is a vector of unobservables with density \\(g\\). If the dimension of \\(\\mathbf{u}\\) is large or \\(h\\) does not admit a closed-form integral, we can approximate by Monte Carlo integration: draw \\(\\mathbf{u}^{1},\\ldots,\\mathbf{u}^{R}\\) from \\(g\\) and compute:\n\\[\\hat{f}(y|\\mathbf{x},\\theta) = \\frac{1}{R}\\sum_{r=1}^{R}h(y|\\mathbf{x},\\theta,\\mathbf{u}^{r})\\]\nBy the law of large numbers, \\(\\hat{f}\\rightarrow_{p} f\\) as \\(R\\rightarrow\\infty\\). This is unbiased by construction: \\(\\mathbb{E}_{\\mathbf{u}}[\\hat{f}]=f\\).",
    "crumbs": [
      "Simulation Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation Methods</span>"
    ]
  },
  {
    "objectID": "lectures/simulation-methods.html#integration-by-simulation",
    "href": "lectures/simulation-methods.html#integration-by-simulation",
    "title": "14  Simulation Methods",
    "section": "",
    "text": "14.1.1 Importance Sampling\nSometimes we want to draw from an alternative density \\(w(\\mathbf{u})\\) instead of \\(g(\\mathbf{u})\\) — for instance because \\(g\\) is hard to sample from, or because draws from \\(w\\) reduce the variance of the estimator. In this case:\n\\[\\hat{f}(y|\\mathbf{x},\\theta) = \\frac{1}{R}\\sum_{r=1}^{R}h(y|\\mathbf{x},\\theta,\\mathbf{u}^{r})\\frac{g(\\mathbf{u}^{r})}{w(\\mathbf{u}^{r})},\\qquad \\mathbf{u}^{r}\\sim w\\]\nThis is still unbiased and consistent, and the choice of \\(w\\) can substantially reduce variance. The ratio \\(g/w\\) is called the importance weight.",
    "crumbs": [
      "Simulation Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation Methods</span>"
    ]
  },
  {
    "objectID": "lectures/simulation-methods.html#maximum-simulated-likelihood",
    "href": "lectures/simulation-methods.html#maximum-simulated-likelihood",
    "title": "14  Simulation Methods",
    "section": "14.2 Maximum Simulated Likelihood",
    "text": "14.2 Maximum Simulated Likelihood\nA natural first idea is to replace the likelihood with its simulated analogue. The maximum simulated likelihood (MSL) estimator is:\n\\[\\hat{\\theta}_{MSL} = \\arg\\max_{\\theta}\\sum_{n=1}^{N}\\log\\hat{f}(y_{n}|\\mathbf{x}_{n},\\theta)\\]\nwhere \\(\\hat{f}\\) is the simulated probability from above.\n\n\n\n\n\n\nA Subtlety with MSL\n\n\n\nThere is an important issue with MSL: because \\(\\log\\) is concave, Jensen’s inequality gives \\(\\mathbb{E}[\\log\\hat{f}]\\leq \\log\\mathbb{E}[\\hat{f}]=\\log f\\). This means the simulated log-likelihood is biased downward, and the MSL estimator is inconsistent for fixed \\(R\\). Consistency requires that the number of simulation draws grows with the sample: both \\(N,R\\rightarrow\\infty\\) with \\(\\sqrt{N}/R\\rightarrow 0\\).\nThis is in contrast to the method of simulated moments, which we discuss next, where a fixed number of simulation draws is sufficient for consistency.\n\n\nUnder standard regularity conditions and \\(\\sqrt{N}/R\\rightarrow 0\\), the MSL estimator has the same asymptotic distribution as the (infeasible) MLE:\n\\[\\sqrt{N}(\\hat{\\theta}_{MSL}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\ \\mathcal{I}(\\theta_{0})^{-1})\\]\nIn practice, one should use enough draws that the simulation error is small relative to the sampling variability. A common rule of thumb is to set \\(R\\) large enough that results are insensitive to further increases.",
    "crumbs": [
      "Simulation Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation Methods</span>"
    ]
  },
  {
    "objectID": "lectures/simulation-methods.html#method-of-simulated-moments",
    "href": "lectures/simulation-methods.html#method-of-simulated-moments",
    "title": "14  Simulation Methods",
    "section": "14.3 Method of Simulated Moments",
    "text": "14.3 Method of Simulated Moments\nThe method of simulated moments (MSM) is the simulation analogue of GMM. Suppose that the moment conditions take the form:\n\\[\\mathbb{E}[g(\\mathbf{w},\\theta_{0})] = \\mathbb{E}\\left[\\int h(\\mathbf{w},\\theta_{0},\\mathbf{u})f(\\mathbf{u}|\\theta_{0})d\\mathbf{u}\\right] = \\mathbf{0}\\]\nIf we cannot evaluate \\(g\\) analytically, we can replace it with a simulated version: \\[\\hat{g}(\\mathbf{w}_{n},\\theta) = \\frac{1}{R}\\sum_{r=1}^{R}\\tilde{g}(\\mathbf{w}_{n},\\mathbf{u}^{r},\\theta)\\]\nwhere \\(\\mathbb{E}_{\\mathbf{u}}[\\tilde{g}(\\mathbf{w},\\mathbf{u},\\theta)]=g(\\mathbf{w},\\theta)\\). The MSM estimator is:\n\\[\\hat{\\theta}_{MSM} = \\arg\\min_{\\theta}\\ \\hat{\\mathbf{g}}_{N}(\\theta)'\\mathbf{W}\\hat{\\mathbf{g}}_{N}(\\theta),\\qquad\\hat{\\mathbf{g}}_{N}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N}\\hat{g}(\\mathbf{w}_{n},\\theta)\\]\n\n14.3.1 A Common Special Case\nIn many structural applications, the moment conditions do not involve observation-level simulation. Instead, we match aggregate statistics from the data to their model-implied counterparts computed by simulating the model. The moments take the form:\n\\[\\hat{\\mathbf{g}}_{N}(\\theta) = \\hat{\\mathbf{m}} - \\tilde{\\mathbf{m}}(\\theta)\\]\nwhere \\(\\hat{\\mathbf{m}}\\) is a vector of sample statistics (means, variances, covariances, etc.) and \\(\\tilde{\\mathbf{m}}(\\theta)\\) is the corresponding vector computed from simulated data. This is the structure we will use for the savings model below.\n\n\n14.3.2 Asymptotic Distribution\n\nTheorem 14.1 (Asymptotic Distribution of MSM) Suppose the standard regularity conditions for GMM hold and \\(\\mathbb{E}_{\\mathbf{u}}[\\tilde{g}(\\mathbf{w},\\mathbf{u},\\theta)]=g(\\mathbf{w},\\theta)\\). Then with \\(R\\) fixed:\n\\[\\sqrt{N}(\\hat{\\theta}_{MSM}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Sigma_{MSM})\\]\nwhere: \\[\\Sigma_{MSM} = (\\nabla_{\\theta}g_{0}\\mathbf{W}\\nabla_{\\theta}g_{0}')^{-1}\\nabla_{\\theta}g_{0}\\mathbf{W}\\mathbb{E}[\\hat{g}_{0}\\hat{g}_{0}']\\mathbf{W}\\nabla_{\\theta}g_{0}'(\\nabla_{\\theta}g_{0}\\mathbf{W}\\nabla_{\\theta}g_{0}')^{-1}\\]\nwith \\(\\nabla_{\\theta}g_{0}=\\mathbb{E}[\\nabla_{\\theta}g(\\mathbf{w},\\theta_{0})']\\).\n\nSeveral features are worth highlighting:\n\nConsistency does not require \\(R\\rightarrow\\infty\\). Because \\(\\hat{g}\\) is an unbiased estimator of \\(g\\), the simulated sample moments converge to zero at \\(\\theta_{0}\\) by the law of large numbers. This is in contrast to MSL.\nSimulation adds variance. Because \\(\\mathbb{V}[\\hat{g}]\\geq\\mathbb{V}[g]\\), the MSM estimator is less efficient than the infeasible GMM estimator that uses the true \\(g\\). For the common case of a frequency simulator (where \\(\\tilde{g}\\) is a binary indicator), the variance inflates by a factor of \\((1+1/R)\\).\nIf \\(\\sqrt{N}/R\\rightarrow 0\\), the efficiency loss from simulation disappears and the MSM achieves the same asymptotic variance as GMM.\n\nSee Newey and McFadden (1994) for a comprehensive treatment.\n\n\n\n\n\n\nExample: Simulating the Savings Model\n\n\n\n\n\n\nExample 14.1 To estimate the savings model by MSM, we first need to be able to simulate data from it. Given a solution (the policy function \\(A\\)), we can forward-simulate a panel of individuals by drawing income shocks and applying the optimal savings rule.\n\nusing Distributions, Random, LinearAlgebra, Statistics\n\n# === Model setup (from savings model chapter) ===\nΦ_cdf(x) = cdf(Normal(),x)\n\nfunction tauchen(ρ,ση,Kϵ)\n    sd = ση/sqrt(1-ρ^2)\n    grid = range(-3sd,stop=3sd,length=Kϵ)\n    Π = zeros(Kϵ,Kϵ)\n    Δ = grid[2]-grid[1]\n    for j=1:Kϵ\n        Π[1,j] = Φ_cdf((grid[1] + Δ/2 - ρ*grid[j])/ση)\n        Π[end,j] = 1 - Φ_cdf((grid[end] - Δ/2 - ρ*grid[j])/ση)\n        for k=2:(Kϵ-1)\n            Π[k,j] = Φ_cdf((grid[k] + Δ/2 - ρ*grid[j])/ση) - Φ_cdf((grid[k] - Δ/2 - ρ*grid[j])/ση)\n        end\n    end\n    return Π,collect(grid)\nend\n\nu(c,σ) = c^(1-σ) / (1-σ)\n\nfunction solve_max(V,t,iϵ,ia,pars)\n    (;agrid,ϵgrid,Π,σ,Ka,r,β) = pars\n    cash = exp(pars.μ[t] + ϵgrid[iϵ]) + agrid[ia]\n    amax = 1\n    vmax = -Inf\n    loop = true\n    a = 1\n    while loop && a&lt;Ka\n        c = cash - agrid[a] / (1+r)\n        if c&gt;0\n            v = u(c,σ)\n            for iϵ′ in axes(V,1)\n                v += β * Π[iϵ′,iϵ] * V[iϵ′,a,t+1]\n            end\n            if v&gt;vmax\n                vmax = v\n                amax = a\n            end\n        else\n            loop = false\n        end\n        a += 1\n    end\n    return amax,vmax\nend\n\nfunction iterate!(V,A,t,pars)\n    for ia in axes(V,2), iϵ in axes(V,1)\n        A[iϵ,ia,t],V[iϵ,ia,t] = solve_max(V,t,iϵ,ia,pars)\n    end\nend\n\nfunction terminal_values!(V,pars)\n    (;σ,ψ,agrid) = pars\n    for ia in axes(V,2), iϵ in axes(V,1)\n        V[iϵ,ia] = ψ * u(agrid[ia],σ)\n    end\nend\n\nfunction backward_induction!(V,A,pars)\n    (;T) = pars\n    @views terminal_values!(V[:,:,T+1],pars)\n    for t in reverse(1:T)\n        iterate!(V,A,t,pars)\n    end\nend\n\nfunction setup_and_solve(θ)\n    β, σ, ψ = θ\n    T = 45\n    Ka = 100\n    Kϵ = 5\n    ρ = 0.9\n    ση = 0.1\n    r = 0.05\n    μ = fill(2., T)\n    agrid = collect(LinRange(0, μ[1] * T, Ka))\n    Π, ϵgrid = tauchen(ρ, ση, Kϵ)\n    pars = (;T, β, σ, ρ, ση, μ, ψ, r, Ka, Kϵ, agrid, Π, ϵgrid)\n    V = zeros(Kϵ, Ka, T+1)\n    A = zeros(Int64, Kϵ, Ka, T)\n    backward_induction!(V, A, pars)\n    return A, pars\nend\n\nsetup_and_solve (generic function with 1 method)\n\n\nNow we write a simulation function. Given the policy function, we draw income shocks and track each individual’s asset and income path.\n\nfunction simulate_panel(A, pars, N_sim; seed=nothing)\n    (;T, Kϵ, Ka, agrid, ϵgrid, Π, μ, r) = pars\n    if !isnothing(seed)\n        Random.seed!(seed)\n    end\n\n    # Pre-compute CDF for sampling from Markov chain\n    cumΠ = cumsum(Π, dims=1)\n\n    # Storage\n    assets = zeros(N_sim, T+1)   # assets at start of each period\n    income = zeros(N_sim, T)\n    consumption = zeros(N_sim, T)\n    iϵ_sim = zeros(Int, N_sim, T)\n\n    # Initial conditions: start with zero assets, draw initial ϵ from stationary dist\n    # Use middle state as starting point for simplicity\n    iϵ_sim[:, 1] .= (Kϵ + 1) ÷ 2\n\n    for n in 1:N_sim\n        ia = 1  # start with zero assets\n        for t in 1:T\n            iϵ = iϵ_sim[n, t]\n            y = exp(μ[t] + ϵgrid[iϵ])\n            income[n, t] = y\n            cash = y + agrid[ia]\n\n            # Look up optimal savings\n            ia_next = A[iϵ, ia, t]\n            c = cash - agrid[ia_next] / (1 + r)\n            consumption[n, t] = c\n            assets[n, t+1] = agrid[ia_next]\n            ia = ia_next\n\n            # Draw next period's ϵ\n            if t &lt; T\n                u_draw = rand()\n                iϵ_next = findfirst(cumΠ[:, iϵ] .&gt;= u_draw)\n                iϵ_sim[n, t+1] = iϵ_next\n            end\n        end\n    end\n    return (;assets, income, consumption)\nend\n\n# Solve and simulate\nA, pars = setup_and_solve((0.95, 2.0, 5.0))\nsim = simulate_panel(A, pars, 1000; seed=123)\n\n(assets = [0.0 0.0 … 12.727272727272727 12.727272727272727; 0.0 0.0 … 19.09090909090909 19.09090909090909; … ; 0.0 0.0 … 15.454545454545453 16.363636363636363; 0.0 0.0 … 17.27272727272727 17.27272727272727], income = [7.38905609893065 7.38905609893065 … 5.2376681994924175 5.2376681994924175; 7.38905609893065 7.38905609893065 … 7.38905609893065 7.38905609893065; … ; 7.38905609893065 7.38905609893065 … 7.38905609893065 7.38905609893065; 7.38905609893065 7.38905609893065 … 7.38905609893065 7.38905609893065], consumption = [7.38905609893065 7.38905609893065 … 5.843728805553026 5.843728805553026; 7.38905609893065 7.38905609893065 … 8.298147008021559 8.298147008021559; … ; 7.38905609893065 7.38905609893065 … 7.215895925770482 7.259185969060521; 7.38905609893065 7.38905609893065 … 8.21156692144147 8.21156692144147])\n\n\nLet’s check the simulation by plotting average assets and consumption over the life cycle.\n\nusing Plots\np1 = plot(1:pars.T, mean(sim.assets[:,1:pars.T], dims=1)', label=\"Mean Assets\",\n          xlabel=\"Age\", title=\"Life-Cycle Profiles\")\nplot!(p1, 1:pars.T, mean(sim.consumption, dims=1)', label=\"Mean Consumption\")\nplot!(p1, 1:pars.T, mean(sim.income, dims=1)', label=\"Mean Income\")\n\n\n\n\nThe hump-shaped asset profile and the smooth consumption path are consistent with the model’s predictions: individuals accumulate assets during working years and draw them down toward the end of life, with the bequest motive preventing full decumulation.\n\n\n\n\n\n\n\n\n\n\nExample: Simulated Moments for the Savings Model\n\n\n\n\n\n\nExample 14.2 Now let’s use the simulation machinery to define a set of moments and construct an MSM estimator for the preference parameters \\((\\beta,\\sigma,\\psi)\\). We take the income process parameters \\((\\rho,\\sigma_{\\eta})\\) as given (estimated in Example 12.2).\nOur target moments will be:\n\nMean assets at ages 30, 40, 50, 60 (to pin down the savings profile)\nVariance of log consumption at ages 30, 40, 50, 60 (to pin down risk aversion)\n\n\n# Compute moments from simulated data\nfunction simulated_moments(θ; N_sim=2000, seed=42)\n    A, pars = setup_and_solve(θ)\n    sim = simulate_panel(A, pars, N_sim; seed=seed)\n    target_ages = [6, 16, 26, 36]  # ages 30, 40, 50, 60 (relative to t=1 at age 25)\n    m_assets = [mean(sim.assets[:, t]) for t in target_ages]\n    m_var_logc = [var(log.(sim.consumption[:, t])) for t in target_ages]\n    return vcat(m_assets, m_var_logc)\nend\n\n# \"True\" parameters and data moments\nθ_true = (0.95, 2.0, 5.0)\nm_data = simulated_moments(θ_true; N_sim=10_000, seed=1)\n\n# MSM objective\nfunction msm_objective(x, m_data)\n    β = 0.8 + 0.19 * (1 / (1 + exp(-x[1])))  # map to (0.8, 0.99)\n    σ = exp(x[2])\n    ψ = exp(x[3])\n    θ = (β, σ, ψ)\n    m_sim = simulated_moments(θ)\n    diff = m_data .- m_sim\n    return diff' * diff\nend\n\nmsm_objective (generic function with 1 method)\n\n\nThis estimator is computationally intensive — each evaluation of the objective requires solving the dynamic programming problem and simulating the model. In practice, derivative-free optimization methods or finite-difference gradients are often used.\n\nusing Optim\nx0 = [0.0, log(2.0), log(5.0)]\nres = optimize(x -&gt; msm_objective(x, m_data), x0, NelderMead(),\n               Optim.Options(iterations=200, show_trace=true))",
    "crumbs": [
      "Simulation Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation Methods</span>"
    ]
  },
  {
    "objectID": "lectures/simulation-methods.html#indirect-inference",
    "href": "lectures/simulation-methods.html#indirect-inference",
    "title": "14  Simulation Methods",
    "section": "14.4 Indirect Inference",
    "text": "14.4 Indirect Inference\nIndirect inference is a simulation-based estimator that works by fitting an auxiliary model to both the real data and to data simulated from the structural model, and then finding the structural parameters that make the two sets of auxiliary estimates as close as possible.\n\n14.4.1 Setup\nLet \\(\\hat{\\beta}_{N}\\) be a vector of statistics estimated from the data (the “auxiliary parameters”). For example, \\(\\hat{\\beta}\\) might be OLS coefficients from a wage regression, or the parameters of a reduced-form probit. The key requirement is that \\(\\hat{\\beta}\\) is asymptotically normal:\n\\[\\sqrt{N}(\\hat{\\beta}_{N}-\\beta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Omega)\\]\nNow, for each candidate \\(\\theta\\), simulate data from the structural model and compute the same statistic on the simulated data, yielding \\(\\hat{\\beta}^{R}(\\theta)\\). Let \\(\\beta(\\theta)=\\text{plim}\\ \\hat{\\beta}^{R}(\\theta)\\) denote the “pseudo-true” value of the auxiliary parameter implied by \\(\\theta\\). The indirect inference estimator is:\n\\[\\hat{\\theta}_{II} = \\arg\\min_{\\theta}\\ (\\hat{\\beta}_{N}-\\hat{\\beta}^{R}(\\theta))'\\mathbf{W}(\\hat{\\beta}_{N}-\\hat{\\beta}^{R}(\\theta))\\]\n\n\n14.4.2 Asymptotic Distribution\n\nTheorem 14.2 (Asymptotic Distribution of Indirect Inference) Suppose:\n\n\\(\\sqrt{N}(\\hat{\\beta}_{N}-\\beta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Omega)\\)\n\\(R,N\\rightarrow\\infty\\) with \\(\\sqrt{N}/R\\rightarrow 0\\)\n\\(\\theta_{0}\\) is the unique solution to \\(\\beta(\\theta)=\\beta_{0}\\) (identification)\n\nThen: \\[\\sqrt{N}(\\hat{\\theta}_{II}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}\\left(\\mathbf{0},\\ (\\nabla_{\\theta}\\beta_{0}\\mathbf{W}\\nabla_{\\theta}\\beta_{0}')^{-1}\\nabla_{\\theta}\\beta_{0}\\mathbf{W}\\Omega\\mathbf{W}\\nabla_{\\theta}\\beta_{0}'(\\nabla_{\\theta}\\beta_{0}\\mathbf{W}\\nabla_{\\theta}\\beta_{0}')^{-1}\\right)\\]\nwhere \\(\\nabla_{\\theta}\\beta_{0}=\\frac{\\partial\\beta(\\theta_{0})'}{\\partial\\theta}\\).\n\nThe variance formula has the same sandwich structure as minimum distance estimation (Theorem 13.7), which makes sense: indirect inference is a simulated minimum distance estimator where the reduced-form statistics play the role of \\(\\pi\\).\n\n\n14.4.3 Relationship to MSM\nIndirect inference nests MSM as a special case. If the auxiliary model consists of sample moments \\(\\hat{\\beta}=\\frac{1}{N}\\sum_{n}g(\\mathbf{w}_{n})\\) and the simulated counterpart is \\(\\hat{\\beta}^{R}(\\theta)=\\frac{1}{NR}\\sum_{r}\\sum_{n}g(\\mathbf{w}_{n}^{r}(\\theta))\\), then the indirect inference estimator reduces to MSM. The appeal of indirect inference is that it allows the use of richer auxiliary models — like regressions or multinomial choice models — that can capture complex features of the data.\n\n\n\n\n\n\nDiscussion: Choosing Auxiliary Statistics\n\n\n\nThe choice of auxiliary model is both an art and a science. A good auxiliary model should:\n\nBe sensitive to the structural parameters of interest (for efficiency).\nCapture the key features of the data that the structural model is designed to explain.\nBe computationally cheap to estimate (since it must be re-estimated on each simulated dataset).\n\nIn practice, it is useful to think of the auxiliary model as a diagnostic: if the structural model cannot match the auxiliary estimates, it suggests the model is missing something important. This perspective connects indirect inference to the broader goal of model evaluation.",
    "crumbs": [
      "Simulation Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation Methods</span>"
    ]
  },
  {
    "objectID": "lectures/simulation-methods.html#the-bootstrap",
    "href": "lectures/simulation-methods.html#the-bootstrap",
    "title": "14  Simulation Methods",
    "section": "14.5 The Bootstrap",
    "text": "14.5 The Bootstrap\nWe now turn to a fundamentally different use of simulation: not for evaluating the objective function, but for inference. The bootstrap uses resampling to approximate the sampling distribution of an estimator, providing standard errors and confidence intervals without requiring analytical derivations.\n\n14.5.1 Motivation\nIn many settings we face one or more of the following challenges:\n\nThe variance formula is difficult to derive (e.g. for multi-step estimators).\nThe derivatives of the criterion function are hard or costly to compute.\nThe estimation criterion is non-smooth in finite samples.\nThe sample size is too small for the CLT to provide a good approximation.\n\nThe bootstrap addresses all of these by directly approximating the finite-sample distribution.\n\n\n14.5.2 Framework\nLet \\(T_{N} = T_{N}(X_{1},\\ldots,X_{N})\\) be a statistic — for instance, \\(T_{N}=\\hat{\\theta}\\) or \\(T_{N}=\\sqrt{N}(\\hat{\\theta}-\\theta_{0})\\). The finite-sample distribution \\(G_{N}(\\cdot,F_{0})\\) is typically unknown. The bootstrap idea is to replace \\(F_{0}\\) with an estimate \\(\\hat{F}_{N}\\) and use \\(G_{N}(\\cdot,\\hat{F}_{N})\\) to approximate \\(G_{N}(\\cdot,F_{0})\\).\nThere are two main approaches to constructing \\(\\hat{F}_{N}\\):\n\nDefinition 14.1 (Nonparametric Bootstrap) Replace \\(F_{0}\\) with the empirical distribution function: \\[\\hat{F}_{N}(x) = \\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{1}\\{X_{n}\\leq x\\}\\] Bootstrap samples are obtained by sampling \\(N\\) observations with replacement from the original data.\n\n\nDefinition 14.2 (Parametric Bootstrap) Replace \\(F_{0}\\) with \\(F(\\cdot,\\hat{\\theta}_{N})\\), the model-implied distribution evaluated at the estimated parameters. Bootstrap samples are obtained by simulating from the estimated model.\n\nThe nonparametric bootstrap is more general and requires fewer assumptions, while the parametric bootstrap can be more efficient when the model is correctly specified. For structural models, the parametric bootstrap is natural since we already have the machinery to simulate from the model.\n\n\n14.5.3 Procedure\nGiven an estimator \\(\\hat{\\theta}_{N}\\):\n\nDraw a bootstrap sample of size \\(N\\): \\(\\mathbf{X}^{b}=\\{X_{1}^{b},\\ldots,X_{N}^{b}\\}\\)\n\nNonparametric: sample with replacement from \\(\\{X_{1},\\ldots,X_{N}\\}\\)\nParametric: simulate from \\(F(\\cdot,\\hat{\\theta}_{N})\\)\n\nCompute the bootstrap estimate: \\(\\hat{\\theta}^{b}_{N}=T_{N}(\\mathbf{X}^{b})\\)\nRepeat steps 1-2 for \\(b=1,\\ldots,B\\) to obtain the bootstrap distribution \\(\\{\\hat{\\theta}^{1}_{N},\\ldots,\\hat{\\theta}^{B}_{N}\\}\\)\n\nFrom this distribution we can compute:\n\nBootstrap standard errors: \\(\\text{se}^{*}(\\hat{\\theta}) = \\text{sd}(\\hat{\\theta}^{1},\\ldots,\\hat{\\theta}^{B})\\)\nPercentile confidence intervals: \\([q_{\\alpha/2},\\ q_{1-\\alpha/2}]\\) where \\(q_{\\alpha}\\) is the \\(\\alpha\\)-quantile of the bootstrap distribution\nBootstrap-\\(t\\) confidence intervals: based on the distribution of the pivotal statistic \\(t^{b}=(\\hat{\\theta}^{b}-\\hat{\\theta})/\\text{se}(\\hat{\\theta}^{b})\\)\n\n\n\n14.5.4 When Does the Bootstrap Work?\nThe bootstrap is consistent when the statistic \\(T_{N}\\) is asymptotically normal. In this case, \\(G_{N}(\\cdot,\\hat{F}_{N})\\rightarrow_{p}G_{\\infty}(\\cdot,F_{0})\\), so the bootstrap distribution converges to the true asymptotic distribution.\nThe bootstrap can actually do better than the asymptotic normal approximation when the statistic is asymptotically pivotal — that is, when its asymptotic distribution does not depend on unknown parameters. The \\(t\\)-statistic \\(\\sqrt{N}(\\hat{\\theta}-\\theta_{0})/\\text{se}(\\hat{\\theta})\\) is a canonical example. In this case, the bootstrap provides a higher-order refinement over the normal approximation. See Horowitz (2001) for a detailed discussion.\n\n\n\n\n\n\nReplicating the Dependence Structure\n\n\n\nThe bootstrap requires that the resampling scheme correctly replicates the dependence structure in the data. For iid data, sampling with replacement is sufficient. For panel data, one should resample entire individuals (not individual observations) to preserve the within-person correlation structure. For time series data, block bootstrap methods are needed.\n\n\n\n\n\n\n\n\nExample: Bootstrap Confidence Intervals for the Income Process\n\n\n\n\n\n\nExample 14.3 Let’s apply the bootstrap to construct confidence intervals for the income process parameters estimated by minimum distance in Example 12.2. Rather than relying on the analytical standard errors from Example 13.2, we resample individuals from the PSID panel and re-estimate the parameters on each bootstrap sample.\nSince the PSID is panel data, we resample entire individuals (all observations for a given person) to preserve the within-person correlation.\n\nusing CSV, DataFrames, DataFramesMeta, Statistics, Optim, Plots, Random\n\n# Load and prepare data\ndata_psid = @chain begin\n    CSV.read(\"../data/abb_aea_data.csv\",DataFrame,missingstring = \"NA\")\n    @select :person :y :tot_assets1 :asset :age :year\n    @subset :age.&gt;=25 :age.&lt;=64\nend\n\n# Model moments and estimation functions (from @exm-md_income)\nfunction model_moments(θ, T)\n    ρ, σ2_α, σ2_η = θ\n    m = [σ2_α + (1-ρ^(2(t-1)))/(1-ρ^2) * σ2_η for t in 1:T]\n    return m\nend\n\nfunction md_estimate(data)\n    m_hat = @chain data begin\n        groupby(:age)\n        @combine :var_logy = var(log.(:y))\n        @orderby :age\n        _.var_logy\n    end\n    T = length(m_hat)\n\n    function md_objective(x)\n        ρ = tanh(x[1])\n        σ2_α = exp(x[2])\n        σ2_η = exp(x[3])\n        θ = (ρ, σ2_α, σ2_η)\n        m_model = model_moments(θ, T)\n        diff = m_hat .- m_model\n        return diff' * diff\n    end\n\n    x0 = [0.5, log(0.1), log(0.05)]\n    res = optimize(md_objective, x0, Newton(), autodiff=:forward)\n    x = res.minimizer\n    return [tanh(x[1]), exp(x[2]), exp(x[3])]\nend\n\n# Point estimate\nθ_hat = md_estimate(data_psid)\nprintln(\"Point estimates: ρ=$(round(θ_hat[1],digits=3)), σ²_α=$(round(θ_hat[2],digits=3)), σ²_η=$(round(θ_hat[3],digits=3))\")\n\nPoint estimates: ρ=0.918, σ²_α=0.279, σ²_η=0.085\n\n\nNow the bootstrap. We resample persons with replacement and re-estimate on each bootstrap sample.\n\nRandom.seed!(42)\npersons = unique(data_psid.person)\nN_persons = length(persons)\nB = 200\n\nθ_boot = mapreduce(vcat, 1:B) do b\n    # Resample persons with replacement\n    boot_persons = persons[rand(1:N_persons, N_persons)]\n    # Build bootstrap dataset (preserving panel structure)\n    boot_data = mapreduce(vcat, boot_persons) do p\n        @subset(data_psid, :person .== p)\n    end\n    try\n        return md_estimate(boot_data)'\n    catch\n        return [NaN NaN NaN]\n    end\nend\n\n# Remove any failed bootstrap replications\nθ_boot = θ_boot[.!any(isnan.(θ_boot), dims=2)[:], :]\n\n# Bootstrap standard errors\nse_boot = std.(eachcol(θ_boot))\nprintln(\"\\nBootstrap standard errors (B=$B):\")\nprintln(\"  se(ρ)    = $(round(se_boot[1], digits=4))\")\nprintln(\"  se(σ²_α) = $(round(se_boot[2], digits=4))\")\nprintln(\"  se(σ²_η) = $(round(se_boot[3], digits=4))\")\n\n# 95% percentile confidence intervals\nci_lower = [quantile(θ_boot[:,j], 0.025) for j in 1:3]\nci_upper = [quantile(θ_boot[:,j], 0.975) for j in 1:3]\nprintln(\"\\n95% Confidence Intervals:\")\nprintln(\"  ρ:    [$(round(ci_lower[1],digits=3)), $(round(ci_upper[1],digits=3))]\")\nprintln(\"  σ²_α: [$(round(ci_lower[2],digits=3)), $(round(ci_upper[2],digits=3))]\")\nprintln(\"  σ²_η: [$(round(ci_lower[3],digits=3)), $(round(ci_upper[3],digits=3))]\")\n\n\nBootstrap standard errors (B=200):\n  se(ρ)    = 0.079\n  se(σ²_α) = 0.115\n  se(σ²_η) = 0.0758\n\n95% Confidence Intervals:\n  ρ:    [0.742, 1.0]\n  σ²_α: [0.144, 0.515]\n  σ²_η: [0.012, 0.249]\n\n\nLet’s visualize the bootstrap distributions.\n\nlabels = [\"ρ\", \"σ²_α\", \"σ²_η\"]\npl = [begin\n    histogram(θ_boot[:,j], normalize=:pdf, label=false, alpha=0.5)\n    plot!([θ_hat[j], θ_hat[j]], [0, ylims()[2]*0.9], color=\"red\",\n          linewidth=2, label=\"Point Estimate\")\n    plot!(title=labels[j])\nend for j in 1:3]\nplot(pl..., layout=(1,3), size=(900,300))\n\n\n\n\nNotice that bootstrapping panel data by resampling individuals is straightforward to implement and automatically accounts for within-person dependence, arbitrary heteroskedasticity, and the full complexity of the minimum distance estimator. Compare these to the analytical standard errors from Example 13.2.\n\n\n\n\n\n\n\n\n\n\nExample: Bootstrap for the Savings Model\n\n\n\n\n\n\nExample 14.4 For the savings model, we use the parametric bootstrap. Since we are estimating by MSM using simulated data, the natural bootstrap procedure is:\n\nEstimate \\(\\hat{\\theta}\\) by MSM.\nSimulate a “new” dataset from the model at \\(\\hat{\\theta}\\) (this is the bootstrap sample).\nRe-estimate \\(\\hat{\\theta}^{b}\\) from the bootstrap sample (using the same MSM procedure).\nRepeat to build the bootstrap distribution.\n\nHere is a sketch of the procedure. The key idea is that the “data moments” in each bootstrap replication come from a fresh simulation at the estimated \\(\\hat{\\theta}\\), mimicking the sampling variability in the real data.\n\nfunction parametric_bootstrap(θ_hat, B; N_data=1000, N_sim=2000)\n    θ_boot = zeros(B, 3)\n    for b in 1:B\n        # Step 1: Simulate \"data\" from model at θ̂ (with new random seed)\n        A, pars = setup_and_solve(θ_hat)\n        sim_data = simulate_panel(A, pars, N_data; seed=b)\n\n        # Step 2: Compute \"data\" moments from simulated data\n        target_ages = [6, 16, 26, 36]\n        m_boot = vcat(\n            [mean(sim_data.assets[:, t]) for t in target_ages],\n            [var(log.(sim_data.consumption[:, t])) for t in target_ages]\n        )\n\n        # Step 3: Re-estimate by MSM (matching m_boot instead of m_data)\n        x0 = [0.0, log(θ_hat[2]), log(θ_hat[3])]\n        res = optimize(x -&gt; msm_objective(x, m_boot), x0, NelderMead(),\n                       Optim.Options(iterations=200))\n        x = res.minimizer\n        θ_boot[b,:] = [0.8 + 0.19/(1+exp(-x[1])), exp(x[2]), exp(x[3])]\n    end\n    return θ_boot\nend\n\n# Run bootstrap (computationally intensive!)\n# θ_boot = parametric_bootstrap(θ_hat, 100)\n# se_boot = std.(eachcol(θ_boot))\n\nEach bootstrap replication requires solving the dynamic programming problem twice (once for the data simulation, once for each objective function evaluation during optimization). This makes the parametric bootstrap for structural models computationally expensive, but it is conceptually simple and automatically accounts for all sources of estimation uncertainty.",
    "crumbs": [
      "Simulation Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation Methods</span>"
    ]
  },
  {
    "objectID": "lectures/simulation-methods.html#practical-considerations",
    "href": "lectures/simulation-methods.html#practical-considerations",
    "title": "14  Simulation Methods",
    "section": "14.6 Practical Considerations",
    "text": "14.6 Practical Considerations\n\n14.6.1 Simulation Noise and Smoothing\nWhen simulation draws are held fixed across evaluations of the objective function, the simulated objective is a smooth function of \\(\\theta\\) (assuming the model itself is smooth in \\(\\theta\\)). This means standard gradient-based optimizers can be used. If simulation draws are re-drawn at each evaluation, the objective becomes noisy and optimization may fail or converge to the wrong point. Always fix the random seed when evaluating the simulated objective across different \\(\\theta\\) values.\n\n\n14.6.2 How Many Draws?\nFor MSM, consistency holds for any fixed \\(R\\), but efficiency improves with \\(R\\). A practical approach is to start with a moderate \\(R\\) for exploration, then increase \\(R\\) and check that the estimates are stable. If the estimates change substantially, \\(R\\) is too small.\nFor MSL, the bias from taking logarithms of simulated probabilities is of order \\(1/R\\). In practice, \\(R\\) should be large enough that this bias is negligible relative to the standard error.\n\n\n14.6.3 Choosing Moments for MSM\nIn Example 14.2, we chose a small set of moments. In practice, the choice of moments should be guided by:\n\nIdentification: the moments should be informative about the parameters. Moments that are insensitive to a parameter will not help estimate it.\nParsimony: more moments improve efficiency (in principle) but increase computational cost and can lead to poorly conditioned weighting matrices.\nModel fit: if the model cannot match a particular moment, including it will distort the estimates of other parameters. Start with moments the model should be able to match.\n\nThe discussion in the identification chapter on which moments pin down which preference parameters is directly relevant here.\n\n\n14.6.4 Diagonal Weighting Matrices\nWhen using MSM or minimum distance with many moments, the optimal weighting matrix \\(\\mathbf{W}^{*}=\\hat{S}^{-1}\\) can be poorly estimated and lead to erratic results in finite samples. A common and practical alternative is to use a diagonal weighting matrix, where each moment is weighted by the inverse of its variance but cross-moment covariances are ignored. This sacrifices some efficiency but tends to be much more stable in practice.",
    "crumbs": [
      "Simulation Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation Methods</span>"
    ]
  },
  {
    "objectID": "lectures/simulation-methods.html#exercises",
    "href": "lectures/simulation-methods.html#exercises",
    "title": "14  Simulation Methods",
    "section": "14.7 Exercises",
    "text": "14.7 Exercises\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 14.1 Estimate the preference parameters \\((\\beta,\\sigma,\\psi)\\) of the savings model using the method of simulated moments. Take the income process parameters as given from Example 12.2.\n\nChoose a set of target moments from the PSID data (Example 10.1) that are informative about \\((\\beta,\\sigma,\\psi)\\). Justify your choice based on the identification discussion in the savings identification chapter.\nImplement the MSM estimator using the simulation code from Example 14.1.\nReport your estimates and plot the model fit for your chosen moments.\nAssess sensitivity: how do the estimates change when you use a different set of moments?\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 14.2 Continuing from Exercise 14.1:\n\nImplement a parametric bootstrap to compute standard errors for \\((\\hat{\\beta},\\hat{\\sigma},\\hat{\\psi})\\) following the approach in Example 14.4. Use \\(B=100\\) bootstrap replications.\nConstruct 95% confidence intervals for each parameter.\nWhich parameter is most precisely estimated? Which is least? Does this match your intuition from the identification discussion?\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 14.3 Return to the minimum distance estimation of the income process from Example 12.2. Instead of using the identity weighting matrix, implement estimation with:\n\nThe diagonal weighting matrix, where each moment is weighted by the inverse of the variance of the corresponding sample moment.\nThe optimal weighting matrix from Example 13.2.\n\nCompare the estimates and standard errors across all three weighting schemes. How much does the choice of weighting matrix matter in practice?\n\n\n\n\n\n\n\nHorowitz, Joel L. 2001. “The Bootstrap.” In Handbook of Econometrics, 5:3159–3228. Elsevier.\n\n\nNewey, Whitney K, and Daniel McFadden. 1994. “Large Sample Estimation and Hypothesis Testing.” Handbook of Econometrics 4: 2111–2245.",
    "crumbs": [
      "Simulation Methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Simulation Methods</span>"
    ]
  }
]