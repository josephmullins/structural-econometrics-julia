[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Structural Econometrics with Julia",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Five Prototype Models",
    "section": "",
    "text": "This chapter introduces four prototype structural models that we will use throughout the course to illustrate econometric methods. These models serve as running examples for identification strategies, estimation techniques, and computational methods.\nFor the purposes of some examples, we may at times perturb particular aspects of these assumptions. Our first task however is simply to familiarize ourselves with the general structure of the models, along with some numerical methods for solving them.",
    "crumbs": [
      "Five Prototype Models"
    ]
  },
  {
    "objectID": "models/generalized_roy.html",
    "href": "models/generalized_roy.html",
    "title": "1  The Generalized Roy Model",
    "section": "",
    "text": "1.1 Overview\nThe generalized Roy model is a framework for understanding selection into treatment based on heterogeneous gains. Theoretically, it is about the simplest model of choice one could write down, but it has surprisingly deep empirical content.\nRoy (1951) used a version of this model to study occupational choice and introduce the concept of selection. It lies at the heart of most econometric treatments of selection and causal inference (J. Heckman and Vytlacil 2005; J. J. Heckman and Honore 1990).\nOriginally developed to study occupational choice, it has become the canonical model for analyzing treatment effects when individuals select into treatment based on anticipated outcomes.\nThis model introduces fundamental concepts:\nThese ideas are central to modern applied microeconometrics and connect directly to some later identification examples we consider.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/generalized_roy.html#overview",
    "href": "models/generalized_roy.html#overview",
    "title": "1  The Generalized Roy Model",
    "section": "",
    "text": "Selection on unobservables\nTreatment effect heterogeneity\nMarginal treatment effects (MTE)\nLocal average treatment effects (LATE)",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/generalized_roy.html#the-model",
    "href": "models/generalized_roy.html#the-model",
    "title": "1  The Generalized Roy Model",
    "section": "1.2 The Model",
    "text": "1.2 The Model\nThe model is very simple. Let \\(D\\in\\{0,1\\}\\) be a treatment or choice made by each individual in an economy. Individuals make the choice / take the treatment if the utility they derive from \\(D=1\\) exceeds that if \\(D=1\\). Let \\(Z\\) be a vector of observables that influences payoffs. The selection equation is:\n\\[ D = \\mathbf{1}\\{\\mu_{d}(Z) - V \\geq 0\\} \\]\nwhere \\(\\mu_{d}(Z)\\) is a deterministic function of \\(Z\\) and \\(V\\) is a random variable that is unobserved to the econometrician. Some other notes:\n\nThe term \\(\\mu_{d}(Z)-V\\) can be interpreted as the difference in utilities and the function \\(\\mu_{d}\\) can be viewed with the usual welfarist interpretations.\nIn this sense, the selection equation is essentially a binary choice model.\nThis model already builds in some special structure: the unobservables that dermine choices (\\(V\\)) are additively separable with respect to the observable factors \\(Z\\). We’ll return to this in future sections on identification.\n\nThe selection equation is paired with a pair of potential outcome equations:\n\\[\\begin{align}\nY_1 &= \\mu_1(X) + U_1 \\\\\nY_0 &= \\mu_0(X) + U_0\n\\end{align}\\]\nwhere:\n\n\\(X\\subset Z\\) are observed characteristics\n\\(U_1, U_0\\) are unobserved components that determine outcomes\n\nKey assumption: \\((U_1, U_0, V)\\) are jointly distributed, potentially correlated. We’ll later return to the implications of this assumption.\nA canonical example of this model is the returns to schooling, where \\(D\\in\\{0,1\\}\\) is the decision to attend college.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/generalized_roy.html#potential-outcomes-and-observability",
    "href": "models/generalized_roy.html#potential-outcomes-and-observability",
    "title": "1  The Generalized Roy Model",
    "section": "1.3 Potential Outcomes and Observability",
    "text": "1.3 Potential Outcomes and Observability\n\n1.3.1 What We Observe vs. What We Want\nObservable:\n\nTreatment status: \\(D\\)\nActual outcome: \\(Y_{D}\\)\nCovariates: \\(X, Z\\)\n\nNot observable:\n\nCounterfactual outcomes: we don’t see \\(Y_{1-D}\\)\nIndividual treatment effects: \\(\\Delta = Y_{1} - Y_{0}\\)\n\nFundamental Problem of Causal Inference: We never observe both \\(Y_1\\) and \\(Y_0\\) for the same individual.\n\n\n1.3.2 Treatment Effects of Interest\n\nIndividual Treatment Effect (ITE): \\[\\Delta_i = Y_{1i} - Y_{0i}\\] Never observed for any individual.\nAverage Treatment Effect (ATE): \\[\\text{ATE} = E[\\Delta] = E[Y_1 - Y_0]\\] Average gain if we randomly assigned everyone to treatment.\nAverage Treatment on the Treated (ATT): \\[\\text{ATT} = E[\\Delta | D=1] = E[Y_1 - Y_0 | D=1]\\] Average gain for those who actually chose treatment.\nAverage Treatment on the Untreated (ATU): \\[\\text{ATU} = E[\\Delta | D=0] = E[Y_1 - Y_0 | D=0]\\] Average gain for those who chose not to be treated.\n\n\n\n1.3.3 Simulation\nHere is code to simulate data from a generalized Roy model under the assumption that (\\(U_0,U_1\\)) are jointly normally distributed and are the sole source of selection on gains.\nusing Distributions, DataFrames, Statistics\n\n# Simulate Roy model with heterogeneous returns\nfunction simulate_roy_model(n=10000)\n    # Parameters\n    α₁, α₀ = 3.0, 2.5  # Mean log wages\n    σᵤ = 0.3           # Std dev of unobservables\n    ρ = 0.5            # Correlation between U₁ and U₀\n\n    # Generate correlated unobservables\n    # (U₁, U₀) ~ Bivariate Normal\n    Σ = [1.0 ρ; ρ 1.0] * σᵤ^2\n    U = rand(MvNormal([0.0, 0.0], Σ), n)'\n    U₁ = U[:, 1]\n    U₀ = U[:, 2]\n\n    # Individual treatment effects\n    Δ = (α₁ - α₀) .+ (U₁ .- U₀)\n\n    # Generate instrument Z (e.g., family income, distance to college)\n    Z = rand(Normal(0, 1), n)\n\n    # Selection: D = 1 if gain &gt; cost\n    # Cost depends on Z and unobserved V\n    V = rand(Normal(0, 0.5), n)\n    cost_threshold = 0.3 .- 0.4 * Z  # Lower cost if Z is high\n    D = (Δ .+ V) .&gt; cost_threshold\n\n    # Observed outcomes\n    Y₁ = α₁ .+ U₁\n    Y₀ = α₀ .+ U₀\n    Y = D .* Y₁ .+ (1 .- D) .* Y₀\n\n    return DataFrame(\n        Y₁ = Y₁,\n        Y₀ = Y₀,\n        Y = Y,\n        D = D,\n        Δ = Δ,\n        Z = Z\n    )\nend\n\n# Simulate data\ndf = simulate_roy_model(10000)\n\n# Calculate different treatment effects\nATE = mean(df.Δ)\nATT = mean(df[df.D .== 1, :Δ])\nATU = mean(df[df.D .== 0, :Δ])\n\n# Naive comparison\nnaive = mean(df[df.D .== 1, :Y]) - mean(df[df.D .== 0, :Y])\n\nprintln(\"True ATE: \", round(ATE, digits=3))\nprintln(\"True ATT: \", round(ATT, digits=3))\nprintln(\"True ATU: \", round(ATU, digits=3))\nprintln(\"Naive estimator: \", round(naive, digits=3))\nprintln(\"Selection bias: \", round(naive - ATE, digits=3))\nOutput:\nTrue ATE: 0.502\nTrue ATT: 0.647\nTrue ATU: 0.291\nNaive estimator: 0.712\nSelection bias: 0.210\nInterpretation: - ATT &gt; ATE &gt; ATU: Those who select college have higher returns - Naive estimator overestimates ATE due to positive selection bias - Selection on gains: people with high \\(\\Delta\\) choose treatment",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/generalized_roy.html#further-reading",
    "href": "models/generalized_roy.html#further-reading",
    "title": "1  The Generalized Roy Model",
    "section": "1.4 Further Reading",
    "text": "1.4 Further Reading\nFoundational papers:\n\nRoy (1951): “Some Thoughts on the Distribution of Earnings” - Original occupational choice model\nHeckman and Honoré (1990): “The Empirical Content of the Roy Model” - Identification analysis\nImbens and Angrist (1994): “Identification and Estimation of Local Average Treatment Effects” - LATE framework\n\nModern treatments:\n\nHeckman and Vytlacil (2005): “Structural Equations, Treatment Effects, and Econometric Policy Evaluation” - Unifying MTE framework\nHeckman et al. (2006): “Understanding Instrumental Variables in Models with Essential Heterogeneity” - Extensions and applications\n\nEmpirical applications:\n\nWillis and Rosen (1979): “Education and Self-Selection” - Returns to schooling\nCarneiro et al. (2011): “Estimating Marginal Returns to Education” - MTE estimation\n\n\n\n\n\nHeckman, James J, and Bo E Honore. 1990. “The Empirical Content of the Roy Model.” Econometrica: Journal of the Econometric Society, 1121–49.\n\n\nHeckman, James, and Edward Vytlacil. 2005. “Structural equations, treatment effects, and econometric policy evaluation.” Econometrica 73 (3): 669–738.\n\n\nRoy, A. D. 1951. “Some Thoughts on the Distribution of Earnings.” Oxford Economic Papers 3 (2): 135–46. http://www.jstor.org/stable/2662082.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/search.html",
    "href": "models/search.html",
    "title": "2  Job Search Model",
    "section": "",
    "text": "2.1 Overview\nThis section presents a simple model of undirected job search. The model demonstrates how workers optimally choose which job offers to accept based on a reservation wage strategy.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#economic-environment",
    "href": "models/search.html#economic-environment",
    "title": "2  Job Search Model",
    "section": "2.2 Economic Environment",
    "text": "2.2 Economic Environment\nTime is discrete and indexed by \\(t\\) over an infinite horizon. Workers move between employment and unemployment, have linear utility, and cannot save.\n\n2.2.1 Parameters\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n\\(\\lambda\\)\nThe probability an unemployed worker receives a job offer\n\n\n\\(\\delta\\)\nThe probability an employed worker loses their job\n\n\n\\(F_{W}\\)\nThe distribution of wage offers\n\n\n\\(1-\\beta\\)\nThe exponential rate of discounting\n\n\n\\(b\\)\nPer-period utility when unemployed",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#recursive-formulation",
    "href": "models/search.html#recursive-formulation",
    "title": "2  Job Search Model",
    "section": "2.3 Recursive Formulation",
    "text": "2.3 Recursive Formulation\nThe classic approach to solve this model is to write the values of unemployment and employment recursively. For example:\n\\[ U = b + \\beta[(1-\\lambda)U + \\lambda\\int\\max\\{V(w),U\\}dF_{W}(w)] \\] \\[ V(w) = w + \\beta[(1-\\delta)V(w) + \\delta U] \\]",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#model-solution",
    "href": "models/search.html#model-solution",
    "title": "2  Job Search Model",
    "section": "2.4 Model solution",
    "text": "2.4 Model solution\nOne can show that the optimal decision rule of the worker is characterized by a reservation wage \\(w^*\\), defined as \\(V(w^*)=U\\). We can also differentiate the expression for \\(V(w)\\) to get:\n\\[ V'(w) = \\frac{1}{1 - \\beta(1-\\delta)} \\]\nand applying integration by parts gives:\n\\[ U = b + \\beta[U + \\lambda\\int_{w^*}\\frac{1-F_{W}(w)}{1-\\beta(1-\\delta)}dw] \\]\nNow applying the definition of the reservation wage gives the reservation wage equation:\n\\[ w^* = b + \\beta\\lambda\\int_{w^*}\\frac{1-F_{W}(w)}{1 - \\beta(1-\\delta)}dw \\]\nand we can characterize the steady state rate of unemployment as:\n\\[ P[E = 0] = \\frac{h}{h+\\delta} \\]\nwhere \\(h = \\lambda(1-F_{W}(w^*))\\) is the rate at which workers exit unemployment.\nSimilarly, we can show that the steady state fraction of unemployment durations \\(t_{U}\\) is\n\\[ P[t_{U}=t] = h(1-h)^{t} \\]",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#numerical-model-solution",
    "href": "models/search.html#numerical-model-solution",
    "title": "2  Job Search Model",
    "section": "2.5 Numerical Model Solution",
    "text": "2.5 Numerical Model Solution\nTo solve the reservation wage equation numerically, we need to evaluate the integral on the right-hand side and find the value of \\(w^*\\) that satisfies the equation. This requires two key numerical methods: quadrature (for integration) and root-finding.\n\n2.5.1 Gauss-Legendre Quadrature\nWhen integrating numerically, we approximate the integral using a weighted sum at specific evaluation points (nodes):\n\\[ \\int_a^b f(x)dx \\approx \\frac{b-a}{2}\\sum_{k=1}^n w_k f\\left(\\frac{a+b}{2} + \\frac{b-a}{2}x_k\\right) \\]\nwhere \\(x_k\\) are the nodes and \\(w_k\\) are the weights from Gauss-Legendre quadrature. This method is particularly accurate for smooth functions and uses a fixed number of nodes, which is important for automatic differentiation (unlike adaptive methods like in the package QuadGK that adjust the number of nodes based on the integrand).\nLet’s implement a simple Gauss-Legendre integration routine:\n\nusing FastGaussQuadrature, Distributions, Roots\n\n# Fixed-node quadrature for integration (compatible with automatic differentiation)\nfunction integrateGL(f, a, b; num_nodes = 10)\n    nodes, weights = gausslegendre(num_nodes)\n    ∫f = 0.\n    for k in eachindex(nodes)\n        x = (a + b)/2 + (b - a)/2 * nodes[k]\n        ∫f += weights[k] * f(x)\n    end\n    return (b - a)/2 * ∫f\nend\n\n# Evaluate the derivative of the surplus function\ndS(x; F, β, δ) = (1 - cdf(F, x)) / (1 - β*(1 - δ))\n\n# Reservation wage equation (should equal zero at the solution)\nfunction res_wage(wres, b, λ, δ, β, F::Distribution)\n    ub = quantile(F, 0.9999)  # Upper bound of integration\n    integral = integrateGL(x -&gt; dS(x; F, β, δ), wres, ub)\n    return wres - b - β * λ * integral\nend\n\npars = (;b = -5., λ = 0.45, δ = 0.03, β = 0.99, F = LogNormal(1, 1))\nres_wage(1., pars.b, pars.λ, pars.δ, pars.β, pars.F)\n\n-33.6935906934783\n\n\n\n\n2.5.2 Root Finding\nThe reservation wage \\(w^*\\) is the value that makes the reservation wage equation equal to zero. We use the Roots.jl package, which implements efficient root-finding algorithms based on combinations of bisection, secant, and inverse quadratic interpolation methods.\nThe find_zero function takes:\n\nA function to find the root of\nAn initial guess\nThe type of the initial guess (to ensure type stability)\n\n\nfunction solve_res_wage(b, λ, δ, β, F)\n    return find_zero(\n        x -&gt; res_wage(x, b, λ, δ, β, F),\n        eltype(b)(4.)  # Initial guess of $4/hour\n    )\nend\n\nrwage = solve_res_wage(pars.b, pars.λ, pars.δ, pars.β, pars.F)\nprintln(\"Reservation wage: \", round(rwage, digits=2))\n\nReservation wage: 7.23\n\n\nThis approach has the advantage of being compatible with automatic differentiation tools like ForwardDiff, which is a very useful tool in numerical methods.\n\n\n2.5.3 Steady-State Statistics\nUsing the computed reservation wage, we can calculate the steady-state unemployment rate and average duration:\n\n# Compute steady-state statistics\nh = pars.λ * (1 - cdf(pars.F, rwage))  # Exit rate from unemployment\nu_rate = pars.δ / (pars.δ + h)          # Unemployment rate\navg_duration = 1 / h                     # Average duration\n\nprintln(\"Exit rate (h): \", round(h, digits=3))\nprintln(\"Unemployment rate: \", round(u_rate * 100, digits=1), \"%\")\nprintln(\"Average duration: \", round(avg_duration, digits=1), \" periods\")\n\nExit rate (h): 0.074\nUnemployment rate: 28.9%\nAverage duration: 13.6 periods",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#further-reading",
    "href": "models/search.html#further-reading",
    "title": "2  Job Search Model",
    "section": "2.6 Further Reading",
    "text": "2.6 Further Reading\n\nMcCall (1970): “Economics of Information and Job Search” - Original search model\nWolpin (1987): “Estimating a Structural Search Model” - Early structural estimation\nEckstein and van den Berg (2007): “Empirical Labor Search” - Survey of search models\nFlinn and Heckman (1982): “New Methods for Analyzing Structural Models of Labor Force Dynamics” - Duration data analysis",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/savings.html",
    "href": "models/savings.html",
    "title": "3  A Life-Cycle Savings Model",
    "section": "",
    "text": "3.1 Overview\nThis section presents a stylized life-cycle model of consumption and savings. Households make dynamic decisions about consumption and asset accumulation over their lifetime, facing income uncertainty and borrowing constraints.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "models/savings.html#economic-environment",
    "href": "models/savings.html#economic-environment",
    "title": "3  A Life-Cycle Savings Model",
    "section": "3.2 Economic Environment",
    "text": "3.2 Economic Environment\nTime is discrete and indexed by \\(t\\). Individuals live for a finite number of periods, \\(T\\). They derive utility from consumption according to a CRRA utility function:\n\\[ u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma} \\]\nand from “bequests”, which are modeled here as cash on hand net of consumption in the final period:\n\\[ \\nu(a) = \\psi \\frac{a^{1-\\sigma}}{1-\\sigma} \\].\nConsumption can be transferred between periods via a portfolio of one-period bonds (“savings’, \\(a\\)) that can be purchased at the price \\(1 / (1+r)\\), with a prdetermined limit, \\(\\underline{a}\\), on borrowing.\nInviduals receive income \\(y\\) every period that is governed by a deterministic (\\(\\mu_{t}\\)) and stochastic component:\n\\[ \\log(y_{t}) = \\mu_{t} + \\varepsilon_{it} \\]\nwhere \\(\\varepsilon_{it}\\) is a first-order Markov process. A particular case of interest is the case where \\(\\varepsilon\\) is a stationary AR 1 process:\n\\[ \\varepsilon_{it} = \\rho \\varepsilon_{it-1} + \\eta_{it} \\]\nwhere \\(\\eta_{it} \\sim \\mathcal{N}(0,\\sigma^2_{\\eta})\\). The unconditional variance of \\(\\varepsilon_{it}\\) is therefore \\(\\sigma^2_{\\eta} / (1-\\rho^2)\\).",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "models/savings.html#model-solution",
    "href": "models/savings.html#model-solution",
    "title": "3  A Life-Cycle Savings Model",
    "section": "3.3 Model Solution",
    "text": "3.3 Model Solution\nDefine\n\\[ V_{T}(a,\\varepsilon) = \\max_{c}\\left\\{u(c) + \\nu(y + a - c)\\right\\} \\]\nAnd now define the remaining value functions recursively:\n\\[ V_{t}(a,\\varepsilon) = \\max_{c,a'}\\left\\{u(c) + \\beta\\mathbb{E}_{\\varepsilon'|\\varepsilon}V(a',\\varepsilon')\\right\\} \\]\nsubject to:\n\\[ c + \\frac{1}{1+r}a' \\leq y + a \\]\nand\n\\[ a' \\geq \\underline{a}\\]\nwhere \\(\\underline{a}\\) is the borrowing constraint.\nWe’re going to write code to solve the model naively using this recursive formulation. You may already be aware that there are more efficient solution methods that exploit the first order conditions of the problem. Not the focus of our class! Please don’t use the example below as a demonstration of best practice when it comes to solving savings models.\nWe’ll start picking some default parameters.\n\npars = (;\n    T = 45, β = 0.95, σ = 2,ρ = 0.9,ση = 0.1, μ = fill(2.,45), ψ = 5., r = 0.05\n)\n\n(T = 45, β = 0.95, σ = 2, ρ = 0.9, ση = 0.1, μ = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0  …  2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], ψ = 5.0, r = 0.05)\n\n\nNext we’ll write a function that uses Tauchen’s method to approximate the income process as a discrete markov process.\n\nusing Distributions,Random\nusing LinearAlgebra\nΦ(x) = cdf(Normal(),x)\n\nfunction tauchen(ρ,ση,Kϵ)\n    sd = ση/sqrt(1-ρ^2)\n    grid = range(-3sd,stop=3sd,length=Kϵ)\n    Π = zeros(Kϵ,Kϵ)\n    Δ = grid[2]-grid[1]\n    for j=1:Kϵ\n        Π[1,j] = Φ((grid[1] + Δ/2 - ρ*grid[j])/ση)\n        Π[end,j] = 1 - Φ((grid[end] - Δ/2 - ρ*grid[j])/ση)\n        for k=2:(Kϵ-1)\n            Π[k,j] = Φ((grid[k] + Δ/2 - ρ*grid[j])/ση) - Φ((grid[k] - Δ/2 - ρ*grid[j])/ση)\n        end\n    end\n    return Π,grid\nend\n\ntauchen (generic function with 1 method)\n\n\nNow, let’s think about how to solve this model. We have two state variables to track. We have discretized \\(\\varepsilon\\), now let’s discretize assets and define a max operator.\n\nKa = 100\nKϵ = 5\nagrid = LinRange(0,pars.μ[1] * pars.T,Ka) #&lt;- is this a reasonable upper bound? We'll find out!\nΠ,ϵgrid = tauchen(pars.ρ,pars.ση,Kϵ)\npars = (;pars...,Ka,agrid,Π,ϵgrid,Kϵ)\n\nu(c,σ) = c^(1-σ) / (1-σ)\n\nfunction solve_max(V,t,iϵ,ia,pars)\n    (;agrid,ϵgrid,Π,σ,Ka,r,β) = pars\n    cash = exp(pars.μ[t] + ϵgrid[iϵ]) + agrid[ia]\n    amax = 0\n    vmax = -Inf\n    loop = true\n    a = 1\n    while loop && a&lt;Ka\n        c = cash - agrid[a] / (1+r)\n        if c&gt;0\n            #@views v = u(c,σ) + β * dot(Π[:,iϵ],V[:,a,t+1])\n            v = u(c,σ)\n            for iϵ′ in axes(V,1)\n                v += β * Π[iϵ′,iϵ] * V[iϵ′,a,t+1]\n            end\n            if v&gt;vmax\n                vmax = v\n                amax = a\n            end\n        else\n            loop = false\n        end\n        a += 1 #&lt;- move one up the grid space\n    end\n    return amax,vmax\nend\n\nsolve_max (generic function with 1 method)\n\n\nNext, a function that uses this max operator to get the value function for all states in a period, \\(t\\), and records the optimal savings policy.\n\nfunction iterate!(V,A,t,pars)\n    for ia in axes(V,2), iϵ in axes(V,1)\n        A[iϵ,ia,t],V[iϵ,ia,t] = solve_max(V,t,iϵ,ia,pars)\n    end\nend\nfunction terminal_values!(V,pars)\n    (;σ,ψ,agrid) = pars\n    for ia in axes(V,2), iϵ in axes(V,1)\n        V[iϵ,ia] = ψ * u(agrid[ia],σ)\n    end\nend\n\nterminal_values! (generic function with 1 method)\n\n\n\nfunction backward_induction!(V,A,pars)\n    (;ψ,σ,T,agrid) = pars\n    # set the values at T+1 (bequest motives)\n    @views terminal_values!(V[:,:,T+1],pars)\n    for t in reverse(1:T)\n        iterate!(V,A,t,pars)\n    end\nend\n\nbackward_induction! (generic function with 1 method)\n\n\nLet’s check the model solution and time it also.\n\nV = zeros(pars.Kϵ,pars.Ka,pars.T+1)\nA = zeros(Int64,pars.Kϵ,pars.Ka,pars.T)\nbackward_induction!(V,A,pars)\n@time backward_induction!(V,A,pars)\n\n  0.008771 seconds\n\n\nSeems ok. We can plot the policy functions as a sanity check. The plot below shows savings policy at the median wage shock over time at different levels of assets.\n\nusing Plots\n\nplot(1:pars.T,agrid[A[3,1:10:Ka,:]'],legend=false)\n\n\n\n\nYou can see that the discreteness creates some jumpiness in the policy functions. As I said, other solution methods that use interpolation can be more efficient and will create smoother pictures, but since that is not the focus of this class we will use this simple solution method.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "models/savings.html#further-reading",
    "href": "models/savings.html#further-reading",
    "title": "3  A Life-Cycle Savings Model",
    "section": "3.4 Further Reading",
    "text": "3.4 Further Reading\nGourinchas and Parker (2002) and De Nardi (2004) are two classic examples of quantitative applications of the life-cycle savings model.\n\n\n\n\nDe Nardi, Mariacristina. 2004. “Wealth Inequality and Intergenerational Links.” The Review of Economic Studies 71 (3): 743–68. https://doi.org/10.1111/j.1467-937X.2004.00302.x.\n\n\nGourinchas, Pierre-Olivier, and Jonathan A. Parker. 2002. “Consumption over the Life Cycle.” Econometrica 70 (1): 47–89. https://doi.org/10.1111/1468-0262.00269.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html",
    "href": "models/entry-exit.html",
    "title": "4  Firm Entry-Exit Model",
    "section": "",
    "text": "4.1 Overview\nThis section presents a symmetric duopoly model of firm entry and exit decisions. Firms make discrete choices about market participation based on profitability and fixed costs. This model illustrates static discrete choice with strategic interactions and is used in Chapter 5 to demonstrate discrete choice estimation methods.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html#model-ingredients",
    "href": "models/entry-exit.html#model-ingredients",
    "title": "4  Firm Entry-Exit Model",
    "section": "4.2 Model Ingredients",
    "text": "4.2 Model Ingredients\nHere are the basic ingredients of the model:\n\nThere are two firms indexed by \\(f\\in\\{0,1\\}\\)\nThere are \\(M\\) markets indexed by \\(m\\)\nTime is discrete and indexed by \\(t\\)\nEach firm makes an entry decision every period. We let \\(d\\in\\{0,1\\}\\) index this decision to enter or not. Let \\(d(f,m,t)\\) indicate the choice of firm \\(f\\) in market \\(m\\) in period \\(t\\).\nWe let \\(a_{f,m,t}=d(f,m,t-1)\\) indicate whether firm \\(f\\) is active in market \\(m\\) in period \\(t\\), which means they entered in the previous period.\nLet \\(x_{m}\\) be a market-level observable that shifts the profitability of operations in market \\(m\\).\nIn addition to the observed states, each firm draws a pair of idiosyncatic shocks to payoffs in each period, \\(\\epsilon_{f}=[\\epsilon_{f0},\\epsilon_{f1}]\\) that is private information to the firm and is iid over markets, firms, and time periods.\nFirms make their decisions in each period simultaneously\n\nTo simplify notation, suppress dependance of outcomes on the market \\(m\\) and time period \\(t\\). Because we are writing a symmetric model, we will also suppress dependence on \\(f\\). The deterministic component of the payoff to entering is a function of the market primitives (\\(x\\)), the firm’s activity status (\\(a\\)), and the other firm’s entry decision \\(d^\\prime\\):\n\\[ u_{1}(x,a,d^{\\prime}) = \\phi_{0} + \\phi_{1}x - \\phi_{2}d^\\prime - \\phi_{3}(1-a) \\]\nThe payoff to not entering is simply:\n\\[{u}_{0}(x,a) = \\phi_{4}a \\]\nBefore characterizing the solution to the firm’s problem, let’s code up these payoff functions:\n\nu1(x,a,d′,ϕ) = ϕ[1] + ϕ[2]*x - ϕ[3]d′ + ϕ[4]*(1-a)\nu0(a,ϕ) = a * ϕ[5]\n\nu0 (generic function with 1 method)",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html#solving-the-firms-problem",
    "href": "models/entry-exit.html#solving-the-firms-problem",
    "title": "4  Firm Entry-Exit Model",
    "section": "4.3 Solving the firm’s problem",
    "text": "4.3 Solving the firm’s problem\nLet \\(d^*(x,a,a',\\epsilon)\\) be the firm’s optimal decision given the state and the idiosyncratic shock. We will focus on symmetric equilibria so this policy function is sufficient to describe the behavior of both firms.\nThe value to either firm of arriving in a period with state \\((x,a,a')\\) can be written recursively as:\n\\[\n\\begin{aligned}\nV(x,a,a') = \\mathbb{E}_{\\epsilon}\\max\\mathbb{E}_{\\epsilon'}\\big\\{\n    &u_{1}(x,a,d^*(x,a',a,\\epsilon'))+\\epsilon_{1} \\\\\n    &\\quad + \\beta V(x,1,d^*(x,a,a',\\epsilon')), \\\\\n    &u_{0}(x,a) + \\epsilon_{0} \\\\\n    &\\quad + \\beta V(x,0,d^*(x,a',a,\\epsilon'))\\big\\}\n\\end{aligned}\n\\]\nDefine the optimal choice probability in equilibrium as:\n\\[ p(x,a,a') = \\int_{\\epsilon}d^*(x,a,a',\\epsilon)dF(\\epsilon) \\]\nWith this in hand we can integrate out the other firm’s shocks \\(\\epsilon'\\) to get:\n\\[\n\\begin{aligned}\nV(x,a,a') = \\mathbb{E}_{\\epsilon}\\max\\big\\{\n    &\\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) +\\epsilon_{1} \\\\\n    &\\quad + \\beta \\big[p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)\\big], \\\\\n    &a \\phi_{4} + \\epsilon_{0} \\\\\n    &\\quad + \\beta \\big[p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)\\big]\\big\\}\n\\end{aligned}\n\\]\nDefine the choice-specific values as:\n\\[\n\\begin{aligned}\nv_{1}(x,a,a') = \\phi_{0}+\\phi_{1}x &- \\phi_{2}p(x,a',a) \\\\\n    &+ \\beta \\big[p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)\\big]\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\nv_{0}(x,a,a') = a \\phi_{4} + \\beta \\big[p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)\\big]\n\\end{aligned}\n\\]\nSo assuming that \\(\\epsilon\\) is distributed as type I extreme value random variable with location parameter 0 and scale parameter 1 we get analytical expressions for the choice probabilities and the expected value of the maximum:\n\\[ V(x) = \\gamma + \\log\\left(\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))\\right)\\]\nwhere \\(\\gamma\\) is the Euler-Mascheroni constant and\n\\[ p(x,a,a') = \\frac{\\exp(v_{1}(x,a,a'))}{\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))} \\]\nBefore we define equilibrium and think about solving the model, let’s quickly write up the mapping between the other firm’s choice probabilities and the choice values:\n\n# Fixing x, assume that V is stored as a 2 x 2 array\n# The argument p is the current guess of p(x,a',a)\nfunction choice_values(x,a,p,V,ϕ,β)\n    v0 = u0(a,ϕ) + β * p * V[1,2] + β * (1-p) * V[1,1]\n    v1 = u1(x,a,p,ϕ) + β * p * V[2,2] + β * (1-p) * V[2,1]\n    return v0,v1\nend\n\nchoice_values (generic function with 1 method)\n\n\nIn principle we could iterate on this mapping to find (for a fixed \\(p\\)), the firm’s optimal solution. But that won’t be an efficient way to try and solve for the equilibrium.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html#equilibrium",
    "href": "models/entry-exit.html#equilibrium",
    "title": "4  Firm Entry-Exit Model",
    "section": "4.4 Equilibrium",
    "text": "4.4 Equilibrium\nThe solution concept for this model is Markov Perfect Equilibrium. Fixing the market \\(x\\), here the equilibrium be characterized as a fixed point in the value function \\(V\\) and choice probabilities, \\(p\\). In words, equilibrium is summarized by a \\(V\\) and a \\(p\\) such that:\n\nGiven \\(p\\), \\(V\\) is a fixed point in the recursive formulation of values; and\n\\(p\\) are the optimal choice probabilities of each firm given \\(V\\) and given the other firm’s choice probabilities are \\(p\\).\n\nHow should we solve for this symmetric equilibrium? We could try iterating on \\(V\\) and \\(p\\) as follows:\n\n# V is a 2x2 array with values\n# p is a 2x2 array with choice probabilities\nfunction iterate_model(V,p,x,ϕ,β)\n    Vnew = copy(V)\n    pnew = copy(p)\n    for a′ in axes(V,2)\n        for a in axes(V,1)\n            p′ = p[a′,a]\n            v0,v1 = choice_values(x,a-1,p′,V,ϕ,β)\n            pnew[a,a′] = exp(v1) / (exp(v0)+exp(v1))\n            Vnew[a,a′] = log(exp(v0)+exp(v1))\n        end\n    end\n    return Vnew,pnew\nend\n\nfunction solve_by_iteration(x,ϕ,β; max_iter = 1000, verbose = false)\n    V0 = zeros(2,2)\n    p0 = fill(0.1,2,2)\n    err = Inf\n    iter = 1\n    while err&gt;1e-10 && iter&lt;max_iter\n        V1,p1 = iterate_model(V0,p0,x,ϕ,β)\n        err = maximum(abs.(V1 .- V0))\n        if mod(iter,100)==0 && verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        V0 = V1\n        p0 = p1\n        iter += 1\n    end\n    return V0,p0\nend\n\nβ = 0.95\nϕ = 2 * [1.,0.1,0.5,2.,0.5]\nsolve_by_iteration(0.,ϕ,β; verbose = true)\n\nIteration 100, error is 0.04823924738592211\nIteration 200, error is 0.001242310554474102\nIteration 300, error is 5.032709619001707e-5\nIteration 400, error is 2.123540213005981e-6\nIteration 500, error is 8.863101186307176e-8\nIteration 600, error is 3.693557459882868e-9\nIteration 700, error is 1.538467131467769e-10\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nThis seems to work! But notice that it takes a while for the iteration to converge. Also, unlike the single agent case, there is no guarantee that this iteration is always a contraction.\nWe can also solve this model relatively easily using Newton’s Method and the magic of Automatic Differentiation. To do this, we’ll solve over the pair of choice-specific values \\(v_{0}\\) and \\(v_{1}\\) (these encode both values and choice probabilities) and store these values as a vector instead of an array:\n\nusing ForwardDiff, LinearAlgebra\n\n# this function returns V as a 2 x 2 array given the vector of choice specific values in V\nfunction calc_V(v)\n    idx = LinearIndices((2,2,2))\n    [log(exp(v[idx[1,1+a,1+a′]]) + exp(v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n# this function returns choice probabilities as a 2x2 array given the vector v\nfunction calc_p(v)\n    idx = LinearIndices((2,2,2))\n    [1 / (1+exp(v[idx[1,1+a,1+a′]] - v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n\nfunction iterate_model_v(v,x,ϕ,β)\n    idx = LinearIndices((2,2,2)) #&lt;- this is for convenient indexing over v\n    vnew = copy(v)\n    V = calc_V(v)\n    for a′ in axes(idx,3)\n        for a in axes(idx,2)\n            i0′ = idx[1,a′,a] #&lt;- this locates the position in v for v_{0}(x,a',a)\n            i1′ = idx[2,a′,a] #&lt;- this locates the position in v for v_{1}(x,a',a)\n            p = 1 / (1 + exp(v[i0′] - v[i1′]))\n            v0,v1 = choice_values(x,a-1,p,V,ϕ,β)\n            vnew[idx[1,a,a′]] = v0\n            vnew[idx[2,a,a′]] = v1\n        end\n    end\n    return vnew\nend\n\nF(v,x,ϕ,β) = v .- iterate_model_v(v,x,ϕ,β)\nfunction solve_model_newton(x,ϕ,β;max_iter = 10, verbose = false)\n    v = zeros(8)\n    dF(v) = ForwardDiff.jacobian(y-&gt;F(y,x,ϕ,β),v)\n    err = Inf\n    iter = 1\n    while (err&gt;1e-10) && (iter&lt;max_iter)\n        Fv = F(v,x,ϕ,β)\n        dFv = dF(v)\n        vnew = v - inv(dFv) * Fv\n        err = maximum(abs.(Fv))\n        if verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        iter += 1\n        v = vnew\n    end\n    return v\nend\n\nsolve_model_newton(0.,ϕ,β;verbose = true);\n\nIteration 1, error is 6.158489821531948\nIteration 2, error is 1.7766463237555712\nIteration 3, error is 0.056247498263360285\nIteration 4, error is 0.00028434628951856666\nIteration 5, error is 3.4473004006940755e-8\nIteration 6, error is 1.4210854715202004e-14\n\n\nLet’s try timing each solution method to quickly compare:\n\nsolve_model_newton(0.,ϕ,β)\nsolve_by_iteration(0.,ϕ,β)\n\n@time solve_model_newton(0.,ϕ,β)\n@time solve_by_iteration(0.,ϕ,β)\n\n  0.000092 seconds (230 allocations: 54.688 KiB)\n  0.000331 seconds (4.29 k allocations: 234.531 KiB)\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nIn this case Newton’s method is faster. Let’s double check that both methods return the same answer:\n\nv0 = solve_model_newton(0.,ϕ,β)\nV0,p = solve_by_iteration(0.,ϕ,β)\np1 = calc_p(v0)\n[p p1]\n\n2×4 Matrix{Float64}:\n 0.910765   0.99055   0.910765   0.99055\n 0.0524759  0.277297  0.0524759  0.277297\n\n\nLooks good! We can re-use this code when we get to thinking about estimation later on. To do this we will have to solve the model for different values of \\(x_{m}\\), but that can be done by using this code and iterating (potentially in parallel) over different values of \\(x\\).\nIf you play around with parameters, you will see how convergence times may change and that solution methods are not always stable, especially when choice probabilities in equilibrium are very close to one or zero.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html#further-redaing",
    "href": "models/entry-exit.html#further-redaing",
    "title": "4  Firm Entry-Exit Model",
    "section": "4.5 Further Redaing",
    "text": "4.5 Further Redaing\nEricson and Pakes (1995) and Aguirregabiria and Mira (2007) are both classic entries in this literature.\n\n\n\n\nAguirregabiria, Victor, and Pedro Mira. 2007. “Sequential Estimation of Dynamic Discrete Games.” Econometrica 75 (1): 1–53.\n\n\nEricson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry Dynamics: A Framework for Empirical Work.” The Review of Economic Studies 62 (1): 53–82. https://doi.org/10.2307/2297841.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/dynamic-labor-supply.html",
    "href": "models/dynamic-labor-supply.html",
    "title": "5  A Simple Labor Supply Model",
    "section": "",
    "text": "5.1 Model Setup and Solution\nConsider a dynamic labor supply model (with no uncertainty) where each agent \\(n\\) chooses a sequence of consumption and hours, \\(\\{c_{t},h_{t}\\}_{t=1}^{\\infty}\\), to solve: \\[ \\max \\sum_{t=0}^\\infty \\beta^{t} \\left(\\frac{c_{t}^{1-\\sigma}}{1-\\sigma} - \\frac{\\alpha_{n}^{-1}}{1 + 1/\\psi}h_{t}^{1+1/\\psi}\\right)\\] subject to the intertemporal budget constraint: \\[ \\sum_{t}q_{t}c_{t} \\leq A_{n,0} + \\sum_{t}q_{t}W_{n,t}h_{t},\\qquad q_{t} = (1+r)^{-t}.\\] Let \\(H_{n,t}\\) and \\(C_{n,t}\\) be the realizations of labor supply for agent \\(n\\) at time \\(t\\). Labor supply in this model obeys: \\[H_{n,t}^{1/\\psi} = (\\alpha_{n}W_{n,t})C^{-\\sigma}_{n,t}.\\] To simplify below, assume that \\(\\beta=(1+r)^{-1}\\), so that the optimal solution features perfectly smoothed consumption, \\(C^*_{n}\\). Making appropriate substitutions gives \\(C^*_{n}\\) as the solution to: \\[ \\left(\\sum_{t}q_{t}\\right)C^*_{n} = \\sum_{t}\\left(q_{t}W_{n,t}^{1+\\psi}\\right)\\alpha_{n}^{\\psi}(C_{n}^*)^{-\\psi\\sigma} + A_{n,0}.\\]",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Simple Labor Supply Model</span>"
    ]
  },
  {
    "objectID": "models/dynamic-labor-supply.html#code-to-solve-the-model",
    "href": "models/dynamic-labor-supply.html#code-to-solve-the-model",
    "title": "5  A Simple Labor Supply Model",
    "section": "5.2 Code to solve the model",
    "text": "5.2 Code to solve the model\nThere is only one object to solve here which is consumption given a sequence of net wages. If one were to assume also constant wages the function below solves optimal consumption.\n\nusing Optim\nfunction solve_consumption(r,α,W,A,σ,ψ)\n    Q = 1/ (1 - 1/(1+r))\n    f(c) = (Q * c - Q * W^(1 + ψ) * α^ψ * c^(-σ*ψ) - A)^2\n    r = Optim.optimize(f,0.,A+W)\n    return r.minimizer\nend\n\nsolve_consumption (generic function with 1 method)",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Simple Labor Supply Model</span>"
    ]
  },
  {
    "objectID": "models/dynamic-labor-supply.html#code-to-simulate-a-cross-section",
    "href": "models/dynamic-labor-supply.html#code-to-simulate-a-cross-section",
    "title": "5  A Simple Labor Supply Model",
    "section": "5.3 Code to simulate a cross-section",
    "text": "5.3 Code to simulate a cross-section\nHere we’ll assume that wages, tastes for work, and assets co-vary systematically. For simplicity we’ll use a multivariate log-normal distribution.\nBelow is code to simulate a cross-section of 1,000 observations.\n\nusing Distributions\nfunction simulate_data(σ,ψ,r,N)\n    ch = [0.3 0. 0.; 0.5 0.5 0.; 0.4 0.8 1.8]\n    Σ = ch * ch'\n    X = rand(MvNormal(Σ),N)\n    α = exp.(X[1,:])\n    W = exp.(X[2,:])\n    A = exp.(X[3,:])\n    C = [solve_consumption(r,α[i],W[i],A[i],σ,ψ) for i in eachindex(A)]\n    @views H = exp.( X[1,:] .+ ψ .* X[2,:] .- ψ * σ .* log.(C) )\n    return (;α,W,A,C,H)\nend\n\n# assume risk-aversion of 2 and frisch of 0.5\nσ = 2.\nψ = 0.5\nr = 0.05\n\ndat = simulate_data(σ,ψ,r,1_000)\n\n(α = [0.8325625872008392, 0.9962630352835987, 0.8885806746692435, 1.0925701005945476, 0.979694089236883, 1.0836163138967156, 1.0331845132092115, 1.1752042758339816, 0.8430023365714854, 1.4876767291144681  …  1.1206653578445815, 1.0154072855426861, 0.8549042815472401, 0.8969690930290717, 1.3941648777204114, 0.7912102235012285, 1.0938611999642693, 0.8021512521340305, 1.2176718460396716, 1.8163966968053349], W = [1.1480125466348308, 1.0591778087855994, 0.6141750014717905, 2.4062527204162683, 0.9515391648667579, 1.4694506696800311, 2.163269816911264, 1.1095567931628112, 0.4213294924515321, 1.0757518906453958  …  0.8040407651475204, 1.7064189700818648, 0.7416468930397977, 0.7406881303740364, 0.5986946904751422, 0.2508092368097511, 2.2250003526030175, 0.5224764274006107, 0.9799857092690983, 1.4170755574152123], A = [34.954371964718916, 69.04149024036246, 0.10642255562358229, 6.1457236413797425, 12.091008629864714, 2.7819511555653023, 2.9945446049005966, 0.11309172021659913, 1.478521981687108, 0.5549403123623208  …  6.643346027575894, 10.681885071519977, 0.0031490102527746084, 0.3948753487051468, 0.036458214755153685, 0.21163283761176657, 1.1894745715936748, 0.5266191646516406, 14.53176268100821, 1.4341312243613662], C = [2.1794608123486925, 3.59070273905277, 0.6761250459659739, 2.1269719912185607, 1.288680812562386, 1.4295585579837027, 1.871075038928553, 1.12831249003358, 0.5375369701111443, 1.1798588416729197  …  1.046008602780442, 1.7744901918035187, 0.7447958815305814, 0.7864584756212141, 0.6351528873514529, 0.3393339393423258, 1.8916444375712305, 0.5942598960199967, 1.4369718900802375, 1.5423453879369953], H = [0.40929943369630645, 0.2855479196058828, 1.0299494441046484, 0.7968162077387962, 0.7415807703196826, 0.9188636907426071, 0.8121608078155509, 1.0971316294222433, 1.0179615385119214, 1.3077796563189361  …  0.9606821578909431, 0.7474970915192722, 0.9885048773507075, 0.9815657004947512, 1.6983944589243865, 1.1677136441386682, 0.8625568972206313, 0.9756930100961451, 0.8388645920473728, 1.401927541932])",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Simple Labor Supply Model</span>"
    ]
  },
  {
    "objectID": "models/dynamic-labor-supply.html#further-reading",
    "href": "models/dynamic-labor-supply.html#further-reading",
    "title": "5  A Simple Labor Supply Model",
    "section": "5.4 Further Reading",
    "text": "5.4 Further Reading\nMaCurdy (1981) and Blundell and Walker (1986) both consider estimation of closely related models of life-cycle labor supply.\n\n\n\n\nBlundell, Richard, and Ian Walker. 1986. “A Life-Cycle Consistent Empirical Model of Family Labour Supply Using Cross-Section Data.” The Review of Economic Studies 53 (4): 539–58.\n\n\nMaCurdy, Thomas E. 1981. “An Empirical Model of Labor Supply in a Life-Cycle Setting.” Journal of Political Economy 89 (6): 1059–85.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Simple Labor Supply Model</span>"
    ]
  },
  {
    "objectID": "lectures/why_models.html",
    "href": "lectures/why_models.html",
    "title": "6  How and Why to Use Models",
    "section": "",
    "text": "6.1 When you possibly don’t need a model.\nSlides for this chapter\nBefore we dive into methods, it will be helpful to review a number of important use cases for quantitative economic models. Why use a model? This might seem like a strange question to devote time to, given that there is no such thing as economics without them. Still, when you are out in the world presenting your research, you may sometimes encounter this question. Indeed, you will read many important and useful applied papers that have no need of an economic model, so it’s not so surprising to think that there are research questions that do not demand a structural estimation exercise. What is my model for? How is it central to answering my research question?\nA lot of students, when they write their first paper, end up posing simple questions like “What is the effect of policy \\(X\\) on outcome \\(Y\\)”? Often, answering these questions requires nothing more than simple statistical models of causality (such as the Potential Outcomes Model or the Generalized Roy Model, which formalize causality in terms of potential outcomes).\nIn particular, if your question concerns the causal effect of an observed historical change in some variable, you likely won’t anything more elaborate than these simple frameworks. Consider below three examples of quasi-experimental variation from our prototype models that answer particular research questions without needing specification of the underlying models.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "lectures/why_models.html#when-you-possibly-dont-need-a-model.",
    "href": "lectures/why_models.html#when-you-possibly-dont-need-a-model.",
    "title": "6  How and Why to Use Models",
    "section": "",
    "text": "6.1.1 Social Security\nIn the life-cycle savings model, consider the inclusion of a social security system that provides income at older ages. The budget constraint becomes:\n\\[ c_{t} + a_{t+1}/(1 + r) \\leq (1-\\tau)y_{t} + \\mathbf{1}\\{t\\geq 65\\}b \\]\nso that individuals become eligible for a benefit, \\(b\\), after age 65, and pay into the system with proportional taxes, \\(\\tau\\). Suppose that the age of eligibility for social security is decreased, unexpectedly, from 65 to 60. And suppose that you have a repeated cross-section of data on individual consumption and age from periods before and after the unannounced reform.\nLet’s consider an approach to identifying objects of interest without specifying the full underlying model. To begin, let \\(t^*\\) index cohorts by their age at the time of policy announcement. Notice that \\(t^*\\) indicates both a cohort and a treatment. Suppose your question is: “What was the effect of this eligibility expansion on consumption at age \\(t\\) for a cohort aged \\(t^*\\) at the time of expansion?” Instead of specifying modeling assumptions, let’s specify how far one can get with a difference-in-differences approach.\nTo specify the causal effects of interest, we’ll use the language of potential outcomes. Let \\(C_{t*,t}(1)\\) be the potential outcome – a random variable – indicating consumption of an individual in cohort \\(t^*\\) at age \\(t\\) under the policy announcement. Similarly let \\(C_{t^*,t}(0)\\) be their corresponding potential outcome under the counterfactual: if the policy had never been announced. The target parameter identified by the research question above is \\[ \\alpha_{t*,t} = \\mathbb{E}[C_{t*,t}(1) - C_{t^*,t}(0)] \\] If there is sufficient variation, this parameter can be identified by assuming parallel trends with an untreated reference cohort. For the sake of this example, consider individuals who were 65 at the time of the policy announcement and hence (in principle) unaffected by the policy change. Parallel trends assumes that for some \\(t^*&lt;65\\) and for a pair of ages \\(t\\geq t^*\\), \\(s&lt;t^*\\): \\[ \\mathbb{E}[C_{t^*,t}(0)] - \\mathbb{E}[C_{65,t}(0)] = \\mathbb{E}[C_{t^*,s}(0)] - \\mathbb{E}[C_{65,t}] \\]\nAll we need then to identify \\(\\alpha_{t,t*}\\) is consumption for both cohorts at age \\(s\\) as well as at age \\(t\\), which allows us to construct the counterfactual \\(\\mathbb{E}[C_{t*,t}(0)]\\) in terms of observable quantities: \\[ \\alpha_{t,t*} = \\mathbb{E}[C_{t^*,t}(1)] - \\mathbb{E}[C_{65,t}(0)] - (\\mathbb{E}[C_{t^*,s}(0)] - \\mathbb{E}[C_{65,s}(0)]) \\] Note of course this was for a specific cohort, using a specific other cohort and a specific age \\(s\\) with which to construct the counterfactual. One could use many other cohorts and ages to do this, as is assumed by the common regression specification: \\[ \\mathbb{E}[C_{t^*,t}] = \\gamma_{t^*} + \\mu_{t} + \\alpha_{t^*,t}\\mathbf{1}\\{t\\geq t^*\\} \\] which implicitly layers in a stricter and interconnected set of parallel trends assumptions.\nTo proceed, we have only to defend the parallel trends assumption, which many view as less burdensome compared to defending the many layers of assumptions in a quantitative model. Of course, the farther apart two cohorts are from each other, the stronger this assumption might appear, and the regression specification does not allow us to select “more ideal” control groups for each cohort (Goodman-Bacon 2021).\n\n\n6.1.2 Firm Entry\nConsider the firm entry model. Suppose you have panel data \\((A_{m,t,1},A_{m,t,2},Z_{m,t})_{m=1,t=1}^{M,T}\\) on a set of markets (indexed by \\(m\\)) in a number of periods (indexed by \\(t\\)). Recall that \\(A_{m,t,j}\\) indicates whether firm \\(j\\) is active in market \\(m\\) at time \\(t\\), and firm entry occurs when \\(A_{m,t+1,j}-A_{m,t,j}=1\\). Recall that \\(X_{m}\\) is a a market-level factor that is unobservable here. Let \\(Z\\in\\{0,1\\}\\) indicate the presence of a policy that applies locally to market \\(m\\). For the purposes of this example, let’s say it’s a local minimum wage policy. To incorporate the policy \\(Z\\), suppose we write:\n\\[ u_{1}(x,a,d') = \\phi_{0} + \\phi_{1}x - \\phi_{2}d' - \\phi_{3}(1-a) + \\phi_{4}z \\]\nso that \\(\\phi_{4}\\) embodies the effect of the policy on payoffs for the firm. Finally, for simplicity, let’s assume that \\(Z_{m,1}=0\\) for all states initially, and in period \\(t^*\\) a subset of states adopt the policy permanently and that this adoption is unanticipated.\nSuppose your question is: “What is the effect of the minimum wage on firm entry?” Let \\(N_{m,t} = D_{m,t,1}+D_{m,t,2}\\) be the number of participating firms in market \\(m\\) at time \\(t\\). Let \\(N_{\\tau,m}(z)\\) be the potential outcomes of \\(N\\) in market \\(m\\), \\(\\tau\\) periods after the adoption of the minimum wage policy. Let’s define the dynamic effect of treatment on the treated (the effect of the minimum wage on markets that adopt it) as:\n\\[ \\alpha_{\\tau} = \\mathbb{E}[N_{\\tau}(1) - N_{\\tau}(0)|Z = 1] = \\mathbb{E}[\\Delta_{\\tau}|Z=1] \\]\nOur model doesn’t outline a theory of why certain markets adopt the minimum wage and why others don’t, but it does highlight that the effects of the policy will differ across markets, so it is important to account for heterogeneity: if there is any selection into policy adoption, we know that \\(\\mathbb{E}[\\Delta_{\\tau}|Z=1] \\neq \\mathbb{E}[\\Delta_{\\tau}]\\).\nA parallel trends assumption that justifies the event-study approach would be: \\[ \\mathbb{E}[N_{t}(0)|Z=1] - \\mathbb{E}[N_{t}(0)|Z=0] = \\text{constant}. \\] Note that if we assume that each market is in the ergodic distribution governed by the Markov Perfect Equilibria, then the distribution of \\(N\\) is stationary in each market absent the policy intervention and the parallel trends assumption is justified. The event-study specification: \\[ D_{m,t} = \\gamma_{m} + \\mu_{t} + \\mathbf{1}\\{t\\geq t^*\\}\\alpha_{t-t^*} + \\epsilon_{m,t} \\] would then robustly identify the average effect of the policy among the markets that adopted it. This is partly true because the timing of adoption was uniform. We should note that if adoption was staggered, given that there may be heterogeneous treatment effects, a regression-based approach to the event study would deliver some weighted average of these impacts that is hard to interpret (Goodman-Bacon 2021).\n\n\n6.1.3 Bundles of Tax Reforms\nFinally, consider a suite of tax reforms in the dynamic labor supply model. Suppose that there are three states, \\(A\\), \\(B\\), \\(C\\). Suppose that each runs a different experiment where they introduce a different set of taxes and transfers. Let \\(\\mathcal{Y}_{j}\\) indicate a net income function for each state \\(j\\). Consider the following examples:\n\\[\\begin{align}\n\\mathcal{Y}_{A}(W,H) = b_{A} + (1-\\tau_{A})WH \\\\\n\\mathcal{Y}_{B}(W,H) = WH + \\sum_{k=0}^{5}\\tau_{k}(WH-\\overline{E}_{k})\\mathbf{1}\\{WH&gt;\\overline{E}_{k}\\} \\\\\n\\mathcal{Y}_{C}(WH) = WH(1-\\tau_{C}) + \\mathbf{1}\\{H&gt;20\\}b_{C}\n\\end{align}\\]\nAnd suppose that these participants do not anticipate their assignment to treatment in this experiment, which is expected to last for 3 periods. Let \\(Z_{j}\\in\\{0,1\\}\\) indicate assignment to either treatment or control group in state \\(j\\).\nIf your research question is: “What is the effect of each unannounced, temporary, tax reform on labor supply?” then one could simply compare the means of treatment and control in each state:\n\\[ \\mathbb{E}[H|Z_{j}=1, j] - \\mathbb{E}[H|Z_{j} = 0, j] \\]\nwhich uncovers the causal effect of \\(Z\\) by virtue of random assignment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "lectures/why_models.html#reasons-to-use-a-model",
    "href": "lectures/why_models.html#reasons-to-use-a-model",
    "title": "6  How and Why to Use Models",
    "section": "6.2 Reasons to Use a Model",
    "text": "6.2 Reasons to Use a Model\nHaving covered these three (perfectly valid) applications of quasi-experimental methods to answer specific research questions, let us now consider some questions that these methods don’t answer and consider the useful role that models can play in these (and related) contexts.\n\n6.2.1 When the question can’t be articulated without one\nPerhaps the most obvious reason to use a model is when you research question simply cannot be articulated without one. Some of the most useful insights from economic modeling are statements about economic efficiency, the potential for policies to resolve market inefficiencies, and the design of policies. Some examples:\n\nIn the labor supply model, given a particular weighted welfare objective, what does the optimal system of taxes and transfers look like?\nWhat is the cheapest way to incentivize competition between two firms in dynamic duopoly?\nWhat is are the welfare costs of incomplete markets in the life-cycle savings model?.\n\n\n\n6.2.2 To make welfare calculations\nRevealed preference is one of the more powerful tools in an economist’s toolkit: if we treat individuals in our data as people who know what they like, we can try to infer their preferences and decide how they value different policy environments. For example:\n\nHow do individuals value the social security program introduced in the savings model? How would they value a program with a different combination of taxes (\\(\\tau\\)) and payments \\(b\\)?\nIn the labor supply model, what is the sum of individual’s willingness to pay for a lumb sum payment \\(b\\) that is financed by proportion taxes \\(\\tau\\)? We’ll come back to this one below.\n\n\n\n6.2.3 To make sense of otherwise puzzling data\nHere it’s hard to look past the most fundamental and basic causal question in our profession: what is the effect of price on quantities? You know of course that this is a silly question, but we only know this because the theory of supply and demand is so fundamental to our view of the world.\nSuppose you observe prices and quantities in a market over time. Without the theory of supply and demand, all you would see is a cloud of points.\n\n\n\n\n\n\n\nFigure 6.1: Shifting supply and demand curves trace out equilibrium points with no apparent pattern\n\n\n\n\nWith the theory of supply and demand, we understand that each point is the simultaneous equilibrium outcome of two underlying structural relationships in equilibrium. Phillip and / or Sewall Wright proposed the solution: Instrumtal Variables, which is perhaps the earliest known example of an estimated structural model.\n\n\n6.2.4 When variation does not identify the counterfactual of interest\nIn our examples above, you may have noted that we were careful to very specifically define the “treatment” in order to specify the causal object of interest. Models can help us articulate just exactly what kind of causal parameters can and cannot be identified by observed variation, as well as outlining a specific set of assumptions under which related counterfactuals can be forecast even though they are not exactly replicated by existing variation. Each section below discussed a number of examples in the context of each application. Researchers often frame this as a question of internal vs external validity, but there are too many interesting examples in the “external validity” column to not discuss them in more depth. In general, a key point made by Heckman and Vytlacil (2005) is that estimands from simple statistical mdoels designed to infer causal effects (such as those we get from difference-in-differences, IV, and regression discontinuity) are rarely parameters of exact policy interest.\nWe’ll use examples to explore these ideas.\n\n6.2.4.1 Social Security\nTo make the example concrete, let’s make some additional simplifying assumptions for the savings model. These make the quantitative model a bit less interesting, but help us think through the issues. Specifically, let’s assume:\n\nEach individual faces a known sequence \\(\\{y_{n,t}\\}_{t=1}^{T}\\) of income realizations.\nAgents face a natural borrowing constraint, yielding an intertemporal borrowing constraint at each \\(t\\): \\[ \\sum_{s=t}^{T}q_{s-t}c_{s} \\leq a_{t} + \\sum_{s=t}^{T}q_{s-t}(1-\\tau)y_{n,s} + \\sum_{s\\geq 65}^{T}q_{s-t}b \\] where \\(q_{\\tau} = 1/(1+r)^{\\tau}\\) is the price of a unit of consumption \\(\\tau\\) periods ahead.\nSet \\(\\beta(1+r)=1\\), and \\(\\psi=0\\) (no bequest motive), indicating that agents will elect to perfectly smoooth their consumption over periods so that consumption is equal to the net present value of net income (something they can do due to the natural borrowing constraint).\n\nWith these assumptions, we can write the mean effect on consumption for cohort \\(t^*\\) at any age \\(t\\geq t^*\\) as simply the effect of the announcement on the NPV of net income at age \\(t^*\\):\n\\[ \\Delta C_{t^*,t} = \\sum_{s=60}^{64}q_{s-t^*}b + \\sum_{s=t^*}^{T}(\\tau - \\tau')\\overline{y}_{t^*,t} \\] where \\(\\overline{y}_{t^*,t}\\) is average income for cohort \\(t^*\\) at age \\(t\\).\nWith these assumptions, note that:\n\nThe parallel trends assumption holds: under the counterfactual of no policy change, differences across cohorts are constant with age (\\(\\mu_{t}\\) can be normalized to zero).\nThe difference-in-difference approach therefore robustly identifies, with \\(\\alpha_{t^*,t}\\), the causal effect of the policy on cohort \\(t^*\\) at age \\(t\\).\nThe model implies that \\(\\alpha_{t^*,t}\\) is constant with \\(t\\).\nEach effect depends on how each cohort \\(t^*\\) expects the policy expansion to be financed, through the change in marginal tax rates \\(\\tau' - \\tau\\). We haven’t specified financing constraints and hence we cannot speculate on this without more structure.\n\nAre the parameters \\(\\alpha_{t^*,t}\\) policy relevant quantities? They are certainly informative, but now that eligibility has been expanded, these effects don’t tell us about the effect of future changes in policy. They don’t even (without additional assumptions) tell us what the effect would be if the expansion were repealed. We need more theory and assumptions to extrapolate. This is a good task for economic models!\nThe points below suggest some compelling counterfactuals that the DD approach does not recover.\n\nThe total effect of the social security policy (not just the expansion).\nThe effect of additional changes in the age of eligibility.\nThe effect of changes in \\(b\\) and \\(\\tau\\) on consumption at different ages, and at differen horizons of anticipation.\nIf there is any reason to think that effects are heterogeneous by cohort (if they face different wage profiles, for example), we also cannot construct estimates on the effect on cohort \\(t^*\\) if they had known learned about the policy at any age other than \\(t^*\\) (there is a one-to-one mapping between cohort and treatment).\nThe effect on consumption for cohorts who have known about the expansion for their whole life-cycle.\nThe effect of the expansion with under alternative financing arrangements.\n\n\n\n6.2.4.2 Tax Reform\nFor concreteness, let’s consider the effect of reform \\(A\\) on labor supply, when \\(\\beta(1+r)=1\\) and optimal consumption is stationary over time. It is given by: \\[\\Delta H_{n,t} = \\psi\\log(1-\\tau) - \\psi\\sigma\\Delta \\log\\left(C^*_{n}\\right). \\] The consumption response \\(\\Delta C^*_{n}\\) embodies the income effect and can be solved by plugging optimal labor supply into the intertemporal budget constraint.\nThis concrete example helps us to understand three more general points about each reform:\n\nThe average treatment effect depends on income effects, which in turn depend on the perceived length of time that the tax reform is enforced.\nThe model exhibits lots of heterogeneity in treatment effects. As such, if there are differences in underlying distributions of wages or work costs, we should expect different impacts.\n\nThus, although each welfare experiment robustly identifies the effect of tax reform \\(A\\) and population \\(A\\), tax reform \\(B\\) on population \\(B\\), and so forth. It does not identify:\n\nThe effect of any tax reform with different persistence (real or perceived). Moreover, one would want to interpret the experimental findings very carefully to ensure that individuals in the experiment were given adequate information about the length of time of the experiment.\nThe effect of tax reform \\(A\\) on population \\(B\\), \\(C\\), etc, and likewise for tax reforms \\(B\\), \\(C\\), etc on alternative populations.\nThe distribution of treatment effects at each location.\nThe effect of tax reforms that are already partly anticipated by individuals.\nThe effect of a scaled up tax reform, where equilibrium effects on wages might appear.\nThe distribution of effects of each tax reform. Here the model can be used to be interpret available panel data and invert out distributions in observed labor market productivities and work costs. One can then return to the experimental data and estimate average treatment effects along these latent dimensions.\n\n\n\n6.2.4.3 Entry-Exit Model\nBy now we have made the point several ways, but when it comes to the model of dynamic duopoly, we can note that the model-free approach does not identify:\n\nThe effect of the minimum wage policy when it’s introduction is anticipated several periods earlier.\nThe effect of the minimum wage on the markets that do not adopt it.\nThe effect of repealing the minimum wage.\nThe effect of nominal changes to the minimum wage.\n\nEach of which might be considered much more useful or compelling policy calculations.\n\n\n\n6.2.5 To interpolate existing variation in the data\nSticking with the tax reform example, let’s consider what it would take to jointly understand the effects. We know that each reform is related because each comes in the form of an infinite dimensional object: a function \\(\\mathcal{Y}(W,H)\\) of wages and hours. A model-free theory that attempts to estimate labor supply as a non-parametric function of this function would not get very far due to the implausible quantities of policy variation that would be needed to estimate it.\nEconomic models often provide a useful way to interpolate related – but not functionally identically related – variation. Notice that in the labor supply model any function \\(\\mathcal{Y}\\) is articulated through its effect on the budget constraint, and different policy reforms can be compared in the model without the addition of any new parameters. I refer to this property as articulated variation: when the effect of a variable can be modeled without the need for additional parameters. Typically, this involves a priori known changes to prices and endowments.\nIn this example, with structural parameters in hand, any function $ of wages and hours and can be modeled, and hence any observed variation in this function can be interpreted through the model without additional parameters. In Mullins (2026) I conduct a very similar example using data on welfare reform experiments in the United States.\nHere is a useful counterexample: policy variation that is not well-articulated inside a model. Consider our choice of modeling the minimum wage in our entry and exit model: embodied as a parameter \\(\\phi_{4}\\). Our choice to not model within-period production decisions of the firm (and instead estimate a reduced form) means that this policy is not well-articulated. We need an additional parameter \\(\\phi_{4}\\) to model its effect, and further changes to the nominal wage cannot be studied.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "lectures/why_models.html#marschaks-maxim",
    "href": "lectures/why_models.html#marschaks-maxim",
    "title": "6  How and Why to Use Models",
    "section": "6.3 Marschak’s Maxim",
    "text": "6.3 Marschak’s Maxim\nOur discussion so far has focused on exploring the boundaries of what simpler causal inference strategies can identify in order to make statements about how economic modeling can add value in research. This is not very instructive for how we do economic modeling. Of course this topic is more of an art than a science, but Marschak’s Maxim (Marschak 1953; Heckman and Vytlacil 2007) is a useful principle that can guide us. Here it is:\n\nResearchers should specify the minimal set of model ingredients in order to answer the research question of interest.\n\nThis may seem obvious, and that the hard part is figuring out what this minimal set is. It’s true that this can be hard to decide, but it’s surprisingly easy to forget this simple rule once you are deep inside your model and making decisions. The question “Is this essential to my question of interest?” is not always easy to answer but it is one you should be repeatedly asking yourself. Your research question is the mast you tie yourself to, and you should decide as early as possible what it is.\nLet’s do some clean examples to make the point clear.\n\n6.3.1 Marschak’s Original Example\nMarschak (1953) considers a very simple problem: a monopolist that chooses quantities to maximize profit, and a government that taxes quantities. Suppose that demand is given by \\[ p = \\alpha_0 - \\alpha_1 q \\] and that firms produce with constant marginal costs, \\(c\\). Firm profits as a function of quantities is: \\[ \\Pi(q) = (\\alpha_0 - c - \\tau)q - \\alpha_1 q^2 \\] yielding optimal quantities \\[ q^* = \\frac{\\alpha_0 - c - \\tau}{2\\alpha_1} \\] The government’s tax revenue is: \\[ R = q \\tau = \\frac{\\alpha_0 - c - \\tau}{2\\alpha_1} \\tau \\]\nSupposing that, intially, quantity \\(q\\) varies for exogenous reasons across markets, Marschak considers three problems and how an individual could learn the solution to their problems from data.\n\nMaximizing profit To maximize profit, the firm could look at how their profits evolve with \\(q\\): \\(\\Pi = a q - b q^2\\), and set \\(q^* = \\frac{a}{2b}\\)\nTo forecast how their revenue changes with firms quantities across markets, the Government needs only to know their tax rate \\(\\tau\\).\nTo set taxes to optimize tax revenue, and assuming that firms learn their optimal output also, the government need only extract the linear component \\(a = \\alpha_0 - c - \\ tau\\) from the observed profit relationship and add the observed tax rate \\(\\tau\\), setting \\(\\tau^* = (a+\\tau)/2 = (\\alpha_0 - c)/2\\).\n\nAlthough the setup is a little unconventional (we are used to assuming agents already act optimally), two lessons resonate from this exercise:\n\nFor each question, the analyst only needs to know certain combinations of parameters – each of which can be observed in reduced-form relationships – to answer their question of interest.\nOnce the firm optimizes, the reduced form relationship between tax revenue and quantities is not sufficient for forecasting the tax revenue from future changes to \\(\\tau\\).\n\nPoint (2) is essentially an early version of the Lucas Jr (1976) critique, while point (1) is the one we will mostly try to take lessons from. It emphasizes that in some stylized cases, the answers to some questions are essentiall invariant to particular model ingredients and we need not consider them. Although in practice we cannot always guarantee this, the insight undergirds a philosophical approach to quantitative modeling that bends always toward finding a minimal set of ingredients.\n\n\n6.3.2 Two-Stage Budgeting\nReturning to the life-cycle savings model, suppose we instead that utility depended on a vector of \\(K\\) commodities \\(x\\) in each period, such that the problem becomes: \\[ V_{t}(p,a,y) = \\max_{x,a'}\\left\\{v(x) + \\beta \\mathbb{E}_{p',y'|p,y}V_{t+1}(p',a',y')\\right\\}\\] subject to: \\[ p \\cdot x + \\frac{1}{1+r}a' \\leq y_t,\\qquad a\\geq \\underline{a} \\] where \\(p\\) is the vector of prices of each commodity. Imagine however that our research question is concerned only with studying the accumulation of savings over the life-cycle, and studying policy counterfactuals (such as the social security system above) that affect the intertemporal allocation of resources. A relatively weak assumption we could place on \\(v\\) is that it is homothetic, in which case there exists a price index \\(P(p)\\) and an indirect utility function \\(u\\) such that (Gorman 1959): \\[ V(p,X) = \\max_{x}v(x)\\ s.t.\\ p\\cdot x \\leq X = u(X/P(p)) \\] This approach, known as two-stage budgeting, says that it is reasonable for consumers to first allocate an aggregate expenditure budget across periods and – with access to a price index – disregard the problem of allocating expenditures to more detailed categories. It is a classic and clean application of Marschak’s maxim: if your research question concerns inter-temporal consumption allocation decisions, and your policy counterfactuals do not affect any intratemporal margins, then under regularity conditions on demand there is no need to consider intra-temporal consumption allocations. All you need is a reasonable price index with which to deflate nominal values.\nIn the same spirit, a main goal of Gorman (1959) and others in the two-stage budgeting literature was to find assumptions that would permit broader categories of expenditure (food, utilities, clothing) when modeling and estimating systems of demand.\n\n\n6.3.3 Sufficient Statistics\nConsider a static labor supply model for individuals \\(i\\in[0,1]\\) in an economy \\[ H_i = \\arg\\max_{h} C - v(h) \\] where \\[ C = b + (1-\\tau)hW_i \\] Assume that the transfer \\(b\\) is financed through \\(\\tau\\), so that in aggregate: \\[ b = \\int \\tau H_iW_i di. \\]\nConsider a utilitarian planner with welfare objective: \\[ V(\\tau) = \\int(C_i - v(H_i))di\\]\nWhat is the welfare effect of a marginal expansion in \\(b\\), financed through \\(\\tau\\)? Taking \\(dV/d\\tau\\) we can apply the envelope theorem to get:\n\\[ \\frac{dV}{d\\tau} = -\\int W_iH_idi + \\int W_iH_idi + \\tau \\int d(W_iH_i)/d\\tau d_i \\] which we can rearrange, assuming a constant elasticity of taxable income wrt to \\(\\tau\\) \\[ \\frac{dV}{d\\tau} = \\int Z_i di \\varepsilon_{Z,\\tau} \\] where \\(Z_i = W_iH_i\\) is taxable income.\nIn this setup, the willingess to pay for the marginal policy expansion exactly cancels out with the mechanical component of the cost, and we are left only with the distortionary cost of behavioral adjustments, similar to Harberger (1954). 1\nThere is a larger literature on similar so-called sufficient statistics in public finance, concerned with deriving approximations to the effect of local policy changes. Their appear is based on formula that tend to rely only on empirical quantities and elasticities, and less on specific functional form assumptions in the model.\nAthough of course, as models get more elaborate, the number of elasticities you need for these sufficient statistics becomes intractable (Kleven 2021). But they can often be a useful guide to understanding the fundamentals of what your policy is doing. And they are a great example of Marschak’s maxim!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "lectures/why_models.html#exercises",
    "href": "lectures/why_models.html#exercises",
    "title": "6  How and Why to Use Models",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nExercise 6.1 Consider a static version of the labor supply model where earnings are the only source of income:\n\\[ H_{n} = \\arg\\max_{h} \\frac{(W_nh)^{1-\\sigma}}{1-\\sigma} - \\alpha_{n}^{-1}\\frac{h^{1+1/\\psi}}{1+1/\\psi} \\]\nAssume that wages follow\n\\[ \\log(W_{n}) = \\gamma_{0} + \\gamma_{1}Z_{n} + \\epsilon_{n} \\]\nwhere \\(Z \\perp \\alpha\\) but \\(\\epsilon\\) and \\(\\alpha\\) are potentially correlated.\n\nSolve for \\(H_{n}\\) in terms of \\(W_{n}\\)\nSuppose you have cross-sectional data \\((W_{n},Z_{n},H_{n})_{n=1}^{N}\\) and that you use \\(Z\\) as an instrument to estimate the relationship \\[ \\log(H) = \\alpha_0 + \\alpha_1 \\log(W) + \\varepsilon \\] via 2SLS. Express the estimand \\(\\alpha_1\\) as a combination of structural parameters. (Hint: it should follow from your answer to part 1).\nConsider a policy reform that introduces a proportional subsidy, \\(\\tau\\), so that net wages are \\((1+\\tau)W\\). Is \\(\\alpha_{1}\\) sufficient for forecasting the effects of this policy? If this is your question of interest, what does Marschak’s Maxim suggest about the need to separately identify the parameters that combine to form \\(\\alpha_1\\)?\nNow suppose an alternative policy reform that consists of a lump sum transfer \\(b\\) and a proportional labor market tax, \\(\\tau\\), so that net income is \\(b + (1-\\tau)WH\\). Do you think \\(\\alpha_{1}\\) is sufficient to forecast the effect of this new policy on labor supply? An intuitive explanation is sufficient, but you can try calculating the local change \\(\\partial H / \\partial b |_{b=0}\\) if you want to be more precise.\nBased on your answer to part 4, and assuming that the model we wrote down is the true data generating process, do you think it is even possible to forecast the effect of this alternative policy given these data?\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nExercise 6.2 Consider the following simple model of time allocation. Individual utility is given by:\n\\[ U(C,L) = (\\phi C^{\\rho} + (1-\\phi) L^{\\rho})^{1/\\rho} \\]\nwhere \\(L\\) is an aggregate leisure good composed of \\(K\\) different activities:\n\\[ L = \\prod_{k=1}^{K}l_{k}^{\\delta_{k}},\\qquad \\sum_{k}\\delta_{k} = 1 \\]\nIn addition to these leisure activities, the agent may supply labor to the market at a wage rate of \\(w\\). Letting \\(h_{k}\\) be hours, the time constraint is:\n\\[ h_{k} + \\sum_{k}l_{k} = 1 \\]\nThe model is static and the individual solves the following problem:\n\\[ \\max_{C,\\{l_{k}\\}_{k=1}^{K}} U(C,L) \\]\nsubject to the constraint:\n\\[ C + w \\left(\\sum_{k} l_{k}\\right) \\leq w \\]\n\nSuppose you are interested in using this model to study the effects of a wage subsidy on labor supply. Notice that the model can be written as \\[ \\max_{C,h} U(wh,L^*(1-h)) \\] where \\[ L^*(1-h) = \\max_{\\{l_{k}\\}_{k=1}^{K}} \\prod_{k=1}^{K}l_{k}^{\\delta_{k}} \\] subject to \\(\\sum_{k}l_{k} = 1-h\\). Given this simplification, what does Marschak’s Maxim (and common sense) suggest about what parameters need to be estimated here?\nBased on your answer to the above, you simplify the model to the following specification: \\[ h^* = \\arg\\max (\\phi (wh)^{\\rho} + (1-\\phi) (1-h)^{\\rho})^{1/\\rho} \\] and you derive the following relationship: \\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi}{1-\\phi}\\right) + \\frac{1}{1-\\rho}\\log(w) \\] where \\(C=wh^*\\) is total labor income and \\(L=1-h^*\\) is non-market time. Suppose you have a cross-section of data \\((C_{n},L_{n},W_{n})\\) where \\(C_{n}\\) is labor market earnings, \\(L_{n}\\) is non-market time, and \\(W_{n}\\) is the wage-rate for person \\(n\\). This could be taken (for example) from the Outgoing Rotation Group of the CPS monthly survey. Does the model, as written, allow for any randomness in the relationship between \\(C_{n}/L_{n}\\) and \\(W_{n}\\)? Is this likely to be replicated in the data?\nSuppose now you augment the model to acommodate some randomness in how much individuals work by allowing for heterogeneity in preferences (\\(\\phi\\)): \\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi_{n}}{1-\\phi_{n}}\\right) + \\frac{1}{1-\\rho}\\log(w) \\] What assumption do you need for an OLS regression of \\(\\log(C_{n}/L_{n})\\) on \\(\\log(W_{n})\\) to consistently recover the elasticity of labor supply, \\(1/(1-\\rho)\\)? Do you consider this credible? Why/why not?\n\n\n\n\n\n\n\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nGorman, W. M. 1959. “Separable Utility and Aggregation.” Econometrica 27 (3): 469–81. http://www.jstor.org/stable/1909472.\n\n\nHarberger, Arnold C. 1954. “Monopoly and Resource Allocation.” The American Economic Review 44 (2): 77–87. http://www.jstor.org/stable/1818325.\n\n\nHeckman, James, and Edward Vytlacil. 2005. “Structural equations, treatment effects, and econometric policy evaluation.” Econometrica 73 (3): 669–738.\n\n\n———. 2007. “Chapter 70 Econometric Evaluation of Social Programs, Part i: Causal Models, Structural Models and Econometric Policy Evaluation.” In, edited by James J. Heckman and Edward E. Leamer, 6:4779–874. Handbook of Econometrics. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(07)06070-9.\n\n\nKleven, Henrik J. 2021. “Sufficient Statistics Revisited.” Journal Article. Annual Review of Economics 13 (Volume 13, 2021): 515–38. https://doi.org/https://doi.org/10.1146/annurev-economics-060220-023547.\n\n\nLucas Jr, Robert E. 1976. “Econometric Policy Evaluation: A Critique.” In Carnegie-Rochester Conference Series on Public Policy, 1:19–46. North-Holland.\n\n\nMarschak, Jacob. 1953. “Economic Measurements for Policy and Prediction.” In Studies in Econometric Method, edited by W. Hood and C. Koopmans. John Wiley & Sons.\n\n\nMullins, Joseph. 2026. “A Structural Meta-Analysis of Welfare Reform Experiments and Their Impacts on Children.” Journal of Political Economy 134 (1): 435–77. https://doi.org/10.1086/738482.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "lectures/why_models.html#footnotes",
    "href": "lectures/why_models.html#footnotes",
    "title": "6  How and Why to Use Models",
    "section": "",
    "text": "The assumption of quasi-linear utility simplifies things but is not required. In the general case, we can express the willingess to pay for the marginal policy change as: \\(-\\partial e(w(1-\\tau),u)/\\partial \\tau + dy/d\\tau\\) where \\(u\\) is utility in the baseline, \\(e(w(1-\\tau),u) = \\min (1-\\tau)wl + c\\) s.t. \\(u(c)+v(1-l)\\geq u\\) is the Hicksian expenditure function, and \\(y=(1-\\tau)w + b\\) is full income. Shephard’s lemma gives that the willingness to pay for the change in price is \\(lw\\) which we can compare to \\(dy/d\\tau\\) which is \\(-w - \\int (1-l_i)w_i di + \\tau\\int d((1-l_i)w_i)/d\\tau di\\). Averaging over everyone gives the total willingness to pay as \\(\\varepsilon_{z,\\tau}\\int z_i di\\) where \\(z_i = (1-l_i)w_i\\) is earnings and \\(\\varepsilon_{z,\\tau}\\) is the elasticity of taxable income with respect to taxes.↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "lectures/identification.html",
    "href": "lectures/identification.html",
    "title": "Identification and Credible Inference",
    "section": "",
    "text": "In this section of the course we will introduce the concept of identification and – with the help of our prototype models – examine some examples of different strategies for establishing identification of economic models.\nAs a fan of cricket, one thing you realize when explaining the rules to a newcomer is that the word “wicket” can refer to one of several different things. That’s confusing! Likewise, economists tend to use the word “identification” in a semantically loose way, and it can mean one of two things depending on context:\n\nFormally, identification refers to whether the mapping between model and data can be inverted to a unique parameter (point identification) or set of parameters (partial identification).\nInformally, identification discussions typically focus on how a particular model is identified, which has more to do with the particular estimation strategy than it does with the question of identification.\n\nOut of pragmatism, most quantitative models already impose parametric restrictions on functional forms (such as utilities, production functions, wage equations, and distributions of unobservables). We will see that this can often mean that there are many ways to identify and estimate parameters, and so much of our effort is directed at issue (2) instead of (1). When there are multiple paths to identification, it is worth considering whether one path might be considered more “credible” in the sense that it is more robust to mild extensions, or relies less heavily on strong functional form and / or distributional assumptions.\nIn my experience, one issue that can be confusing for students is that economists in some fields spend more time talking about identification than they do actually showing identification. In this chapter, we’ll show identification of our simple prototype models, and then consider extensions that show that this identification can sometimes rest on overly strong assumptions. Then we’ll consider some common strategies as potential remedies to making these overly strong assumptions.\nAs we work through the examples, I will (reluctantly) apply the labels “credible” and “incredible” inference, but only to help convey a broader understanding of what this labeling even means. I would like to establish, by way of these examples, that “credible inference” – a phrase you will hear more broadly in the profession (Angrist and Pischke 2010) – is not a particularly useful taxonomy for quantitative economics. Credibility is subjective and so-called “credible” inference strategies often contain quietly embedded structure, and require much stronger assumptions to draw any portable lessons from the exercise (we exhaustively discussed examples in the previous chapter). A more productive – and philosophically neutral – strategy is to be clear and transparent about the assumptions under which key parameters are identified, how they are identified, and therefore the sources of data that are most influential in determining the calculations from a given quantitative modeling exercise. That’s an honorable objective that satisfies the main imperative of the “credibility revolution”.\n\n\n\n\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2010. “The Credibility Revolution in Empirical Economics: How Better Research Design Is Taking the Con Out of Econometrics.” Journal of Economic Perspectives 24 (2).",
    "crumbs": [
      "Identification and Credible Inference"
    ]
  },
  {
    "objectID": "lectures/identification_roy.html",
    "href": "lectures/identification_roy.html",
    "title": "7  The Generalized Roy Model",
    "section": "",
    "text": "7.1 Selection in General\nLet us review the empirical content of this model without placing further restrictions on the functional forms. Let’s start with the selection equation. Let \\(P(X,Z) = P[D=1|X,Z]\\). For any distribution of \\(V\\), since \\(F_{V}\\) is monotonically increasing (under support conditions on \\(V\\)), we can write: \\[ D = \\mathbf{1}\\{\\mu_{d}(X,Z) \\geq V\\} = \\mathbf{1}\\{F_{V}(\\mu_{d}(X,Z)) \\geq F_{V}(V)\\}.\\] Since \\(F_{V}(V)\\) is a uniform random variable in \\([0,1]\\), we can always write the selection equation without loss of generality as:\n\\[ D = \\mathbf{1}\\{P(X,Z) - V \\geq 0\\},\\qquad V\\sim U[0,1] \\]\nNow consider the conditional expectations: \\[ \\mathbb{E}[Y|X,Z,D=1] = \\mu_{1}(X) + \\underbrace{\\mathbb{E}[U_1 | V \\leq P(X,Z)]}_{h_{1}(P(X,Z))} \\] \\[ \\mathbb{E}[Y|X,Z,D=0] = \\mu_{0}(X) + \\underbrace{\\mathbb{E}[U_0 | V &gt; P(X,Z)]}_{h_{0}(P(X,Z))} \\]\nTwo observations follow:",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "lectures/identification_roy.html#selection-in-general",
    "href": "lectures/identification_roy.html#selection-in-general",
    "title": "7  The Generalized Roy Model",
    "section": "",
    "text": "These equations illustrate the classic selection problem. If the unobservable that determines \\(D\\) is related to the unobservables that determine the potential outcomes \\((Y_0,Y_1)\\), then the difference in conditional means is partly contaminated with this selection effect.\nThe selection model implies dimension reduction in the conditional expectation of each \\(U_{D}\\) given \\(X\\) and \\(Z\\): the combined propensity \\(P(X,Z)\\) encodes all the relevant information to control for selection. This is a useful property, but the underlying index model is not without loss of generality. We will explore a little more below.",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "lectures/identification_roy.html#identification-by-functional-form",
    "href": "lectures/identification_roy.html#identification-by-functional-form",
    "title": "7  The Generalized Roy Model",
    "section": "7.2 Identification by Functional Form",
    "text": "7.2 Identification by Functional Form\nWe’ll begin by considering identification under some additional functional form restrictions. First, consider the example where the triple \\((V,U_0,U_1)\\) are jointly normally distributed:\n\\[\n\\left[\\begin{array}{c}\nV \\\\ U_0 \\\\ U_1\n\\end{array}\\right] = \\mathcal{N}\\left(\\mathbf{0},\\left[\n    \\begin{array}{ccc}\n    1 & \\sigma_{V0} & \\sigma_{V1} \\\\\n    \\sigma_{V0} & \\sigma^2_{0} & \\sigma_{01} \\\\\n    \\sigma_{V1} & \\sigma_{01} & \\sigma^2_{1}\n    \\end{array}\\right]\n    \\right)\n\\]\nwhere we have normalized the location of the unobservables by setting the mean to zero, and normalized the scale of \\(V\\) by assuming a unit variance. Additionally, let us specify that each \\(\\mu_{D}\\) is linear in \\(X\\): \\[ \\mu_{D}(X) = X\\beta_{D} \\] Assume that our data is a single cross-section of observations \\((Y_{D},D,X)\\). Identification is a statement about population values, so can we take as given that we see the joint distribution \\(\\mathbb{P}_{Y_{D},D,X}\\).\nStep 1 Note that the distribution of \\(D\\) given \\(X\\) is a probit model: \\[ P[D=1|X] = \\Phi(\\mu_{d}(X)) \\] where \\(\\Phi\\) is the cdf of the standard normal. Thus \\(\\mu_{d}(X)\\) is identified as \\(\\mu_{d}(X) = \\Phi^{-1}(P[D=1|X])\\) for any \\(X\\). If we additionally impose that \\(\\mu_{d}(X) = X\\gamma\\), then identification of each \\(\\gamma\\) follows from the usual assumption that \\(X\\) is full-rank with positive probability (just like OLS).\nStep 2 Now consider the identification of each \\(\\beta_{D}\\). We have:\n\\[\\begin{align}\n\\mathbb{E}[Y_{1}|X,D=1] &= X\\beta_{1} + \\mathbb{E}[U_1 | V&lt;\\mu_{d}(X)] \\\\\n&= X\\beta_{1} - \\sigma_{V1}\\frac{\\phi(\\mu_{d}(X))}{\\Phi(\\mu_{d}(X))}\n\\end{align}\\] and similarly: \\[\n\\mathbb{E}[Y_{0}|X,D=0] = X\\beta_1 + \\sigma_{V0}\\frac{\\phi(\\mu_{d}(X))}{1-\\Phi(\\mu_{d}(X))}\n\\] Under the same rank conditions for \\(X\\), both \\(\\beta_0\\) and \\(\\beta_1\\) are identified due to. This also means that \\(ATE(X) = X(\\beta_1 - \\beta_0)\\) is identified for all \\(X\\).\nAlthough we cannot identify the full distribution of treatment effects, this also gives us the average treatment effect among individuals with treatment propensity \\(V\\): \\[\\mathbb{E}[Y_0-Y_0|X,V] = X(\\beta_1-\\beta_0) + \\underbrace{(\\sigma_{V1}-\\sigma_{V0})V}_{\\mathbb{E}[U_1-U_0|V]}\\] We’ll come back to this object in the next section.\nNotice that identification holds here even without an excluded variable \\(Z\\). It follows from the assumption of linearity and normality of the error terms, which yield a particular parametric decomposition of the conditional expectation that can be identified.\nThis is an example of identification by functional form: identification depends crucially on these particular functional form assumptions. Notice that without linearity in \\(\\mu_{d}\\), it is not possible to separately identify \\(\\mu_{d}\\) from the selection correction. Would you say that this is a very credible approach to identifying the key causal parameters in the model? Lewbel (2019) provides a broad and satisfying discussion of these issues.",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "lectures/identification_roy.html#identification-with-exclusion-restrictions",
    "href": "lectures/identification_roy.html#identification-with-exclusion-restrictions",
    "title": "7  The Generalized Roy Model",
    "section": "7.3 Identification with Exclusion Restrictions",
    "text": "7.3 Identification with Exclusion Restrictions\nIf we have access to an excluded regressor, we do not have to rely so much on seemingly arbitrary functional form restrictions. Formally, we make two assumptions:\n\n\\(\\mu_{D}(X,Z)=\\mu_{D}(X)\\) almost everywhere (exclusion)\n$ Z(V,U_0,U_1) | X $ (independence)\n\nThis implies that we can write: \\[\\mathbb{E}[U_1|X,Z,D=1] = \\mathbb{E}[U_1|V\\leq P(X,Z)] \\] and similarly for \\(\\mathbb{E}[Y|X,Z,D=0]\\). So the expectation of \\(Y\\) (unconditional on \\(D\\)) is: \\[\\begin{align}\n\\mathbb{E}[Y|X,P(X,Z)=p] &= \\mu_{0}(X) + p[\\mu_{1}(X)-\\mu_{0}(X)] + p\\mathbb{E}[(U_{1}-U_{0})|V\\leq P(X,Z)] \\\\\n& \\mu_{0}(X) + P(X,Z)[\\mu_{1}(X)-\\mu_{0}(X)] + \\int_{0}^{p}\\mathbb{E}[(U_1 - U_0)|V=u]du\n\\end{align}\\]\nTaking a derivative with respect to \\(p\\) gives: \\[\\begin{align}\n\\frac{\\partial \\mathbb{E}[Y|X,P(X,Z)=p]}{\\partial p} &= \\mu_{1}(X)-\\mu_{0}(X) + \\mathbb{E}[U_1 - U_0 | V = p] \\\\\n& \\mathbb{E}[Y_1 - Y_0 | V = p] \\\\\n& MTE(p)\n\\end{align}\\]\nHeckman and Vytlacil (2005) call this derivative local instrumental variables approach and define the estimand, \\(MTE(p)\\) as the marginal treatment effect. It is the average treatment effect among individuals with a propensity \\(1-p\\) to take the treatment. They show that, commonly used estimators (instrumental variables, difference-in-differences) are often weighted averages of this marginal treatment effect, and that the marginal treatment effect is a useful building block for.\nNote that under support conditions on \\(Z\\), as the support of \\(P(X,Z)\\) approaches the unit interval \\([0,1]\\), the average treatment effect \\(ATE(X) = \\mu_{1}(X)-\\mu_{0}(X)\\) is also identified.\nCredibility Whether or not you think that this approach to identifying treatment effects is more or less credible than using functional form restrictions may depend on how valid you think the exclusion and independence restrictions are. However, assuming that we have access to a “good” instrument, it should be clear that this approach is preferable.\nIn practice, even when we have access to an instrument, the data requirements for fully non-parametric estimation can be overly demanding. A reasonable compromise is to illustrate the conditions under which your instrument provides nonparametric identification, then introduce parametric assumptions that can interpolate this plausible variation. This is a way to establish that identification of your parameters of interest in not purely driven by functional form restrictions. See Cunha, Heckman, and Schennach (2010) and Carneiro, Heckman, and Vytlacil (2011) for two examples of this kind of approach.\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 7.1 Consider the following problem that features endogeneity and selection. Let \\(W\\) be wages, let \\(X\\) be an endogenous variable of interest, and let \\(Z\\) be an instrument for \\(X\\). Wages are selected because we only observe then when the individual chooses to work (\\(H=1\\)):\n\\[\\begin{eqnarray}\n\\log(W) = \\alpha_0 + \\alpha_1 X + \\epsilon + \\rho\\eta \\\\\nX = \\gamma_0 + \\gamma_1 Z + \\eta \\\\\nH = \\mathbf{1}\\{\\beta_0 + \\beta_1\\log(W) + \\zeta &gt;0\\}\n\\end{eqnarray}\\]\nHere is a function that simulates these data for a fixed set of parameter values:\n\nusing Distributions, Random\nfunction sim_data(N)\n    α = [1., 0.5]\n    γ = [1., 0.5]\n    ρ = 0.4\n    β = [-0.25, 0.5]\n    ϵ = rand(Normal(),N)\n    η = rand(Normal(),N)\n    ζ = rand(Normal(),N)\n    Z = rand(Normal(),N)\n    X = γ[1] .+ γ[2]*Z .+ η\n    logW = α[1] .+ α[2]*X .+ ρ*η .+ ϵ\n    H = (β[1] .+ β[2]*logW .+ ζ) .&gt; 0\n    logW[H.==0] .= -1 #&lt;- set to missing if H=0\n    return (;logW,X,Z,H)\nend\n\nsim_data (generic function with 1 method)\n\n\nSuppose that you decide to handle the endogeneity problem by estimating \\(\\alpha_1\\) with 2SLS. Below is a monte-carlo simulation of the performance of this estimator:\n\nusing Plots, StatsBase\nfunction monte_carlo_trial(N)\n    data = sim_data(N)\n    # pull out non-missing data\n    W = data.logW[data.H.==1]\n    Z = data.Z[data.H.==1]\n    X = data.X[data.H.==1]\n    # run 2SLS\n    alpha_est = cov(W,Z) / cov(X,Z)\n    return alpha_est\nend\n# run 500 trials\nalpha_boot = [monte_carlo_trial(10_000) for b in 1:500]\nhistogram(alpha_boot,normalize=:probability,label=false)\nplot!([0.5,0.5],[0.,0.2],color=\"red\",label = \"True Value\")\nxlabel!(\"2SLS Estimate\")\n\n\n\n\n\nExplain why this estimator appears to be showing asymptotic bias.\nAssume that \\(\\zeta\\sim\\mathcal{N}(0,1)\\) and \\(\\epsilon\\sim\\mathcal(0,\\sigma^2_\\epsilon)\\). Show that the reduced form for this model (\\(H\\) and \\(W\\) written in terms of exogenous variables) can be written as: \\[ \\log(W) = A_0 + A_1z + A_2\\eta + \\xi_1 \\] \\[ H = \\mathbf{1}\\{B_0 + B_1z + B_2\\eta + \\xi_2 &gt;0\\} \\] \\[ X = \\gamma_0 + \\gamma_1 z + \\eta \\] where \\([\\xi_1,\\ \\xi_2]\\) is a bivariate normal with non-zero covariance and the coefficients \\(A\\) and \\(B\\) are combinations of underlying structural parameters.\nShow that the parameters of the reduced form are identified (hint: start by identifying \\([\\gamma_0,\\gamma_1]\\) and then inverting \\(\\eta\\) from \\(Z\\) and \\(X\\), treating it as observable).\nShow that the structural parameters can be identified from this reduced form.\nDoes this approach to identification work if we do not make these functional form assumptions? In other words, could we nonparametrically identify the reduced form parameters of the model?\nIntroduce a variable to this model that would allow you to nonparametrically estimate the reduced form once again (hint: think back to what allows this in the Generalized Roy Model).",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "lectures/identification_roy.html#monotonicity-and-potential-outcomes",
    "href": "lectures/identification_roy.html#monotonicity-and-potential-outcomes",
    "title": "7  The Generalized Roy Model",
    "section": "7.4 Monotonicity and Potential Outcomes",
    "text": "7.4 Monotonicity and Potential Outcomes\nRecall that the generalized roy model embeds the potential outcomes framework (this is the model you get if you throw away the selection equation). Imbens and Angrist (1994) consider what can be estimated from two stage least squares when you have access to an instrument \\(Z\\) and heterogeneous potential outcomes. To formalize their setup, assume that individuals can be indexed in some measurable space \\(\\omega\\in\\Omega\\), so that we write the model as a triple: \\((Y_1(\\omega),Y_0(\\omega),D_{Z}(\\omega))\\) where \\(D_{Z}(\\omega)\\in\\{0,1\\}\\) indicates the choice of an individual of type \\(\\omega\\) when the instrument takes a value \\(Z\\).\nThey combine four assumptions:\n\nExclusion: \\(Y_{D}(\\omega,Z) = Y_{D}(\\omega)\\) for \\(D\\in\\{0,1\\}\\)\nIndependence: $Z $\nMonotonicity: For any pair \\((z,z')\\) in the support of \\(Z\\), either \\(D_{Z}(\\omega)\\geq D_{Z'}(\\omega)\\) for all \\(\\omega\\in\\Omega\\) or \\(D_{Z}(\\omega)\\leq D_{Z'}(\\omega)\\) for all \\(\\omega\\in\\Omega\\).\nRelevance: \\(P[\\omega: D_{1}(\\omega) \\neq D_{0}(\\omega)]&gt;0\\).\n\nMonotonicity is easiest to understand when \\(Z\\in\\{0,1\\}\\). In this case one can partition \\(\\Omega\\) into\n\nAlways takers: \\(D_{1}(\\omega)=D_{0}(\\omega)=1\\)\nNever takers: \\(D_{1}(\\omega)=D_{0}(\\omega)=0\\)\nCompliers: \\(D_{1}(\\omega)=1\\), \\(D_{0}(\\omega)=0\\)\nDefiers: \\(D_{0}(\\omega) = 0\\), \\(D_{1}(\\omega)=1\\)\n\nIn this case, monotonicity rules out (without loss of generality) the existence of defiers.\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 7.2 Under these assumptions, show that the 2SLS estimand: \\[\n\\frac{\\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0]}{P[D=1|Z=1]-P[D=1|Z=0]}\n\\] is equal to the average treatment effect among compliers: \\(\\mathbb{E}[Y_1(\\omega)-Y_0(\\omega)|\\omega \\in\\text{Compliers}]\\).\n\n\n\nImbens and Angrist (1994) call this object the Local Average Treatment Effect. They show that for multi-valued instruments, 2SLS produces a weighted average of these LATES for difference complier groups.\nThis interpretation of IV estimators is immensely widely used, so you should know the definition and the assumptions under which it works.\n\n7.4.1 Relationship to the Generalized Roy Model\nThere is a deeper relationship between the monotonicity assumption and the Generalized Roy Model. Vytlacil (2002) shows that the monotonicity assumption is equivalent to a latent index model representation. One direction of this proof is easy, since it is simple to verify that the latent index model: \\[D = \\mathbf{1}\\{P(Z)-V \\geq 0\\}\\] obeys monotonicity. Going the other way, it is relatively intuitive to see that one can construct a mapping from \\(\\Omega\\) to \\([0,1]\\) such that: \\[ D_{Z}(\\omega) = \\mathbf{1}\\{P(Z)-V(\\omega) \\geq 0\\}. \\]\n\n\n\n\n\n\nExercise\n\n\n\n\nExercise 7.3 Use the Generalized Roy Model to show that for some binary instrument \\(Z\\in\\{0,1\\}\\), the 2SLS estimand is equal to: \\[\\int_{P(Z=0)}^{P(Z=1)}MTE(u)du \\]\n\n\n\n\n\n\n\nCarneiro, P., J. J. Heckman, and E. J. Vytlacil. 2011. “Estimating Marginal Returns to Education.” American Economic Review 101 (October): 2754–81. http://www.nber.org/papers/w16474.\n\n\nCunha, Flavio, James Heckman, and Susanne Schennach. 2010. “Estimating the Technology of Cognitive and Noncognitive Skill Formation.” Econometrica 78 (3): 883–931.\n\n\nHeckman, James, and Edward Vytlacil. 2005. “Structural equations, treatment effects, and econometric policy evaluation.” Econometrica 73 (3): 669–738.\n\n\nImbens, Guido W., and Joshua D. Angrist. 1994. “Identification and Estimation of Local Average Treatment Effects.” Econometrica 62 (2): 467–75. http://www.jstor.org/stable/2951620.\n\n\nLewbel, Arthur. 2019. “The Identification Zoo: Meanings of Identification in Econometrics.” Journal of Economic Literature 57 (4): 835–903. https://doi.org/10.1257/jel.20181361.\n\n\nVytlacil, Edward J. 2002. “Independence , Monotonicity and Latent Index Models : An Equivalence Result.” Econometrica 70 (1): 331–41.",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "lectures/identification_search.html",
    "href": "lectures/identification_search.html",
    "title": "8  The Search Model",
    "section": "",
    "text": "8.1 Identification of the Model without Heterogeneity\nThe McCall (1970) Search Model that we introduced as a prototype model is an excellent introduction to the identification of structural models. A classic treatment of identification of this model is provided by Flinn and Heckman (1982).",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Search Model</span>"
    ]
  },
  {
    "objectID": "lectures/identification_search.html#identification-of-the-model-without-heterogeneity",
    "href": "lectures/identification_search.html#identification-of-the-model-without-heterogeneity",
    "title": "8  The Search Model",
    "section": "",
    "text": "8.1.1 Data\nIdentification arguments always begin with an assumption on available data. Even though the model is dynamic, we will show that all we really need is a single cross-section of data:\n\\[ (E_{n},t_{U,n},W_{n})_{n=0}^{N} \\]\nwhere :\n\n\\(E_{n}\\in\\{0,1\\}\\) indicates the employment status of individual \\(n\\)\nIf the individual is employed (\\(E_{n}=1\\)) we see the wage, \\(W_{n}\\), which is otherwise assumed to be missing.\nIf the individual is unemployed (\\(E_{n}=0\\)), then we see the duration of unemployment \\(t_{U,n}\\), which is otherwise assumed to be missing.\n\n\n\n8.1.2 Steady State\nA key assumption for identification in this case is that the economy is in steady state. Let \\(U_{t}\\) be fraction of individuals that are unemployed at time \\(t\\). Let \\(p_{\\tau,t}\\) be the fraction of individuals at time \\(t\\) with unemployment duration \\(\\tau\\). In general, these objects evolve according to the rules:\n\\[\\begin{eqnarray}\nU_{t+1} = (1-h)U_{t} + \\delta (1-U_{t+1}) \\\\\np_{\\tau+1,t+1} = (1-h)p_{\\tau,t} \\\\\np_{0,t+1} = \\delta (1-U_{t})\n\\end{eqnarray}\\] where \\(h\\) is the “hazard rate” of exiting unemployment, \\(\\lambda(1-F_{W}(w^*))\\). Enforcing that these objects are constant between \\(t\\) and \\(t+1\\) (the steady state assumption) gives:\n\\[\\begin{eqnarray}\nU_{t} = \\frac{\\delta}{\\delta+h} \\\\\np_\\tau = \\frac{\\delta h}{\\delta+h}(1-h)^{\\tau}\n\\end{eqnarray}\\]\n\n\n8.1.3 Writing Joint Probabilities\nWith these probabilities in hand, we can write the sampling distribution \\(\\mathbb{P}\\) in terms of equilibrium objects:\n\\[ \\mathbb{P}(E,t_U,W) = \\left(\\frac{h}{h+\\delta}f_{W}(W|W&gt;w^*)\\right)^{E}\\left(\\frac{\\delta h}{\\delta+h}(1-h)^{t_{U}}\\right)^{1-E} \\]\n\n\n8.1.4 Thinking Through Identification\nOften it is helpful to break down the joint distribution of observables into unconditional and conditional distributions. Notice here that:\n\\[ \\mathbb{P}(t_{U}|E=0) = h\\times(1-h)^{t_{U}} \\]\nand so the hazard rate \\(h\\) can be inferred from the distribution of unemployment durations.\nNext, notice that the probability of being unemployed is\n\\[ \\mathbb{P}(E=0) = \\frac{\\delta}{\\delta+h} \\]\nand so \\(\\delta\\) is given by \\(h\\) (which we now know) and the fraction of unemployed.\nFinally, we see that\n\\[ \\mathbb{P}(W|E=1) = f_{W}(W|W&gt;w^*) \\]\nObserved wages are equal to the offer distribution conditional on the wage offer being acceptable. This tells us that, although the conditional distribution can be identifiable, we do not know the distribution of wage offers that are never accepted (those below \\(w^*\\)). This is still sufficient to identify \\(w^*\\) however, since it is the lower bound on the support of this conditional distribution.\nCan we identify the deeper structural parameters, \\(b\\), \\(\\lambda\\), and \\(\\beta\\)? Not quite. Let’s rewrite the reservation wage equation as:\n\\[ w^* = b + h\\int_{w^*}\\frac{1-F(W|W&gt;w^*)}{1-\\beta(1-\\delta)}dw \\]\nand we can see that infinitely many combinations of \\(b\\) and \\(\\beta\\) can rationalize the same reservation wage. Assuming a plausible value for \\(\\beta\\) (you will learn that this is common across many structural estimation exercises), \\(b\\) is identified by this equation.\n\n\n\n\nFlinn, Christopher, and James Heckman. 1982. “New Methods for Analyzing Structural Models of Labor Force Dynamics.” Journal of Econometrics 18 (1): 115–68.\n\n\nMcCall, John Joseph. 1970. “Economics of Information and Job Search.” The Quarterly Journal of Economics 84 (1): 113–26.",
    "crumbs": [
      "Identification and Credible Inference",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>The Search Model</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aguirregabiria, Victor, and Pedro Mira. 2007. “Sequential\nEstimation of Dynamic Discrete Games.” Econometrica 75\n(1): 1–53.\n\n\nAngrist, Joshua D., and Jörn-Steffen Pischke. 2010. “The\nCredibility Revolution in Empirical Economics: How Better Research\nDesign Is Taking the Con Out of Econometrics.” Journal of\nEconomic Perspectives 24 (2).\n\n\nArellano, Manuel, Richard Blundell, and Stephane Bonhomme. 2018.\n“Nonlinear Persistence and Partial Insurance: Income and\nConsumption Dynamics in the PSID.” AEA Papers and\nProceedings 108 (May): 281–86. https://doi.org/10.1257/pandp.20181049.\n\n\nBlundell, Richard, and Ian Walker. 1986. “A Life-Cycle Consistent\nEmpirical Model of Family Labour Supply Using Cross-Section\nData.” The Review of Economic Studies 53 (4): 539–58.\n\n\nCarneiro, P., J. J. Heckman, and E. J. Vytlacil. 2011. “Estimating\nMarginal Returns to Education.” American Economic Review\n101 (October): 2754–81. http://www.nber.org/papers/w16474.\n\n\nCunha, Flavio, James Heckman, and Susanne Schennach. 2010.\n“Estimating the Technology of Cognitive and Noncognitive Skill\nFormation.” Econometrica 78 (3): 883–931.\n\n\nDe Nardi, Mariacristina. 2004. “Wealth Inequality and\nIntergenerational Links.” The Review of Economic Studies\n71 (3): 743–68. https://doi.org/10.1111/j.1467-937X.2004.00302.x.\n\n\nDearing, Adam, and Jason R. Blevins. 2024. “Efficient and\nConvergent Sequential Pseudo-Likelihood Estimation of Dynamic Discrete\nGames.” Review of Economic Studies.\n\n\nEricson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry\nDynamics: A Framework for Empirical Work.” The Review of\nEconomic Studies 62 (1): 53–82. https://doi.org/10.2307/2297841.\n\n\nFlinn, Christopher, and James Heckman. 1982. “New Methods for\nAnalyzing Structural Models of Labor Force Dynamics.” Journal\nof Econometrics 18 (1): 115–68.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with\nVariation in Treatment Timing.” Journal of Econometrics\n225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nGorman, W. M. 1959. “Separable Utility and Aggregation.”\nEconometrica 27 (3): 469–81. http://www.jstor.org/stable/1909472.\n\n\nGourinchas, Pierre-Olivier, and Jonathan A. Parker. 2002.\n“Consumption over the Life Cycle.” Econometrica 70\n(1): 47–89. https://doi.org/10.1111/1468-0262.00269.\n\n\nHarberger, Arnold C. 1954. “Monopoly and Resource\nAllocation.” The American Economic Review 44 (2): 77–87.\nhttp://www.jstor.org/stable/1818325.\n\n\nHeckman, James J, and Bo E Honore. 1990. “The Empirical Content of\nthe Roy Model.” Econometrica: Journal of the Econometric\nSociety, 1121–49.\n\n\nHeckman, James, and Edward Vytlacil. 2005. “Structural equations, treatment effects, and econometric\npolicy evaluation.” Econometrica 73 (3): 669–738.\n\n\n———. 2007. “Chapter 70 Econometric Evaluation of Social Programs,\nPart i: Causal Models, Structural Models and Econometric Policy\nEvaluation.” In, edited by James J. Heckman and Edward E. Leamer,\n6:4779–874. Handbook of Econometrics. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(07)06070-9.\n\n\nImbens, Guido W., and Joshua D. Angrist. 1994. “Identification and\nEstimation of Local Average Treatment Effects.”\nEconometrica 62 (2): 467–75. http://www.jstor.org/stable/2951620.\n\n\nKleven, Henrik J. 2021. “Sufficient Statistics Revisited.”\nJournal Article. Annual Review of Economics 13 (Volume 13,\n2021): 515–38. https://doi.org/https://doi.org/10.1146/annurev-economics-060220-023547.\n\n\nLewbel, Arthur. 2019. “The Identification Zoo: Meanings of\nIdentification in Econometrics.” Journal of Economic\nLiterature 57 (4): 835–903. https://doi.org/10.1257/jel.20181361.\n\n\nLucas Jr, Robert E. 1976. “Econometric Policy Evaluation: A\nCritique.” In Carnegie-Rochester Conference Series on Public\nPolicy, 1:19–46. North-Holland.\n\n\nMaCurdy, Thomas E. 1981. “An Empirical Model of Labor Supply in a\nLife-Cycle Setting.” Journal of Political Economy 89\n(6): 1059–85.\n\n\nMarschak, Jacob. 1953. “Economic Measurements for Policy and\nPrediction.” In Studies in Econometric Method, edited by\nW. Hood and C. Koopmans. John Wiley & Sons.\n\n\nMcCall, John Joseph. 1970. “Economics of Information and Job\nSearch.” The Quarterly Journal of Economics 84 (1):\n113–26.\n\n\nMullins, Joseph. 2026. “A Structural Meta-Analysis of Welfare\nReform Experiments and Their Impacts on Children.” Journal of\nPolitical Economy 134 (1): 435–77. https://doi.org/10.1086/738482.\n\n\nRoy, A. D. 1951. “Some Thoughts on the Distribution of\nEarnings.” Oxford Economic Papers 3 (2): 135–46. http://www.jstor.org/stable/2662082.\n\n\nVytlacil, Edward J. 2002. “Independence ,\nMonotonicity and Latent Index Models : An Equivalence\nResult.” Econometrica 70 (1): 331–41.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendices/data.html",
    "href": "appendices/data.html",
    "title": "Appendix A — Data",
    "section": "",
    "text": "A.1 A Disclaimer for IPUMS CPS data\nFor the practical applications in this course we will use three datasets:\nThese data are included in the course git repo, and you should be able to run all of the example code in this class using the relative file paths in the git repo if you clone it. Alternatively you can download these datasets and save them wherever you wish, but you will need to edit file paths accordingly.\nThese data are a subsample of the IPUMS CPS data available from cps.ipums.org. Any use of these data should be cited as follows:\nSarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D030.V11.0\nThe CPS data file is intended only for exercises as part of ECON8208. Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests. For all other uses of these data, please access data directly via cps.ipums.org.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "appendices/data.html#a-disclaimer-for-ipums-cps-data",
    "href": "appendices/data.html#a-disclaimer-for-ipums-cps-data",
    "title": "Appendix A — Data",
    "section": "",
    "text": "Arellano, Manuel, Richard Blundell, and Stephane Bonhomme. 2018. “Nonlinear Persistence and Partial Insurance: Income and Consumption Dynamics in the PSID.” AEA Papers and Proceedings 108 (May): 281–86. https://doi.org/10.1257/pandp.20181049.\n\n\nDearing, Adam, and Jason R. Blevins. 2024. “Efficient and Convergent Sequential Pseudo-Likelihood Estimation of Dynamic Discrete Games.” Review of Economic Studies.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html",
    "href": "appendices/autodiff.html",
    "title": "Appendix B — Automatic Differentiation",
    "section": "",
    "text": "B.1 Why AD Matters for Structural Estimation\nAutomatic differentiation (AD) is a technique for computing exact derivatives of functions specified by computer programs. Unlike symbolic differentiation (which manipulates mathematical expressions) or numerical differentiation (which uses finite differences), AD exploits the fact that every program, no matter how complex, executes a sequence of elementary operations. By applying the chain rule systematically to these operations, AD computes derivatives to machine precision.\nIn structural econometrics, we frequently need gradients for:\nHand-coding derivatives is tedious and error-prone. Finite differences are slow (requiring \\(O(n)\\) function evaluations for an \\(n\\)-dimensional gradient) and can be numerically unstable. AD provides exact gradients efficiently.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#why-ad-matters-for-structural-estimation",
    "href": "appendices/autodiff.html#why-ad-matters-for-structural-estimation",
    "title": "Appendix B — Automatic Differentiation",
    "section": "",
    "text": "Optimization (MLE, GMM, minimum distance)\n\nGradient-free methods such as Nelder-Mead are popular, of course, but are less efficient\n\nComputing standard errors\nSolving models with equilibrium conditions (using Newton’s method, for example)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#forward-mode-vs-reverse-mode",
    "href": "appendices/autodiff.html#forward-mode-vs-reverse-mode",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.2 Forward Mode vs Reverse Mode",
    "text": "B.2 Forward Mode vs Reverse Mode\nAD comes in two flavors:\nForward mode propagates derivatives forward through the computation. For a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), computing the full Jacobian requires \\(n\\) forward passes. This is efficient when \\(n \\ll m\\).\nReverse mode propagates derivatives backward (like backpropagation in neural networks). Computing the full Jacobian requires \\(m\\) reverse passes. This is efficient when \\(m \\ll n\\).\nFor most estimation problems, we have a scalar objective (\\(m = 1\\)) and many parameters (\\(n\\) large), so reverse mode is typically preferred. In my experience however, I have had more success writing code that is compatible with forward differencing. You will learn from experience that these tools can be fussy.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#ad-in-julia",
    "href": "appendices/autodiff.html#ad-in-julia",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.3 AD in Julia",
    "text": "B.3 AD in Julia\nJulia’s AD ecosystem is excellent. The main packages are:\n\nB.3.1 ForwardDiff.jl\nForward-mode AD. Simple and robust, works out-of-the-box for most pure Julia code.\n\nusing ForwardDiff\n\nf(x) = sum(x.^2)\nx = [1.0, 2.0, 3.0]\n\n# Gradient\nForwardDiff.gradient(f, x)\n\n3-element Vector{Float64}:\n 2.0\n 4.0\n 6.0\n\n\n\n# Hessian\nForwardDiff.hessian(f, x)\n\n3×3 Matrix{Float64}:\n 2.0  0.0  0.0\n 0.0  2.0  0.0\n 0.0  0.0  2.0\n\n\n\n\nB.3.2 Enzyme.jl\nA high-performance AD engine that works at the LLVM level. Supports both forward and reverse mode. Often the fastest option, especially for code with loops and mutations.\n\nusing Enzyme\n\nf(x) = x[1]^2 + sin(x[2])\n\nx = [1.0, 2.0]\ndx = zeros(2)\n\n# Reverse mode gradient\nEnzyme.autodiff(Reverse, f, Active, Duplicated(x, dx))\ndx\n\n2-element Vector{Float64}:\n  2.0\n -0.4161468365471424\n\n\n\n\nB.3.3 Zygote.jl\nA source-to-source reverse-mode AD system. Popular in machine learning (used by Flux.jl). Works well for array-heavy code but may struggle with control flow.\n\nusing Zygote\n\nf(x) = sum(x.^2)\nx = [1.0, 2.0, 3.0]\n\nZygote.gradient(f, x)\n\n([2.0, 4.0, 6.0],)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#practical-recommendations",
    "href": "appendices/autodiff.html#practical-recommendations",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.4 Practical Recommendations",
    "text": "B.4 Practical Recommendations\n\nStart with ForwardDiff for problems with few parameters (&lt; 100). It’s the most reliable.\nUse Enzyme for performance-critical code, especially if you have loops or in-place mutations.\nBe aware of limitations: AD systems can fail on code that uses certain constructs (try-catch, foreign function calls, some global variables). When in doubt, test that your gradients match finite differences:\n\n\nusing ForwardDiff, FiniteDiff\n\nf(x) = log(1 + exp(x[1] * x[2])) + x[3]^2\nx = [1.0, 2.0, 3.0]\n\nad_grad = ForwardDiff.gradient(f, x)\nfd_grad = FiniteDiff.finite_difference_gradient(f, x)\n\nmaximum(abs.(ad_grad .- fd_grad))\n\n5.3512749786932545e-12",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#integration-with-optimization",
    "href": "appendices/autodiff.html#integration-with-optimization",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.5 Integration with Optimization",
    "text": "B.5 Integration with Optimization\nMost Julia optimization packages accept AD gradients. Here’s an example with Optim.jl:\n\nusing Optim, ForwardDiff\n\nrosenbrock(x) = (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2\nx0 = [0.0, 0.0]\n\n# With automatic gradients via ForwardDiff\nresult = optimize(rosenbrock, x0, LBFGS(); autodiff = :forward)\nresult.minimizer\n\n2-element Vector{Float64}:\n 0.999999999999928\n 0.9999999999998559",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#example-maximum-likelihood-with-optim.jl",
    "href": "appendices/autodiff.html#example-maximum-likelihood-with-optim.jl",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.6 Example: Maximum Likelihood with Optim.jl",
    "text": "B.6 Example: Maximum Likelihood with Optim.jl\nConsider a simple probit model:\n\\[ D = \\mathbf{1}\\{X\\beta - \\nu \\geq 0\\},\\qquad \\nu \\sim \\mathcal{N}(0,1) \\]\nHere is code to simulate data for this model:\n\nusing Random, Distributions\n\nfunction sim_data(X ; γ)\n    N = size(X,1)\n    ν = rand(Normal(),N)\n    D = (X * γ .- ν) .&gt; 0\n    return D\nend\n\n# a quick test of the function:\nN = 1000\nX = [ones(N) 2*rand(Normal(),N)]\nγ = [0.1, 0.5]\nD = sim_data(X ; γ);\n\nConsider the problem of estimating \\(\\gamma\\) using maximum likelihood. We will establish the properties of this estimator in class. Here let’s just focus on numerically how to attack the minimization problem. The log-likelihood of the data D given X is:\n\\[ \\mathcal{L}(\\gamma) = \\sum_{n}l(D_{n}; X_{n},\\gamma) = \\sum_{n=1}^{N}D_{n}\\log(\\Phi(X\\gamma)) + (1-D_{n})\\log(1-\\Phi(X\\gamma)) \\]\nLet’s write up this likelihood function.\n\nfunction log_likelihood(D,X,γ)\n    ll = 0.\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            ll += log(cdf(Normal(),xg))\n        else\n            ll += log(1-cdf(Normal(),xg))\n        end\n    end\n    return ll\nend\nlog_likelihood(D,X,[0.,0.])\n\n-693.1471805599322\n\n\n\nB.6.1 Numerical Optimization\nOptimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses \\(\\gamma_{k+1}\\) given \\(\\gamma_{k}\\) as:\n\\[ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} \\]\nwhere \\(Q\\) is the function being maximized (or minimized). \\(A_{k}\\) defines a direction in which to search (providing weights on the derivatives) and \\(\\lambda_{k}\\) is a scalar variable known as a step-size which is often calculated optimally in each iteration \\(k\\). For Newton’s method, the matrix \\(A_{k}\\) is the inverse of the Hessian of the objective function \\(Q\\). Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\nSince we have a simple model, we can calculate derivatives relatively easily. Below we’ll compare a hard-coded derivative to this automatic differentiation.\n\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\ndx = zeros(2)\n# forward mode\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x-&gt;log_likelihood(D,X,x),γ)\n# reverse mode\nauto_deriv2_ll(D,X,γ,dx) = Enzyme.autodiff(Reverse, x-&gt;log_likelihood(D,X,x), Active, Duplicated(γ, dx))\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\nauto_deriv2_ll(D,X,γ,dx)\n[d1 d2 dx]\n\n2×3 Matrix{Float64}:\n 18.585   18.585   18.585\n 36.7039  36.7039  36.7039\n\n\nOk so we’re confident that these functions work as intended, but how do they compare in performance?\n\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n@time auto_deriv2_ll(D,X,γ,dx);\n\n  0.000021 seconds (2 allocations: 80 bytes)\n  0.000035 seconds (7 allocations: 304 bytes)\n  0.000033 seconds\n\n\nAll are quite quick and you can see that we’re not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions.\nSo now let’s try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton’s Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shannon (BFGS) algorithm (which updates search direction using changes in the first derivative).\nWhile Newton’s method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let’s test them.\n\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #&lt;- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     8.794794e+02\n * time: 0.011163949966430664\n     1     5.038529e+02     1.424353e+02\n * time: 0.3925299644470215\n     2     4.896962e+02     5.718137e+00\n * time: 0.39277005195617676\n     3     4.896776e+02     6.554352e-04\n * time: 0.39298009872436523\n     4     4.896776e+02     2.812263e-10\n * time: 0.3931429386138916\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     8.794794e+02\n * time: 5.507469177246094e-5\n     1     5.022184e+02     1.277309e+02\n * time: 0.007002115249633789\n     2     4.993056e+02     1.210926e+02\n * time: 0.007193088531494141\n     3     4.898180e+02     1.538296e+01\n * time: 0.007416963577270508\n     4     4.896776e+02     7.292141e-02\n * time: 0.007581949234008789\n     5     4.896776e+02     2.642307e-03\n * time: 0.007750034332275391\n     6     4.896776e+02     6.079906e-08\n * time: 0.007950067520141602\n     7     4.896776e+02     3.867739e-14\n * time: 0.008134126663208008\n\n\n2×3 Matrix{Float64}:\n 0.143143  0.143143  0.1\n 0.540119  0.540119  0.5",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/autodiff.html#further-reading",
    "href": "appendices/autodiff.html#further-reading",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.7 Further Reading",
    "text": "B.7 Further Reading\n\nJuliaDiff documentation\nForwardDiff.jl docs\nEnzyme.jl docs",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "appendices/performance.html",
    "href": "appendices/performance.html",
    "title": "Appendix C — Performance Tips",
    "section": "",
    "text": "C.1 The Golden Rules\nJulia can be extremely fast, but achieving good performance requires understanding a few key principles. This appendix provides a brief summary; see the official Performance Tips for comprehensive guidance.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "appendices/performance.html#the-golden-rules",
    "href": "appendices/performance.html#the-golden-rules",
    "title": "Appendix C — Performance Tips",
    "section": "",
    "text": "C.1.1 1. Avoid Global Variables\nGlobal variables with non-constant types force the compiler to generate slow, generic code.\n# Bad\ndata = [1.0, 2.0, 3.0]\nf() = sum(data)  # `data` could change type\n\n# Good: use const\nconst DATA = [1.0, 2.0, 3.0]\nf() = sum(DATA)\n\n# Good: pass as argument\nf(data) = sum(data)\n\n\nC.1.2 2. Write Type-Stable Functions\nA function is type-stable if the output type can be inferred from the input types. Type instability forces runtime dispatch.\n# Bad: returns Int or Float64 depending on value\nfunction unstable(x)\n    if x &gt; 0\n        return 1\n    else\n        return 0.0\n    end\nend\n\n# Good: consistent return type\nfunction stable(x)\n    if x &gt; 0\n        return 1.0\n    else\n        return 0.0\n    end\nend\nUse @code_warntype to check for type instabilities (look for red Any or Union types).\n\n\nC.1.3 3. Pre-allocate Arrays\nAvoid creating arrays inside loops. Pre-allocate and use in-place operations.\n# Bad: allocates on every iteration\nfunction bad_example(n)\n    result = 0.0\n    for i in 1:n\n        v = zeros(100)  # allocation!\n        v .= rand(100)\n        result += sum(v)\n    end\n    result\nend\n\n# Good: pre-allocate\nfunction good_example(n)\n    result = 0.0\n    v = zeros(100)\n    for i in 1:n\n        rand!(v)  # in-place\n        result += sum(v)\n    end\n    result\nend\n\n\nC.1.4 4. Use @views for Array Slices\nArray slices create copies by default. Use @views or view() to avoid allocation.\nA = rand(1000, 1000)\n\n# Bad: creates a copy\nf(A[1:100, :])\n\n# Good: creates a view\nf(@views A[1:100, :])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "appendices/performance.html#quick-profiling",
    "href": "appendices/performance.html#quick-profiling",
    "title": "Appendix C — Performance Tips",
    "section": "C.2 Quick Profiling",
    "text": "C.2 Quick Profiling\nUse @time for basic timing (run twice—first call includes compilation):\n@time my_function(args)  # compile\n@time my_function(args)  # actual timing\nFor more detailed analysis: - BenchmarkTools.jl: Accurate microbenchmarks with @btime - Profile (stdlib): Sampling profiler - ProfileView.jl: Flame graph visualization",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "appendices/performance.html#further-resources",
    "href": "appendices/performance.html#further-resources",
    "title": "Appendix C — Performance Tips",
    "section": "C.3 Further Resources",
    "text": "C.3 Further Resources\n\nJulia Performance Tips — the official guide, essential reading\nJulia Academy performance course — free video tutorials\nBenchmarkTools.jl documentation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  }
]