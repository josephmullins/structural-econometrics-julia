[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Structural Econometrics with Julia",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Five Prototype Models",
    "section": "",
    "text": "This chapter introduces four prototype structural models that we will use throughout the course to illustrate econometric methods. These models serve as running examples for identification strategies, estimation techniques, and computational methods.\nFor the purposes of some examples, we may at times perturb particular aspects of these assumptions. Our first task however is simply to familiarize ourselves with the general structure of the models, along with some numerical methods for solving them.",
    "crumbs": [
      "Five Prototype Models"
    ]
  },
  {
    "objectID": "models/generalized_roy.html",
    "href": "models/generalized_roy.html",
    "title": "1  The Generalized Roy Model",
    "section": "",
    "text": "1.1 Overview\nThe generalized Roy model is a framework for understanding selection into treatment based on heterogeneous gains. Theoretically, it is about the simplest model of choice one could write down, but it has surprisingly deep empirical content.\nRoy (1951) used a version of this model to study occupational choice and introduce the concept of selection. It lies at the heart of most econometric treatments of selection and causal inference (J. Heckman and Vytlacil 2005; J. J. Heckman and Honore 1990).\nOriginally developed to study occupational choice, it has become the canonical model for analyzing treatment effects when individuals select into treatment based on anticipated outcomes.\nThis model introduces fundamental concepts:\nThese ideas are central to modern applied microeconometrics and connect directly to some later identification examples we consider.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/generalized_roy.html#overview",
    "href": "models/generalized_roy.html#overview",
    "title": "1  The Generalized Roy Model",
    "section": "",
    "text": "Selection on unobservables\nTreatment effect heterogeneity\nMarginal treatment effects (MTE)\nLocal average treatment effects (LATE)",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/generalized_roy.html#the-model",
    "href": "models/generalized_roy.html#the-model",
    "title": "1  The Generalized Roy Model",
    "section": "1.2 The Model",
    "text": "1.2 The Model\nThe model is very simple. Let \\(D\\in\\{0,1\\}\\) be a treatment or choice made by each individual in an economy. Individuals make the choice / take the treatment if the utility they derive from \\(D=1\\) exceeds that if \\(D=1\\). Let \\(Z\\) be a vector of observables that influences payoffs. The selection equation is:\n\\[ D = \\mathbf{1}\\{\\mu_{d}(Z) - V \\geq 0\\} \\]\nwhere \\(\\mu_{d}(Z)\\) is a deterministic function of \\(Z\\) and \\(V\\) is a random variable that is unobserved to the econometrician. Some other notes:\n\nThe term \\(\\mu_{d}(Z)-V\\) can be interpreted as the difference in utilities and the function \\(\\mu_{d}\\) can be viewed with the usual welfarist interpretations.\nIn this sense, the selection equation is essentially a binary choice model.\nThis model already builds in some special structure: the unobservables that dermine choices (\\(V\\)) are additively separable with respect to the observable factors \\(Z\\). We’ll return to this in future sections on identification.\n\nThe selection equation is paired with a pair of potential outcome equations:\n\\[\\begin{align}\nY_1 &= \\mu_1(X) + U_1 \\\\\nY_0 &= \\mu_0(X) + U_0\n\\end{align}\\]\nwhere:\n\n\\(X\\subset Z\\) are observed characteristics\n\\(U_1, U_0\\) are unobserved components that determine outcomes\n\nKey assumption: \\((U_1, U_0, V)\\) are jointly distributed, potentially correlated. We’ll later return to the implications of this assumption.\nA canonical example of this model is the returns to schooling, where \\(D\\in\\{0,1\\}\\) is the decision to attend college.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/generalized_roy.html#potential-outcomes-and-observability",
    "href": "models/generalized_roy.html#potential-outcomes-and-observability",
    "title": "1  The Generalized Roy Model",
    "section": "1.3 Potential Outcomes and Observability",
    "text": "1.3 Potential Outcomes and Observability\n\n1.3.1 What We Observe vs. What We Want\nObservable:\n\nTreatment status: \\(D\\)\nActual outcome: \\(Y_{D}\\)\nCovariates: \\(X, Z\\)\n\nNot observable:\n\nCounterfactual outcomes: we don’t see \\(Y_{1-D}\\)\nIndividual treatment effects: \\(\\Delta = Y_{1} - Y_{0}\\)\n\nFundamental Problem of Causal Inference: We never observe both \\(Y_1\\) and \\(Y_0\\) for the same individual.\n\n\n1.3.2 Treatment Effects of Interest\n\nIndividual Treatment Effect (ITE): \\[\\Delta_i = Y_{1i} - Y_{0i}\\] Never observed for any individual.\nAverage Treatment Effect (ATE): \\[\\text{ATE} = E[\\Delta] = E[Y_1 - Y_0]\\] Average gain if we randomly assigned everyone to treatment.\nAverage Treatment on the Treated (ATT): \\[\\text{ATT} = E[\\Delta | D=1] = E[Y_1 - Y_0 | D=1]\\] Average gain for those who actually chose treatment.\nAverage Treatment on the Untreated (ATU): \\[\\text{ATU} = E[\\Delta | D=0] = E[Y_1 - Y_0 | D=0]\\] Average gain for those who chose not to be treated.\n\n\n\n1.3.3 Simulation\nHere is code to simulate data from a generalized Roy model under the assumption that (\\(U_0,U_1\\)) are jointly normally distributed and are the sole source of selection on gains.\nusing Distributions, DataFrames, Statistics\n\n# Simulate Roy model with heterogeneous returns\nfunction simulate_roy_model(n=10000)\n    # Parameters\n    α₁, α₀ = 3.0, 2.5  # Mean log wages\n    σᵤ = 0.3           # Std dev of unobservables\n    ρ = 0.5            # Correlation between U₁ and U₀\n\n    # Generate correlated unobservables\n    # (U₁, U₀) ~ Bivariate Normal\n    Σ = [1.0 ρ; ρ 1.0] * σᵤ^2\n    U = rand(MvNormal([0.0, 0.0], Σ), n)'\n    U₁ = U[:, 1]\n    U₀ = U[:, 2]\n\n    # Individual treatment effects\n    Δ = (α₁ - α₀) .+ (U₁ .- U₀)\n\n    # Generate instrument Z (e.g., family income, distance to college)\n    Z = rand(Normal(0, 1), n)\n\n    # Selection: D = 1 if gain &gt; cost\n    # Cost depends on Z and unobserved V\n    V = rand(Normal(0, 0.5), n)\n    cost_threshold = 0.3 .- 0.4 * Z  # Lower cost if Z is high\n    D = (Δ .+ V) .&gt; cost_threshold\n\n    # Observed outcomes\n    Y₁ = α₁ .+ U₁\n    Y₀ = α₀ .+ U₀\n    Y = D .* Y₁ .+ (1 .- D) .* Y₀\n\n    return DataFrame(\n        Y₁ = Y₁,\n        Y₀ = Y₀,\n        Y = Y,\n        D = D,\n        Δ = Δ,\n        Z = Z\n    )\nend\n\n# Simulate data\ndf = simulate_roy_model(10000)\n\n# Calculate different treatment effects\nATE = mean(df.Δ)\nATT = mean(df[df.D .== 1, :Δ])\nATU = mean(df[df.D .== 0, :Δ])\n\n# Naive comparison\nnaive = mean(df[df.D .== 1, :Y]) - mean(df[df.D .== 0, :Y])\n\nprintln(\"True ATE: \", round(ATE, digits=3))\nprintln(\"True ATT: \", round(ATT, digits=3))\nprintln(\"True ATU: \", round(ATU, digits=3))\nprintln(\"Naive estimator: \", round(naive, digits=3))\nprintln(\"Selection bias: \", round(naive - ATE, digits=3))\nOutput:\nTrue ATE: 0.502\nTrue ATT: 0.647\nTrue ATU: 0.291\nNaive estimator: 0.712\nSelection bias: 0.210\nInterpretation: - ATT &gt; ATE &gt; ATU: Those who select college have higher returns - Naive estimator overestimates ATE due to positive selection bias - Selection on gains: people with high \\(\\Delta\\) choose treatment",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/generalized_roy.html#further-reading",
    "href": "models/generalized_roy.html#further-reading",
    "title": "1  The Generalized Roy Model",
    "section": "1.4 Further Reading",
    "text": "1.4 Further Reading\nFoundational papers:\n\nRoy (1951): “Some Thoughts on the Distribution of Earnings” - Original occupational choice model\nHeckman and Honoré (1990): “The Empirical Content of the Roy Model” - Identification analysis\nImbens and Angrist (1994): “Identification and Estimation of Local Average Treatment Effects” - LATE framework\n\nModern treatments:\n\nHeckman and Vytlacil (2005): “Structural Equations, Treatment Effects, and Econometric Policy Evaluation” - Unifying MTE framework\nHeckman et al. (2006): “Understanding Instrumental Variables in Models with Essential Heterogeneity” - Extensions and applications\n\nEmpirical applications:\n\nWillis and Rosen (1979): “Education and Self-Selection” - Returns to schooling\nCarneiro et al. (2011): “Estimating Marginal Returns to Education” - MTE estimation\n\n\n\n\n\nHeckman, James J, and Bo E Honore. 1990. “The Empirical Content of the Roy Model.” Econometrica: Journal of the Econometric Society, 1121–49.\n\n\nHeckman, James, and Edward Vytlacil. 2005. “Structural equations, treatment effects, and econometric policy evaluation.” Econometrica 73 (3): 669–738.\n\n\nRoy, A. D. 1951. “Some Thoughts on the Distribution of Earnings.” Oxford Economic Papers 3 (2): 135–46. http://www.jstor.org/stable/2662082.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Generalized Roy Model</span>"
    ]
  },
  {
    "objectID": "models/search.html",
    "href": "models/search.html",
    "title": "2  Job Search Model",
    "section": "",
    "text": "2.1 Overview\nThis section presents a simple model of undirected job search. The model demonstrates how workers optimally choose which job offers to accept based on a reservation wage strategy.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#economic-environment",
    "href": "models/search.html#economic-environment",
    "title": "2  Job Search Model",
    "section": "2.2 Economic Environment",
    "text": "2.2 Economic Environment\nTime is discrete and indexed by \\(t\\) over an infinite horizon. Workers move between employment and unemployment, have linear utility, and cannot save.\n\n2.2.1 Parameters\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\n\\(\\lambda\\)\nThe probability an unemployed worker receives a job offer\n\n\n\\(\\delta\\)\nThe probability an employed worker loses their job\n\n\n\\(F_{W}\\)\nThe distribution of wage offers\n\n\n\\(1-\\beta\\)\nThe exponential rate of discounting\n\n\n\\(b\\)\nPer-period utility when unemployed",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#recursive-formulation",
    "href": "models/search.html#recursive-formulation",
    "title": "2  Job Search Model",
    "section": "2.3 Recursive Formulation",
    "text": "2.3 Recursive Formulation\nThe classic approach to solve this model is to write the values of unemployment and employment recursively. For example:\n\\[ U = b + \\beta[(1-\\lambda)U + \\lambda\\int\\max\\{V(w),U\\}dF_{W}(w)] \\] \\[ V(w) = w + \\beta[(1-\\delta)V(w) + \\delta U] \\]",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#model-solution",
    "href": "models/search.html#model-solution",
    "title": "2  Job Search Model",
    "section": "2.4 Model solution",
    "text": "2.4 Model solution\nOne can show that the optimal decision rule of the worker is characterized by a reservation wage \\(w^*\\), defined as \\(V(w^*)=U\\). We can also differentiate the expression for \\(V(w)\\) to get:\n\\[ V'(w) = \\frac{1}{1 - \\beta(1-\\delta)} \\]\nand applying integration by parts gives:\n\\[ U = b + \\beta[U + \\lambda\\int_{w^*}\\frac{1-F_{W}(w)}{1-\\beta(1-\\delta)}dw] \\]\nNow applying the definition of the reservation wage gives the reservation wage equation:\n\\[ w^* = b + \\beta\\lambda\\int_{w^*}\\frac{1-F_{W}(w)}{1 - \\beta(1-\\delta)}dw \\]\nand we can characterize the steady state rate of unemployment as:\n\\[ P[E = 0] = \\frac{h}{h+\\delta} \\]\nwhere \\(h = \\lambda(1-F_{W}(w^*))\\) is the rate at which workers exit unemployment.\nSimilarly, we can show that the steady state fraction of unemployment durations \\(t_{U}\\) is\n\\[ P[t_{U}=t] = h(1-h)^{t} \\]",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#numerical-model-solution",
    "href": "models/search.html#numerical-model-solution",
    "title": "2  Job Search Model",
    "section": "2.5 Numerical Model Solution",
    "text": "2.5 Numerical Model Solution\nTo solve the reservation wage equation numerically, we need to evaluate the integral on the right-hand side and find the value of \\(w^*\\) that satisfies the equation. This requires two key numerical methods: quadrature (for integration) and root-finding.\n\n2.5.1 Gauss-Legendre Quadrature\nWhen integrating numerically, we approximate the integral using a weighted sum at specific evaluation points (nodes):\n\\[ \\int_a^b f(x)dx \\approx \\frac{b-a}{2}\\sum_{k=1}^n w_k f\\left(\\frac{a+b}{2} + \\frac{b-a}{2}x_k\\right) \\]\nwhere \\(x_k\\) are the nodes and \\(w_k\\) are the weights from Gauss-Legendre quadrature. This method is particularly accurate for smooth functions and uses a fixed number of nodes, which is important for automatic differentiation (unlike adaptive methods like in the package QuadGK that adjust the number of nodes based on the integrand).\nLet’s implement a simple Gauss-Legendre integration routine:\n\nusing FastGaussQuadrature, Distributions, Roots\n\n# Fixed-node quadrature for integration (compatible with automatic differentiation)\nfunction integrateGL(f, a, b; num_nodes = 10)\n    nodes, weights = gausslegendre(num_nodes)\n    ∫f = 0.\n    for k in eachindex(nodes)\n        x = (a + b)/2 + (b - a)/2 * nodes[k]\n        ∫f += weights[k] * f(x)\n    end\n    return (b - a)/2 * ∫f\nend\n\n# Evaluate the derivative of the surplus function\ndS(x; F, β, δ) = (1 - cdf(F, x)) / (1 - β*(1 - δ))\n\n# Reservation wage equation (should equal zero at the solution)\nfunction res_wage(wres, b, λ, δ, β, F::Distribution)\n    ub = quantile(F, 0.9999)  # Upper bound of integration\n    integral = integrateGL(x -&gt; dS(x; F, β, δ), wres, ub)\n    return wres - b - β * λ * integral\nend\n\npars = (;b = -5., λ = 0.45, δ = 0.03, β = 0.99, F = LogNormal(1, 1))\nres_wage(1., pars.b, pars.λ, pars.δ, pars.β, pars.F)\n\n-33.6935906934783\n\n\n\n\n2.5.2 Root Finding\nThe reservation wage \\(w^*\\) is the value that makes the reservation wage equation equal to zero. We use the Roots.jl package, which implements efficient root-finding algorithms based on combinations of bisection, secant, and inverse quadratic interpolation methods.\nThe find_zero function takes:\n\nA function to find the root of\nAn initial guess\nThe type of the initial guess (to ensure type stability)\n\n\nfunction solve_res_wage(b, λ, δ, β, F)\n    return find_zero(\n        x -&gt; res_wage(x, b, λ, δ, β, F),\n        eltype(b)(4.)  # Initial guess of $4/hour\n    )\nend\n\nrwage = solve_res_wage(pars.b, pars.λ, pars.δ, pars.β, pars.F)\nprintln(\"Reservation wage: \", round(rwage, digits=2))\n\nReservation wage: 7.23\n\n\nThis approach has the advantage of being compatible with automatic differentiation tools like ForwardDiff, which is a very useful tool in numerical methods.\n\n\n2.5.3 Steady-State Statistics\nUsing the computed reservation wage, we can calculate the steady-state unemployment rate and average duration:\n\n# Compute steady-state statistics\nh = pars.λ * (1 - cdf(pars.F, rwage))  # Exit rate from unemployment\nu_rate = pars.δ / (pars.δ + h)          # Unemployment rate\navg_duration = 1 / h                     # Average duration\n\nprintln(\"Exit rate (h): \", round(h, digits=3))\nprintln(\"Unemployment rate: \", round(u_rate * 100, digits=1), \"%\")\nprintln(\"Average duration: \", round(avg_duration, digits=1), \" periods\")\n\nExit rate (h): 0.074\nUnemployment rate: 28.9%\nAverage duration: 13.6 periods",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/search.html#further-reading",
    "href": "models/search.html#further-reading",
    "title": "2  Job Search Model",
    "section": "2.6 Further Reading",
    "text": "2.6 Further Reading\n\nMcCall (1970): “Economics of Information and Job Search” - Original search model\nWolpin (1987): “Estimating a Structural Search Model” - Early structural estimation\nEckstein and van den Berg (2007): “Empirical Labor Search” - Survey of search models\nFlinn and Heckman (1982): “New Methods for Analyzing Structural Models of Labor Force Dynamics” - Duration data analysis",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Job Search Model</span>"
    ]
  },
  {
    "objectID": "models/savings.html",
    "href": "models/savings.html",
    "title": "3  A Life-Cycle Savings Model",
    "section": "",
    "text": "3.1 Overview\nThis section presents a stylized life-cycle model of consumption and savings. Households make dynamic decisions about consumption and asset accumulation over their lifetime, facing income uncertainty and borrowing constraints.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "models/savings.html#economic-environment",
    "href": "models/savings.html#economic-environment",
    "title": "3  A Life-Cycle Savings Model",
    "section": "3.2 Economic Environment",
    "text": "3.2 Economic Environment\nTime is discrete and indexed by \\(t\\). Individuals live for a finite number of periods, \\(T\\). They derive utility from consumption according to a CRRA utility function:\n\\[ u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma} \\]\nand from “bequests”, which are modeled here as cash on hand net of consumption in the final period:\n\\[ \\nu(a) = \\psi \\frac{a^{1-\\sigma}}{1-\\sigma} \\].\nConsumption can be transferred between periods via a portfolio of one-period bonds (“savings’, \\(a\\)) that can be purchased at the price \\(1 / (1+r)\\), with a prdetermined limit, \\(\\underline{a}\\), on borrowing.\nInviduals receive income \\(y\\) every period that is governed by a deterministic (\\(\\mu_{t}\\)) and stochastic component:\n\\[ \\log(y_{t}) = \\mu_{t} + \\varepsilon_{it} \\]\nwhere \\(\\varepsilon_{it}\\) is a first-order Markov process. A particular case of interest is the case where \\(\\varepsilon\\) is a stationary AR 1 process:\n\\[ \\varepsilon_{it} = \\rho \\varepsilon_{it-1} + \\eta_{it} \\]\nwhere \\(\\eta_{it} \\sim \\mathcal{N}(0,\\sigma^2_{\\eta})\\). The unconditional variance of \\(\\varepsilon_{it}\\) is therefore \\(\\sigma^2_{\\eta} / (1-\\rho^2)\\).",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "models/savings.html#model-solution",
    "href": "models/savings.html#model-solution",
    "title": "3  A Life-Cycle Savings Model",
    "section": "3.3 Model Solution",
    "text": "3.3 Model Solution\nDefine\n\\[ V_{T}(a,\\varepsilon) = \\max_{c}\\left\\{u(c) + \\nu(y + a - c)\\right\\} \\]\nAnd now define the remaining value functions recursively:\n\\[ V_{t}(a,\\varepsilon) = \\max_{c,a'}\\left\\{u(c) + \\beta\\mathbb{E}_{\\varepsilon'|\\varepsilon}V(a',\\varepsilon')\\right\\} \\]\nsubject to:\n\\[ c + \\frac{1}{1+r}a' \\leq y + a \\]\nand\n\\[ a' \\geq \\underline{a}\\]\nwhere \\(\\underline{a}\\) is the borrowing constraint.\nWe’re going to write code to solve the model naively using this recursive formulation. You may already be aware that there are more efficient solution methods that exploit the first order conditions of the problem. Not the focus of our class! Please don’t use the example below as a demonstration of best practice when it comes to solving savings models.\nWe’ll start picking some default parameters.\n\npars = (;\n    T = 45, β = 0.95, σ = 2,ρ = 0.9,ση = 0.1, μ = fill(2.,45), ψ = 5., r = 0.05\n)\n\n(T = 45, β = 0.95, σ = 2, ρ = 0.9, ση = 0.1, μ = [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0  …  2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], ψ = 5.0, r = 0.05)\n\n\nNext we’ll write a function that uses Tauchen’s method to approximate the income process as a discrete markov process.\n\nusing Distributions,Random\nusing LinearAlgebra\nΦ(x) = cdf(Normal(),x)\n\nfunction tauchen(ρ,ση,Kϵ)\n    sd = ση/sqrt(1-ρ^2)\n    grid = range(-3sd,stop=3sd,length=Kϵ)\n    Π = zeros(Kϵ,Kϵ)\n    Δ = grid[2]-grid[1]\n    for j=1:Kϵ\n        Π[1,j] = Φ((grid[1] + Δ/2 - ρ*grid[j])/ση)\n        Π[end,j] = 1 - Φ((grid[end] - Δ/2 - ρ*grid[j])/ση)\n        for k=2:(Kϵ-1)\n            Π[k,j] = Φ((grid[k] + Δ/2 - ρ*grid[j])/ση) - Φ((grid[k] - Δ/2 - ρ*grid[j])/ση)\n        end\n    end\n    return Π,grid\nend\n\ntauchen (generic function with 1 method)\n\n\nNow, let’s think about how to solve this model. We have two state variables to track. We have discretized \\(\\varepsilon\\), now let’s discretize assets and define a max operator.\n\nKa = 100\nKϵ = 5\nagrid = LinRange(0,pars.μ[1] * pars.T,Ka) #&lt;- is this a reasonable upper bound? We'll find out!\nΠ,ϵgrid = tauchen(pars.ρ,pars.ση,Kϵ)\npars = (;pars...,Ka,agrid,Π,ϵgrid,Kϵ)\n\nu(c,σ) = c^(1-σ) / (1-σ)\n\nfunction solve_max(V,t,iϵ,ia,pars)\n    (;agrid,ϵgrid,Π,σ,Ka,r,β) = pars\n    cash = exp(pars.μ[t] + ϵgrid[iϵ]) + agrid[ia]\n    amax = 0\n    vmax = -Inf\n    loop = true\n    a = 1\n    while loop && a&lt;Ka\n        c = cash - agrid[a] / (1+r)\n        if c&gt;0\n            #@views v = u(c,σ) + β * dot(Π[:,iϵ],V[:,a,t+1])\n            v = u(c,σ)\n            for iϵ′ in axes(V,1)\n                v += β * Π[iϵ′,iϵ] * V[iϵ′,a,t+1]\n            end\n            if v&gt;vmax\n                vmax = v\n                amax = a\n            end\n        else\n            loop = false\n        end\n        a += 1 #&lt;- move one up the grid space\n    end\n    return amax,vmax\nend\n\nsolve_max (generic function with 1 method)\n\n\nNext, a function that uses this max operator to get the value function for all states in a period, \\(t\\), and records the optimal savings policy.\n\nfunction iterate!(V,A,t,pars)\n    for ia in axes(V,2), iϵ in axes(V,1)\n        A[iϵ,ia,t],V[iϵ,ia,t] = solve_max(V,t,iϵ,ia,pars)\n    end\nend\nfunction terminal_values!(V,pars)\n    (;σ,ψ,agrid) = pars\n    for ia in axes(V,2), iϵ in axes(V,1)\n        V[iϵ,ia] = ψ * u(agrid[ia],σ)\n    end\nend\n\nterminal_values! (generic function with 1 method)\n\n\n\nfunction backward_induction!(V,A,pars)\n    (;ψ,σ,T,agrid) = pars\n    # set the values at T+1 (bequest motives)\n    @views terminal_values!(V[:,:,T+1],pars)\n    for t in reverse(1:T)\n        iterate!(V,A,t,pars)\n    end\nend\n\nbackward_induction! (generic function with 1 method)\n\n\nLet’s check the model solution and time it also.\n\nV = zeros(pars.Kϵ,pars.Ka,pars.T+1)\nA = zeros(Int64,pars.Kϵ,pars.Ka,pars.T)\nbackward_induction!(V,A,pars)\n@time backward_induction!(V,A,pars)\n\n  0.008771 seconds\n\n\nSeems ok. We can plot the policy functions as a sanity check. The plot below shows savings policy at the median wage shock over time at different levels of assets.\n\nusing Plots\n\nplot(1:pars.T,agrid[A[3,1:10:Ka,:]'],legend=false)\n\n\n\n\nYou can see that the discreteness creates some jumpiness in the policy functions. As I said, other solution methods that use interpolation can be more efficient and will create smoother pictures, but since that is not the focus of this class we will use this simple solution method.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "models/savings.html#further-reading",
    "href": "models/savings.html#further-reading",
    "title": "3  A Life-Cycle Savings Model",
    "section": "3.4 Further Reading",
    "text": "3.4 Further Reading\nGourinchas and Parker (2002) and De Nardi (2004) are two classic examples of quantitative applications of the life-cycle savings model.\n\n\n\n\nDe Nardi, Mariacristina. 2004. “Wealth Inequality and Intergenerational Links.” The Review of Economic Studies 71 (3): 743–68. https://doi.org/10.1111/j.1467-937X.2004.00302.x.\n\n\nGourinchas, Pierre-Olivier, and Jonathan A. Parker. 2002. “Consumption over the Life Cycle.” Econometrica 70 (1): 47–89. https://doi.org/10.1111/1468-0262.00269.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>A Life-Cycle Savings Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html",
    "href": "models/entry-exit.html",
    "title": "4  Firm Entry-Exit Model",
    "section": "",
    "text": "4.1 Overview\nThis section presents a symmetric duopoly model of firm entry and exit decisions. Firms make discrete choices about market participation based on profitability and fixed costs. This model illustrates static discrete choice with strategic interactions and is used in Chapter 5 to demonstrate discrete choice estimation methods.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html#model-ingredients",
    "href": "models/entry-exit.html#model-ingredients",
    "title": "4  Firm Entry-Exit Model",
    "section": "4.2 Model Ingredients",
    "text": "4.2 Model Ingredients\nHere are the basic ingredients of the model:\n\nThere are two firms indexed by \\(f\\in\\{0,1\\}\\)\nThere are \\(M\\) markets indexed by \\(m\\)\nTime is discrete and indexed by \\(t\\)\nEach firm makes an entry decision every period. We let \\(d\\in\\{0,1\\}\\) index this decision to enter or not. Let \\(d(f,m,t)\\) indicate the choice of firm \\(f\\) in market \\(m\\) in period \\(t\\).\nWe let \\(a_{f,m,t}=d(f,m,t-1)\\) indicate whether firm \\(f\\) is active in market \\(m\\) in period \\(t\\), which means they entered in the previous period.\nLet \\(x_{m}\\) be a market-level observable that shifts the profitability of operations in market \\(m\\).\nIn addition to the observed states, each firm draws a pair of idiosyncatic shocks to payoffs in each period, \\(\\epsilon_{f}=[\\epsilon_{f0},\\epsilon_{f1}]\\) that is private information to the firm and is iid over markets, firms, and time periods.\nFirms make their decisions in each period simultaneously\n\nTo simplify notation, suppress dependance of outcomes on the market \\(m\\) and time period \\(t\\). Because we are writing a symmetric model, we will also suppress dependence on \\(f\\). The deterministic component of the payoff to entering is a function of the market primitives (\\(x\\)), the firm’s activity status (\\(a\\)), and the other firm’s entry decision \\(d^\\prime\\):\n\\[ u_{1}(x,a,d^{\\prime}) = \\phi_{0} + \\phi_{1}x - \\phi_{2}d^\\prime - \\phi_{3}(1-a) \\]\nThe payoff to not entering is simply:\n\\[{u}_{0}(x,a) = \\phi_{4}a \\]\nBefore characterizing the solution to the firm’s problem, let’s code up these payoff functions:\n\nu1(x,a,d′,ϕ) = ϕ[1] + ϕ[2]*x - ϕ[3]d′ + ϕ[4]*(1-a)\nu0(a,ϕ) = a * ϕ[5]\n\nu0 (generic function with 1 method)",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html#solving-the-firms-problem",
    "href": "models/entry-exit.html#solving-the-firms-problem",
    "title": "4  Firm Entry-Exit Model",
    "section": "4.3 Solving the firm’s problem",
    "text": "4.3 Solving the firm’s problem\nLet \\(d^*(x,a,a',\\epsilon)\\) be the firm’s optimal decision given the state and the idiosyncratic shock. We will focus on symmetric equilibria so this policy function is sufficient to describe the behavior of both firms.\nThe value to either firm of arriving in a period with state \\((x,a,a')\\) can be written recursively as:\n\\[\n\\begin{aligned}\nV(x,a,a') = \\mathbb{E}_{\\epsilon}\\max\\mathbb{E}_{\\epsilon'}\\big\\{\n    &u_{1}(x,a,d^*(x,a',a,\\epsilon'))+\\epsilon_{1} \\\\\n    &\\quad + \\beta V(x,1,d^*(x,a,a',\\epsilon')), \\\\\n    &u_{0}(x,a) + \\epsilon_{0} \\\\\n    &\\quad + \\beta V(x,0,d^*(x,a',a,\\epsilon'))\\big\\}\n\\end{aligned}\n\\]\nDefine the optimal choice probability in equilibrium as:\n\\[ p(x,a,a') = \\int_{\\epsilon}d^*(x,a,a',\\epsilon)dF(\\epsilon) \\]\nWith this in hand we can integrate out the other firm’s shocks \\(\\epsilon'\\) to get:\n\\[\n\\begin{aligned}\nV(x,a,a') = \\mathbb{E}_{\\epsilon}\\max\\big\\{\n    &\\phi_{0}+\\phi_{1}x - \\phi_{2}p(x,a',a) +\\epsilon_{1} \\\\\n    &\\quad + \\beta \\big[p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)\\big], \\\\\n    &a \\phi_{4} + \\epsilon_{0} \\\\\n    &\\quad + \\beta \\big[p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)\\big]\\big\\}\n\\end{aligned}\n\\]\nDefine the choice-specific values as:\n\\[\n\\begin{aligned}\nv_{1}(x,a,a') = \\phi_{0}+\\phi_{1}x &- \\phi_{2}p(x,a',a) \\\\\n    &+ \\beta \\big[p(x,a',a)V(x,1,1) + (1-p(x,a',a))V(x,1,0)\\big]\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\nv_{0}(x,a,a') = a \\phi_{4} + \\beta \\big[p(x,a',a)V(x,0,1) + (1-p(x,a',a))V(x,0,0)\\big]\n\\end{aligned}\n\\]\nSo assuming that \\(\\epsilon\\) is distributed as type I extreme value random variable with location parameter 0 and scale parameter 1 we get analytical expressions for the choice probabilities and the expected value of the maximum:\n\\[ V(x) = \\gamma + \\log\\left(\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))\\right)\\]\nwhere \\(\\gamma\\) is the Euler-Mascheroni constant and\n\\[ p(x,a,a') = \\frac{\\exp(v_{1}(x,a,a'))}{\\exp(v_{0}(x,a,a'))+\\exp(v_{1}(x,a,a'))} \\]\nBefore we define equilibrium and think about solving the model, let’s quickly write up the mapping between the other firm’s choice probabilities and the choice values:\n\n# Fixing x, assume that V is stored as a 2 x 2 array\n# The argument p is the current guess of p(x,a',a)\nfunction choice_values(x,a,p,V,ϕ,β)\n    v0 = u0(a,ϕ) + β * p * V[1,2] + β * (1-p) * V[1,1]\n    v1 = u1(x,a,p,ϕ) + β * p * V[2,2] + β * (1-p) * V[2,1]\n    return v0,v1\nend\n\nchoice_values (generic function with 1 method)\n\n\nIn principle we could iterate on this mapping to find (for a fixed \\(p\\)), the firm’s optimal solution. But that won’t be an efficient way to try and solve for the equilibrium.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html#equilibrium",
    "href": "models/entry-exit.html#equilibrium",
    "title": "4  Firm Entry-Exit Model",
    "section": "4.4 Equilibrium",
    "text": "4.4 Equilibrium\nThe solution concept for this model is Markov Perfect Equilibrium. Fixing the market \\(x\\), here the equilibrium be characterized as a fixed point in the value function \\(V\\) and choice probabilities, \\(p\\). In words, equilibrium is summarized by a \\(V\\) and a \\(p\\) such that:\n\nGiven \\(p\\), \\(V\\) is a fixed point in the recursive formulation of values; and\n\\(p\\) are the optimal choice probabilities of each firm given \\(V\\) and given the other firm’s choice probabilities are \\(p\\).\n\nHow should we solve for this symmetric equilibrium? We could try iterating on \\(V\\) and \\(p\\) as follows:\n\n# V is a 2x2 array with values\n# p is a 2x2 array with choice probabilities\nfunction iterate_model(V,p,x,ϕ,β)\n    Vnew = copy(V)\n    pnew = copy(p)\n    for a′ in axes(V,2)\n        for a in axes(V,1)\n            p′ = p[a′,a]\n            v0,v1 = choice_values(x,a-1,p′,V,ϕ,β)\n            pnew[a,a′] = exp(v1) / (exp(v0)+exp(v1))\n            Vnew[a,a′] = log(exp(v0)+exp(v1))\n        end\n    end\n    return Vnew,pnew\nend\n\nfunction solve_by_iteration(x,ϕ,β; max_iter = 1000, verbose = false)\n    V0 = zeros(2,2)\n    p0 = fill(0.1,2,2)\n    err = Inf\n    iter = 1\n    while err&gt;1e-10 && iter&lt;max_iter\n        V1,p1 = iterate_model(V0,p0,x,ϕ,β)\n        err = maximum(abs.(V1 .- V0))\n        if mod(iter,100)==0 && verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        V0 = V1\n        p0 = p1\n        iter += 1\n    end\n    return V0,p0\nend\n\nβ = 0.95\nϕ = 2 * [1.,0.1,0.5,2.,0.5]\nsolve_by_iteration(0.,ϕ,β; verbose = true)\n\nIteration 100, error is 0.04823924738592211\nIteration 200, error is 0.001242310554474102\nIteration 300, error is 5.032709619001707e-5\nIteration 400, error is 2.123540213005981e-6\nIteration 500, error is 8.863101186307176e-8\nIteration 600, error is 3.693557459882868e-9\nIteration 700, error is 1.538467131467769e-10\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nThis seems to work! But notice that it takes a while for the iteration to converge. Also, unlike the single agent case, there is no guarantee that this iteration is always a contraction.\nWe can also solve this model relatively easily using Newton’s Method and the magic of Automatic Differentiation. To do this, we’ll solve over the pair of choice-specific values \\(v_{0}\\) and \\(v_{1}\\) (these encode both values and choice probabilities) and store these values as a vector instead of an array:\n\nusing ForwardDiff, LinearAlgebra\n\n# this function returns V as a 2 x 2 array given the vector of choice specific values in V\nfunction calc_V(v)\n    idx = LinearIndices((2,2,2))\n    [log(exp(v[idx[1,1+a,1+a′]]) + exp(v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n# this function returns choice probabilities as a 2x2 array given the vector v\nfunction calc_p(v)\n    idx = LinearIndices((2,2,2))\n    [1 / (1+exp(v[idx[1,1+a,1+a′]] - v[idx[2,1+a,1+a′]])) for a in 0:1, a′ in 0:1]\nend\n\n\nfunction iterate_model_v(v,x,ϕ,β)\n    idx = LinearIndices((2,2,2)) #&lt;- this is for convenient indexing over v\n    vnew = copy(v)\n    V = calc_V(v)\n    for a′ in axes(idx,3)\n        for a in axes(idx,2)\n            i0′ = idx[1,a′,a] #&lt;- this locates the position in v for v_{0}(x,a',a)\n            i1′ = idx[2,a′,a] #&lt;- this locates the position in v for v_{1}(x,a',a)\n            p = 1 / (1 + exp(v[i0′] - v[i1′]))\n            v0,v1 = choice_values(x,a-1,p,V,ϕ,β)\n            vnew[idx[1,a,a′]] = v0\n            vnew[idx[2,a,a′]] = v1\n        end\n    end\n    return vnew\nend\n\nF(v,x,ϕ,β) = v .- iterate_model_v(v,x,ϕ,β)\nfunction solve_model_newton(x,ϕ,β;max_iter = 10, verbose = false)\n    v = zeros(8)\n    dF(v) = ForwardDiff.jacobian(y-&gt;F(y,x,ϕ,β),v)\n    err = Inf\n    iter = 1\n    while (err&gt;1e-10) && (iter&lt;max_iter)\n        Fv = F(v,x,ϕ,β)\n        dFv = dF(v)\n        vnew = v - inv(dFv) * Fv\n        err = maximum(abs.(Fv))\n        if verbose\n            println(\"Iteration $iter, error is $err\")\n        end\n        iter += 1\n        v = vnew\n    end\n    return v\nend\n\nsolve_model_newton(0.,ϕ,β;verbose = true);\n\nIteration 1, error is 6.158489821531948\nIteration 2, error is 1.7766463237555712\nIteration 3, error is 0.056247498263360285\nIteration 4, error is 0.00028434628951856666\nIteration 5, error is 3.4473004006940755e-8\nIteration 6, error is 1.4210854715202004e-14\n\n\nLet’s try timing each solution method to quickly compare:\n\nsolve_model_newton(0.,ϕ,β)\nsolve_by_iteration(0.,ϕ,β)\n\n@time solve_model_newton(0.,ϕ,β)\n@time solve_by_iteration(0.,ϕ,β)\n\n  0.000091 seconds (230 allocations: 54.688 KiB)\n  0.000204 seconds (4.29 k allocations: 234.531 KiB)\n\n\n([69.73147518902888 70.96824731388737; 68.46263413289174 67.89546273371974], [0.9107652821657111 0.990549524651413; 0.052475860075290155 0.27729654446688445])\n\n\nIn this case Newton’s method is faster. Let’s double check that both methods return the same answer:\n\nv0 = solve_model_newton(0.,ϕ,β)\nV0,p = solve_by_iteration(0.,ϕ,β)\np1 = calc_p(v0)\n[p p1]\n\n2×4 Matrix{Float64}:\n 0.910765   0.99055   0.910765   0.99055\n 0.0524759  0.277297  0.0524759  0.277297\n\n\nLooks good! We can re-use this code when we get to thinking about estimation later on. To do this we will have to solve the model for different values of \\(x_{m}\\), but that can be done by using this code and iterating (potentially in parallel) over different values of \\(x\\).\nIf you play around with parameters, you will see how convergence times may change and that solution methods are not always stable, especially when choice probabilities in equilibrium are very close to one or zero.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/entry-exit.html#further-redaing",
    "href": "models/entry-exit.html#further-redaing",
    "title": "4  Firm Entry-Exit Model",
    "section": "4.5 Further Redaing",
    "text": "4.5 Further Redaing\nEricson and Pakes (1995) and Aguirregabiria and Mira (2007) are both classic entries in this literature.\n\n\n\n\nAguirregabiria, Victor, and Pedro Mira. 2007. “Sequential Estimation of Dynamic Discrete Games.” Econometrica 75 (1): 1–53.\n\n\nEricson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry Dynamics: A Framework for Empirical Work.” The Review of Economic Studies 62 (1): 53–82. https://doi.org/10.2307/2297841.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Firm Entry-Exit Model</span>"
    ]
  },
  {
    "objectID": "models/dynamic-labor-supply.html",
    "href": "models/dynamic-labor-supply.html",
    "title": "5  A Simple Labor Supply Model",
    "section": "",
    "text": "5.1 Model Setup and Solution\nConsider a dynamic labor supply model (with no uncertainty) where each agent \\(n\\) chooses a sequence of consumption and hours, \\(\\{c_{t},h_{t}\\}_{t=1}^{\\infty}\\), to solve: \\[ \\max \\sum_{t=0}^\\infty \\beta^{t} \\left(\\frac{c_{t}^{1-\\sigma}}{1-\\sigma} - \\frac{\\alpha_{n}^{-1}}{1 + 1/\\psi}h_{t}^{1+1/\\psi}\\right)\\] subject to the intertemporal budget constraint: \\[ \\sum_{t}q_{t}c_{t} \\leq A_{n,0} + \\sum_{t}q_{t}W_{n,t}h_{t},\\qquad q_{t} = (1+r)^{-t}.\\] Let \\(H_{n,t}\\) and \\(C_{n,t}\\) be the realizations of labor supply for agent \\(n\\) at time \\(t\\). Labor supply in this model obeys: \\[H_{n,t}^{1/\\psi} = (\\alpha_{n}W_{n,t})C^{-\\sigma}_{n,t}.\\] To simplify below, assume that \\(\\beta=(1+r)^{-1}\\), so that the optimal solution features perfectly smoothed consumption, \\(C^*_{n}\\). Making appropriate substitutions gives \\(C^*_{n}\\) as the solution to: \\[ \\left(\\sum_{t}q_{t}\\right)C^*_{n} = \\sum_{t}\\left(q_{t}W_{n,t}^{1+\\psi}\\right)\\alpha_{n}^{\\psi}(C_{n}^*)^{-\\psi\\sigma} + A_{n,0}.\\]",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Simple Labor Supply Model</span>"
    ]
  },
  {
    "objectID": "models/dynamic-labor-supply.html#code-to-solve-the-model",
    "href": "models/dynamic-labor-supply.html#code-to-solve-the-model",
    "title": "5  A Simple Labor Supply Model",
    "section": "5.2 Code to solve the model",
    "text": "5.2 Code to solve the model\nThere is only one object to solve here which is consumption given a sequence of net wages. If one were to assume also constant wages the function below solves optimal consumption.\n\nusing Optim\nfunction solve_consumption(r,α,W,A,σ,ψ)\n    Q = 1/ (1 - 1/(1+r))\n    f(c) = (Q * c - Q * W^(1 + ψ) * α^ψ * c^(-σ*ψ) - A)^2\n    r = Optim.optimize(f,0.,A+W)\n    return r.minimizer\nend\n\nsolve_consumption (generic function with 1 method)",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Simple Labor Supply Model</span>"
    ]
  },
  {
    "objectID": "models/dynamic-labor-supply.html#code-to-simulate-a-cross-section",
    "href": "models/dynamic-labor-supply.html#code-to-simulate-a-cross-section",
    "title": "5  A Simple Labor Supply Model",
    "section": "5.3 Code to simulate a cross-section",
    "text": "5.3 Code to simulate a cross-section\nHere we’ll assume that wages, tastes for work, and assets co-vary systematically. For simplicity we’ll use a multivariate log-normal distribution.\nBelow is code to simulate a cross-section of 1,000 observations.\n\nusing Distributions\nfunction simulate_data(σ,ψ,r,N)\n    ch = [0.3 0. 0.; 0.5 0.5 0.; 0.4 0.8 1.8]\n    Σ = ch * ch'\n    X = rand(MvNormal(Σ),N)\n    α = exp.(X[1,:])\n    W = exp.(X[2,:])\n    A = exp.(X[3,:])\n    C = [solve_consumption(r,α[i],W[i],A[i],σ,ψ) for i in eachindex(A)]\n    @views H = exp.( X[1,:] .+ ψ .* X[2,:] .- ψ * σ .* log.(C) )\n    return (;α,W,A,C,H)\nend\n\n# assume risk-aversion of 2 and frisch of 0.5\nσ = 2.\nψ = 0.5\nr = 0.05\n\ndat = simulate_data(σ,ψ,r,1_000)\n\n(α = [0.8325625872008392, 0.9962630352835987, 0.8885806746692435, 1.0925701005945476, 0.979694089236883, 1.0836163138967156, 1.0331845132092115, 1.1752042758339816, 0.8430023365714854, 1.4876767291144681  …  1.1206653578445815, 1.0154072855426861, 0.8549042815472401, 0.8969690930290717, 1.3941648777204114, 0.7912102235012285, 1.0938611999642693, 0.8021512521340305, 1.2176718460396716, 1.8163966968053349], W = [1.1480125466348308, 1.0591778087855994, 0.6141750014717905, 2.4062527204162683, 0.9515391648667579, 1.4694506696800311, 2.163269816911264, 1.1095567931628112, 0.4213294924515321, 1.0757518906453958  …  0.8040407651475204, 1.7064189700818648, 0.7416468930397977, 0.7406881303740364, 0.5986946904751422, 0.2508092368097511, 2.2250003526030175, 0.5224764274006107, 0.9799857092690983, 1.4170755574152123], A = [34.954371964718916, 69.04149024036246, 0.10642255562358229, 6.1457236413797425, 12.091008629864714, 2.7819511555653023, 2.9945446049005966, 0.11309172021659913, 1.478521981687108, 0.5549403123623208  …  6.643346027575894, 10.681885071519977, 0.0031490102527746084, 0.3948753487051468, 0.036458214755153685, 0.21163283761176657, 1.1894745715936748, 0.5266191646516406, 14.53176268100821, 1.4341312243613662], C = [2.1794608123486925, 3.59070273905277, 0.6761250459659739, 2.1269719912185607, 1.288680812562386, 1.4295585579837027, 1.871075038928553, 1.12831249003358, 0.5375369701111443, 1.1798588416729197  …  1.046008602780442, 1.7744901918035187, 0.7447958815305814, 0.7864584756212141, 0.6351528873514529, 0.3393339393423258, 1.8916444375712305, 0.5942598960199967, 1.4369718900802375, 1.5423453879369953], H = [0.40929943369630645, 0.2855479196058828, 1.0299494441046484, 0.7968162077387962, 0.7415807703196826, 0.9188636907426071, 0.8121608078155509, 1.0971316294222433, 1.0179615385119214, 1.3077796563189361  …  0.9606821578909431, 0.7474970915192722, 0.9885048773507075, 0.9815657004947512, 1.6983944589243865, 1.1677136441386682, 0.8625568972206313, 0.9756930100961451, 0.8388645920473728, 1.401927541932])",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Simple Labor Supply Model</span>"
    ]
  },
  {
    "objectID": "models/dynamic-labor-supply.html#further-reading",
    "href": "models/dynamic-labor-supply.html#further-reading",
    "title": "5  A Simple Labor Supply Model",
    "section": "5.4 Further Reading",
    "text": "5.4 Further Reading\nMaCurdy (1981) and Blundell and Walker (1986) both consider estimation of closely related models of life-cycle labor supply.\n\n\n\n\nBlundell, Richard, and Ian Walker. 1986. “A Life-Cycle Consistent Empirical Model of Family Labour Supply Using Cross-Section Data.” The Review of Economic Studies 53 (4): 539–58.\n\n\nMaCurdy, Thomas E. 1981. “An Empirical Model of Labor Supply in a Life-Cycle Setting.” Journal of Political Economy 89 (6): 1059–85.",
    "crumbs": [
      "Five Prototype Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>A Simple Labor Supply Model</span>"
    ]
  },
  {
    "objectID": "why_models.html",
    "href": "why_models.html",
    "title": "6  How and Why to Use Models",
    "section": "",
    "text": "6.1 When you possibly don’t need a model.\nA lot of students, when they write their first paper, end up posing simple questions like “What is the effect of policy \\(X\\) on outcome \\(Y\\)”? Often, answering these questions requires nothing more than simple statistical models of causality (such as the Potential Outcomes Model or the Generalized Roy Model, which formalize causality in terms of potential outcomes).\nIn particular, if your question concerns the causal effect of an observed historical change in some variable, you likely won’t anything more elaborate than these simple frameworks. Consider below three examples of quasi-experimental variation from our prototype models that answer particular research questions without needing specification of the underlying models.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "why_models.html#when-you-possibly-dont-need-a-model.",
    "href": "why_models.html#when-you-possibly-dont-need-a-model.",
    "title": "6  How and Why to Use Models",
    "section": "",
    "text": "6.1.1 Social Security\nIn the life-cycle savings model, consider the inclusion of a social security system that provides income at older ages. The budget constraint becomes:\n\\[ c_{t} + a_{t+1}/(1 + r) \\leq (1-\\tau)y_{t} + \\mathbf{1}\\{t\\geq 65\\}b \\]\nso that individuals become eligible for a benefit, \\(b\\), after age 65, and pay into the system with proportional taxes, \\(\\tau\\). Suppose that the age of eligibility for social security is decreased, unexpectedly, from 65 to 60. And suppose that you have a repeated cross-section of data on individual consumption and age from periods before and after the unannounced reform.\nLet’s consider an approach to identifying objects of interest without specifying the full underlying model. To begin, let \\(t^*\\) index cohorts by their age at the time of policy announcement. Notice that \\(t^*\\) indicates both a cohort and a treatment. Suppose your question is: “What was the effect of this eligibility expansion on consumption at age \\(t\\) for a cohort aged \\(t^*\\) at the time of expansion?” Instead of specifying modeling assumptions, let’s specify how far one can get with a difference-in-differences approach.\nTo specify the causal effects of interest, we’ll use the language of potential outcomes. Let \\(C_{t*,t}(1)\\) be the potential outcome – a random variable – indicating consumption of an individual in cohort \\(t^*\\) at age \\(t\\) under the policy announcement. Similarly let \\(C_{t^*,t}(0)\\) be their corresponding potential outcome under the counterfactual: if the policy had never been announced. The target parameter identified by the research question above is \\[ \\alpha_{t*,t} = \\mathbb{E}[C_{t*,t}(1) - C_{t^*,t}(0)] \\] If there is sufficient variation, this parameter can be identified by assuming parallel trends with an untreated reference cohort. For the sake of this example, consider individuals who were 65 at the time of the policy announcement and hence (in principle) unaffected by the policy change. Parallel trends assumes that for some \\(t^*&lt;65\\) and for a pair of ages \\(t\\geq t^*\\), \\(s&lt;t^*\\): \\[ \\mathbb{E}[C_{t^*,t}(0)] - \\mathbb{E}[C_{65,t}(0)] = \\mathbb{E}[C_{t^*,s}(0)] - \\mathbb{E}[C_{65,t}] \\]\nAll we need then to identify \\(\\alpha_{t,t*}\\) is consumption for both cohorts at age \\(s\\) as well as at age \\(t\\), which allows us to construct the counterfactual \\(\\mathbb{E}[C_{t*,t}(0)]\\) in terms of observable quantities: \\[ \\alpha_{t,t*} = \\mathbb{E}[C_{t^*,t}(1)] - \\mathbb{E}[C_{65,t}(0)] - (\\mathbb{E}[C_{t^*,s}(0)] - \\mathbb{E}[C_{65,s}(0)]) \\] Note of course this was for a specific cohort, using a specific other cohort and a specific age \\(s\\) with which to construct the counterfactual. One could use many other cohorts and ages to do this, as is assumed by the common regression specification: \\[ \\mathbb{E}[C_{t^*,t}] = \\gamma_{t^*} + \\mu_{t} + \\alpha_{t^*,t}\\mathbf{1}\\{t\\geq t^*\\} \\] which implicitly layers in a stricter and interconnected set of parallel trends assumptions.\nTo proceed, we have only to defend the parallel trends assumption, which many view as less burdensome compared to defending the many layers of assumptions in a quantitative model. Of course, the farther apart two cohorts are from each other, the stronger this assumption might appear, and the regression specification does not allow us to select “more ideal” control groups for each cohort (Goodman-Bacon 2021).\n\n\n6.1.2 Firm Entry\nConsider the firm entry model. Suppose you have panel data \\((A_{m,t,1},A_{m,t,2},Z_{m,t})_{m=1,t=1}^{M,T}\\) on a set of markets (indexed by \\(m\\)) in a number of periods (indexed by \\(t\\)). Recall that \\(A_{m,t,j}\\) indicates whether firm \\(j\\) is active in market \\(m\\) at time \\(t\\), and firm entry occurs when \\(A_{m,t+1,j}-A_{m,t,j}=1\\). Recall that \\(X_{m}\\) is a a market-level factor that is unobservable here. Let \\(Z\\in\\{0,1\\}\\) indicate the presence of a policy that applies locally to market \\(m\\). For the purposes of this example, let’s say it’s a local minimum wage policy. To incorporate the policy \\(Z\\), suppose we write:\n\\[ u_{1}(x,a,d') = \\phi_{0} + \\phi_{1}x - \\phi_{2}d' - \\phi_{3}(1-a) + \\phi_{4}z \\]\nso that \\(\\phi_{4}\\) embodies the effect of the policy on payoffs for the firm. Finally, for simplicity, let’s assume that \\(Z_{m,1}=0\\) for all states initially, and in period \\(t^*\\) a subset of states adopt the policy permanently and that this adoption is unanticipated.\nSuppose your question is: “What is the effect of the minimum wage on firm entry?” Let \\(N_{m,t} = D_{m,t,1}+D_{m,t,2}\\) be the number of participating firms in market \\(m\\) at time \\(t\\). Let \\(N_{\\tau,m}(z)\\) be the potential outcomes of \\(N\\) in market \\(m\\), \\(\\tau\\) periods after the adoption of the minimum wage policy. Let’s define the dynamic effect of treatment on the treated (the effect of the minimum wage on markets that adopt it) as:\n\\[ \\alpha_{\\tau} = \\mathbb{E}[N_{\\tau}(1) - N_{\\tau}(0)|Z = 1] = \\mathbb{E}[\\Delta_{\\tau}|Z=1] \\]\nOur model doesn’t outline a theory of why certain markets adopt the minimum wage and why others don’t, but it does highlight that the effects of the policy will differ across markets, so it is important to account for heterogeneity: if there is any selection into policy adoption, we know that \\(\\mathbb{E}[\\Delta_{\\tau}|Z=1] \\neq \\mathbb{E}[\\Delta_{\\tau}]\\).\nA parallel trends assumption that justifies the event-study approach would be: \\[ \\mathbb{E}[N_{t}(0)|Z=1] - \\mathbb{E}[N_{t}(0)|Z=0] = \\text{constant}. \\] Note that if we assume that each market is in the ergodic distribution governed by the Markov Perfect Equilibria, then the distribution of \\(N\\) is stationary in each market absent the policy intervention and the parallel trends assumption is justified. The event-study specification: \\[ D_{m,t} = \\gamma_{m} + \\mu_{t} + \\mathbf{1}\\{t\\geq t^*\\}\\alpha_{t-t^*} + \\epsilon_{m,t} \\] would then robustly identify the average effect of the policy among the markets that adopted it. This is partly true because the timing of adoption was uniform. We should note that if adoption was staggered, given that there may be heterogeneous treatment effects, a regression-based approach to the event study would deliver some weighted average of these impacts that is hard to interpret (Goodman-Bacon 2021).\n\n\n6.1.3 Bundles of Tax Reforms\nFinally, consider a suite of tax reforms in the dynamic labor supply model. Suppose that there are three states, \\(A\\), \\(B\\), \\(C\\). Suppose that each runs a different experiment where they introduce a different set of taxes and transfers. Let \\(\\mathcal{Y}_{j}\\) indicate a net income function for each state \\(j\\). Consider the following examples:\n\\[\\begin{align}\n\\mathcal{Y}_{A}(W,H) = b_{A} + (1-\\tau_{A})WH \\\\\n\\mathcal{Y}_{B}(W,H) = WH + \\sum_{k=0}^{5}\\tau_{k}(WH-\\overline{E}_{k})\\mathbf{1}\\{WH&gt;\\overline{E}_{k}\\} \\\\\n\\mathcal{Y}_{C}(WH) = WH(1-\\tau_{C}) + \\mathbf{1}\\{H&gt;20\\}b_{C}\n\\end{align}\\]\nAnd suppose that these participants do not anticipate their assignment to treatment in this experiment, which is expected to last for 3 periods. Let \\(Z_{j}\\in\\{0,1\\}\\) indicate assignment to either treatment or control group in state \\(j\\).\nIf your research question is: “What is the effect of each unannounced, temporary, tax reform on labor supply?” then one could simply compare the means of treatment and control in each state:\n\\[ \\mathbb{E}[H|Z_{j}=1, j] - \\mathbb{E}[H|Z_{j} = 0, j] \\]\nwhich uncovers the causal effect of \\(Z\\) by virtue of random assignment.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "why_models.html#reasons-to-use-a-model",
    "href": "why_models.html#reasons-to-use-a-model",
    "title": "6  How and Why to Use Models",
    "section": "6.2 Reasons to Use a Model",
    "text": "6.2 Reasons to Use a Model\nHaving covered these three (perfectly valid) applications of quasi-experimental methods to answer specific research questions, let us now consider some questions that these methods don’t answer and consider the useful role that models can play in these (and related) contexts.\n\n6.2.1 When the question can’t be articulated without one\nPerhaps the most obvious reason to use a model is when you research question simply cannot be articulated without one. Some of the most useful insights from economic modeling are statements about economic efficiency, the potential for policies to resolve market inefficiencies, and the design of policies. Some examples:\n\nIn the labor supply model, given a particular weighted welfare objective, what does the optimal system of taxes and transfers look like?\nWhat is the cheapest way to incentivize competition between two firms in dynamic duopoly?\nWhat is are the welfare costs of incomplete markets in the life-cycle savings model?.\n\n\n\n6.2.2 To make welfare calculations\nRevealed preference is one of the more powerful tools in an economist’s toolkit: if we treat individuals in our data as people who know what they like, we can try to infer their preferences and decide how they value different policy environments. For example:\n\nHow do individuals value the social security program introduced in the savings model? How would they value a program with a different combination of taxes (\\(\\tau\\)) and payments \\(b\\)?\nIn the labor supply model, what is the sum of individual’s willingness to pay for a lumb sum payment \\(b\\) that is financed by proportion taxes \\(\\tau\\)? We’ll come back to this one below.\n\n\n\n6.2.3 To make sense of otherwise puzzling data\nHere it’s hard to look past the most fundamental and basic causal question in our profession: what is the effect of price on quantities? You know of course that this is a silly question, but we only know this because the theory of supply and demand is so fundamental to our view of the world.\nSuppose you observe prices and quantities in a market over time. Without the theory of supply and demand, all you would see is a cloud of points.\n\n\n\n\n\n\n\nFigure 6.1: Shifting supply and demand curves trace out equilibrium points with no apparent pattern\n\n\n\n\nWith the theory of supply and demand, you and understand that each point is the simultaneous equilibrium outcome of two underlying structural relationships in equilibrium. Phillip and / or Sewall Wright proposed the solution: Instrumtal Variables, which is perhaps the earliest known example of an estimated structural model.\n\n\n6.2.4 When variation does not identify the counterfactual of interest\nIn our examples above, you may have noted that we were careful to very specifically define the “treatment” in order to specify the causal object of interest. Models can help us articulate just exactly what kind of causal parameters can and cannot be identified by observed variation, as well as outlining a specific set of assumptions under which related counterfactuals can be forecast even though they are not exactly replicated by existing variation. Each section below discussed a number of examples in the context of each application. Researchers often frame this as a question of internal vs external validity, but there are too many interesting examples in the “external validity” column to not discuss them in more depth. In general, a key point made by Heckman and Vytlacil (2005) is that estimands from simple statistical mdoels designed to infer causal effects (such as those we get from difference-in-differences, IV, and regression discontinuity) are rarely parameters of exact policy interest.\nWe’ll use examples to explore these ideas.\n\n6.2.4.1 Social Security\nTo make the example concrete, let’s make some additional simplifying assumptions for the savings model. These make the quantitative model a bit less interesting, but help us think through the issues. Specifically, let’s assume:\n\nEach individual faces a known sequence \\(\\{y_{n,t}\\}_{t=1}^{T}\\) of income realizations.\nAgents face a natural borrowing constraint, yielding an intertemporal borrowing constraint at each \\(t\\): \\[ \\sum_{s=t}^{T}q_{s-t}c_{s} \\leq a_{t} + \\sum_{s=t}^{T}q_{s-t}(1-\\tau)y_{n,s} + \\sum_{s\\geq 65}^{T}q_{s-t}b \\] where \\(q_{\\tau} = 1/(1+r)^{\\tau}\\) is the price of a unit of consumption \\(\\tau\\) periods ahead.\nSet \\(\\beta(1+r)=1\\), and \\(\\psi=0\\) (no bequest motive), indicating that agents will elect to perfectly smoooth their consumption over periods so that consumption is equal to the net present value of net income (something they can do due to the natural borrowing constraint).\n\nWith these assumptions, we can write the mean effect on consumption for cohort \\(t^*\\) at any age \\(t\\geq t^*\\) as simply the effect of the announcement on the NPV of net income at age \\(t^*\\):\n\\[ \\Delta C_{t^*,t} = \\sum_{s=60}^{64}q_{s-t^*}b + \\sum_{s=t^*}^{T}(\\tau - \\tau')\\overline{y}_{t^*,t} \\] where \\(\\overline{y}_{t^*,t}\\) is average income for cohort \\(t^*\\) at age \\(t\\).\nWith these assumptions, note that:\n\nThe parallel trends assumption holds: under the counterfactual of no policy change, differences across cohorts are constant with age (\\(\\mu_{t}\\) can be normalized to zero).\nThe difference-in-difference approach therefore robustly identifies, with \\(\\alpha_{t^*,t}\\), the causal effect of the policy on cohort \\(t^*\\) at age \\(t\\).\nThe model implies that \\(\\alpha_{t^*,t}\\) is constant with \\(t\\).\nEach effect depends on how each cohort \\(t^*\\) expects the policy expansion to be financed, through the change in marginal tax rates \\(\\tau' - \\tau\\). We haven’t specified financing constraints and hence we cannot speculate on this without more structure.\n\nAre the parameters \\(\\alpha_{t^*,t}\\) policy relevant quantities? They are certainly informative, but now that eligibility has been expanded, these effects don’t tell us about the effect of future changes in policy. They don’t even (without additional assumptions) tell us what the effect would be if the expansion were repealed. We need more theory and assumptions to extrapolate. This is a good task for economic models!\nThe points below suggest some compelling counterfactuals that the DD approach does not recover.\n\nThe total effect of the social security policy (not just the expansion).\nThe effect of additional changes in the age of eligibility.\nThe effect of changes in \\(b\\) and \\(\\tau\\) on consumption at different ages, and at differen horizons of anticipation.\nIf there is any reason to think that effects are heterogeneous by cohort (if they face different wage profiles, for example), we also cannot construct estimates on the effect on cohort \\(t^*\\) if they had known learned about the policy at any age other than \\(t^*\\) (there is a one-to-one mapping between cohort and treatment).\nThe effect on consumption for cohorts who have known about the expansion for their whole life-cycle.\nThe effect of the expansion with under alternative financing arrangements.\n\n\n\n6.2.4.2 Tax Reform\nFor concreteness, let’s consider the effect of reform \\(A\\) on labor supply, when \\(\\beta(1+r)=1\\) and optimal consumption is stationary over time. It is given by: \\[\\Delta H_{n,t} = \\psi\\log(1-\\tau) - \\psi\\sigma\\Delta \\log\\left(C^*_{n}\\right). \\] The consumption response \\(\\Delta C^*_{n}\\) embodies the income effect and can be solved by plugging optimal labor supply into the intertemporal budget constraint.\nThis concrete example helps us to understand three more general points about each reform:\n\nThe average treatment effect depends on income effects, which in turn depend on the perceived length of time that the tax reform is enforced.\nThe model exhibits lots of heterogeneity in treatment effects. As such, if there are differences in underlying distributions of wages or work costs, we should expect different impacts.\n\nThus, although each welfare experiment robustly identifies the effect of tax reform \\(A\\) and population \\(A\\), tax reform \\(B\\) on population \\(B\\), and so forth. It does not identify:\n\nThe effect of any tax reform with different persistence (real or perceived). Moreover, one would want to interpret the experimental findings very carefully to ensure that individuals in the experiment were given adequate information about the length of time of the experiment.\nThe effect of tax reform \\(A\\) on population \\(B\\), \\(C\\), etc, and likewise for tax reforms \\(B\\), \\(C\\), etc on alternative populations.\nThe distribution of treatment effects at each location.\nThe effect of tax reforms that are already partly anticipated by individuals.\nThe effect of a scaled up tax reform, where equilibrium effects on wages might appear.\nThe distribution of effects of each tax reform. Here the model can be used to be interpret available panel data and invert out distributions in observed labor market productivities and work costs. One can then return to the experimental data and estimate average treatment effects along these latent dimensions.\n\n\n\n6.2.4.3 Entry-Exit Model\nBy now we have made the point several ways, but when it comes to the model of dynamic duopoly, we can note that the model-free approach does not identify:\n\nThe effect of the minimum wage policy when it’s introduction is anticipated several periods earlier.\nThe effect of the minimum wage on the markets that do not adopt it.\nThe effect of repealing the minimum wage.\nThe effect of nominal changes to the minimum wage.\n\nEach of which might be considered much more useful or compelling policy calculations.\n\n\n\n6.2.5 To interpolate existing variation in the data\nSticking with the tax reform example, let’s consider what it would take to jointly understand the effects. We know that each reform is related because each comes in the form of an infinite dimensional object: a function \\(\\mathcal{T}(W,H)\\) of wages and hours. A model-free theory that attempts to estimate labor supply as a non-parametric function of this function would not get very far due to the implausible quantities of policy variation that would be needed to estimate it.\nEconomic models often provide a useful way to interpolate related – but not functionally identically related – variation. Notice that in the labor supply model any function \\(\\mathcal{T}\\) is articulated through its effect on the budget constraint, and different policy reforms can be compared in the model without the addition of any new parameters. I refer to this property as articulated variation: when the effect of a variable can be modeled without the need for additional parameters. Typically, this involves a priori known changes to prices and endowments.\nIn this example, with structural parameters in hand, any function $ of wages and hours and can be modeled, and hence any observed variation in this function can be interpreted through the model without additional parameters. In Mullins (2026) I conduct a very similar example using data on welfare reform experiments in the United States.\nHere is a useful counterexample: policy variation that is not well-articulated inside a model. Consider our choice of modeling the minimum wage in our entry and exit model: embodied as a parameter \\(\\phi_{4}\\). Our choice to not model within-period production decisions of the firm (and instead estimate a reduced form) means that this policy is not well-articulated. We need an additional parameter \\(\\phi_{4}\\) to model its effect, and further changes to the nominal wage cannot be studied.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "why_models.html#marschaks-maxim",
    "href": "why_models.html#marschaks-maxim",
    "title": "6  How and Why to Use Models",
    "section": "6.3 Marschak’s Maxim",
    "text": "6.3 Marschak’s Maxim\nOur discussion so far has focused on exploring the boundaries of what simpler causal inference strategies can identify in order to make statements about how economic modeling can add value in research. This is not very instructive for how we do economic modeling. Of course this topic is more of an art than a science, but Marschak’s Maxim (Marschak 1953; Heckman and Vytlacil 2007) is a useful principle that can guide us. Here it is:\n\nResearchers should specify the minimal set of model ingredients in order to answer the research question of interest.\n\nThis may seem obvious, and that the hard part is figuring out what this minimal set is. It’s true that this can be hard to decide, but it’s surprisingly easy to forget this simple rule once you are deep inside your model and making decisions. The question “Is this essential to my question of interest?” is not always easy to answer but it is one you should be repeatedly asking yourself. Your research question is the mast you tie yourself to, and you should decide as early as possible what it is.\nLet’s do some clean examples to make the point clear.\n\n6.3.1 Marschak’s Original Example\nMarschak (1953) considers a very simple problem: a monopolist that chooses quantities to maximize profit, and a government that taxes quantities. Suppose that demand is given by \\[ p = \\alpha_0 - \\alpha_1 q \\] and that firms produce with constant marginal costs, \\(c\\). Firm profits as a function of quantities is: \\[ \\Pi(q) = (\\alpha_0 - c - \\tau)q - \\alpha_1 q^2 \\] yielding optimal quantities \\[ q^* = \\frac{\\alpha_0 - c - \\tau}{2\\alpha_1} \\] The government’s tax revenue is: \\[ R = q \\tau = \\frac{\\alpha_0 - c - \\tau}{2\\alpha_1} \\tau \\]\nSupposing that, intially, quantity \\(q\\) varies for exogenous reasons across markets, Marschak considers three problems and how an individual could learn the solution to their problems from data.\n\nMaximizing profit To maximize profit, the firm could look at how their profits evolve with \\(q\\): \\(\\Pi = a q - b q^2\\), and set \\(q^* = \\frac{a}{2b}\\)\nTo forecast how their revenue changes with firms quantities across markets, the Government needs only to know their tax rate \\(\\tau\\).\nTo set taxes to optimize tax revenue, and assuming that firms learn their optimal output also, the government need only extract the linear component \\(a = \\alpha_0 - c - \\ tau\\) from the observed profit relationship and add the observed tax rate \\(\\tau\\), setting \\(\\tau^* = (a+\\tau)/2 = (\\alpha_0 - c)/2\\).\n\nAlthough the setup is a little unconventional (we are used to assuming agents already act optimally), two lessons resonate from this exercise:\n\nFor each question, the analyst only needs to know certain combinations of parameters – each of which can be observed in reduced-form relationships – to answer their question of interest.\nOnce the firm optimizes, the reduced form relationship between tax revenue and quantities is not sufficient for forecasting the tax revenue from future changes to \\(\\tau\\).\n\nPoint (2) is essentially an early version of the Lucas Jr (1976) critique, while point (1) is the one we will mostly try to take lessons from. It emphasizes that in some stylized cases, the answers to some questions are essentiall invariant to particular model ingredients and we need not consider them. Although in practice we cannot always guarantee this, the insight undergirds a philosophical approach to quantitative modeling that bends always toward finding a minimal set of ingredients.\n\n\n6.3.2 Two-Stage Budgeting\n\n\n6.3.3 Sufficient Statistics\nThe welfare objective becomes: \\[ V(\\tau) = \\int(WH(1-\\tau))dF(W) - v(H) + \\tau\\int WHdF(W) \\]\nNow taking \\(dV/d\\tau\\) we can apply the envelope theorem to get:\n\\[ \\frac{dV}{d\\tau} = \\int - WH + WH + \\tau \\int W dH/d\\tau dF(W) \\]\nwhich we can rearrange, noting that \\(dH/d\\tau = \\varepsilon_{H,W}H\\) where\nto get:\n$$ \n\n\n6.3.4 Exercises\n\nExercise 6.1 Consider a static version of the labor supply model where earnings are the only source of income:\n\\[ H_{n} = \\arg\\max_{h} \\frac{(Wh)^{1-\\sigma}}{1-\\sigma} - \\alpha_{n}^{-1}\\frac{h^{1+1/\\psi}}{1+1/\\psi} \\]\nAssume that wages follow\n\\[ \\log(W) = \\gamma_{0} + \\gamma_{1}Z + \\epsilon \\]\nwhere \\(Z \\perp \\alpha\\) but \\(\\epsilon\\) and \\(\\alpha\\) are potentially correlated.\n\nSolve for \\(H_{n}\\) in terms of \\(W\\)\nSuppose you have cross-sectional data \\((W_{n},Z_{n},H_{n},C_{n})_{n=1}^{N}\\) used \\(Z\\) as an instrument to estimate the relationship \\[ \\log(H) = \\alpha_0 + \\alpha_1 \\log(W) + \\varepsilon \\] via 2SLS. What is the population limit of \\(\\alpha_1\\) as \\(N\\rightarrow\\infty\\)? (Hint: it should follow from your answer to part 1).\nConsider a policy reform that introduces a proportional subsidy, \\(\\tau\\), so that net wages are \\((1+\\tau)W\\). Is the combination of parameters \\(\\alpha_{1}\\) sufficient for forecasting the effects of this policy? If this is your question of interst, what does Marschak’s Maxim suggest about the need to separately identify parameters here?\nNow suppose an alternative policy reform that consists of a lump sum transfer \\(b\\) and a proportional labor market tax, \\(\\tau\\), so that net income is \\(b + (1-\\tau)WH\\). Do you think \\(\\alpha_{1}\\) is sufficient to forecast the effect of this new policy on labor supply? An intuitive explanation is sufficient, but you can try calculating the local change \\(\\partial H / \\partial b |_{b=0}\\) if you want to be more precise.\nBased on your answer to part 4, and assuming that the model we wrote down is the true data generating process, do you think it is even possible to forecast the effect of this alternative policy given these data?\n\n\n\nExercise 6.2 Consider the following simple model of time allocation. Individual utility is given by:\n\\[ U(C,L) = (\\phi C^{\\rho} + (1-\\phi) L^{\\rho})^{1/\\rho} \\]\nwhere \\(L\\) is an aggregate leisure good composed of \\(K\\) different activities:\n\\[ L = \\prod_{k=1}^{K}l_{k}^{\\delta_{k}},\\qquad \\sum_{k}\\delta_{k} = 1 \\]\nIn addition to these leisure activities, the agent may supply labor to the market at a wage rate of \\(w\\). Letting \\(h_{k}\\) be hours, the time constraint is:\n\\[ h_{k} + \\sum_{k}l_{k} = 1 \\]\nThe model is static and the individual solves the following problem:\n\\[ \\max_{C,\\{l_{k}\\}_{k=1}^{K}} U(C,L) \\]\nsubject to the constraint:\n\\[ C + w \\left(\\sum_{k} l_{k}\\right) \\leq w \\]\n\nSuppose you are interested in using this model to study the effects of a wage subsidy on labor supply. Notice that the model can be written as \\[ \\max_{C,h} U(wh,L^*(1-h)) \\] where \\[ L^*(1-h) = \\max_{\\{l_{k}\\}_{k=1}^{K}} \\prod_{k=1}^{K}l_{k}^{\\delta_{k}} \\] subject to \\(\\sum_{k}l_{k} = 1-h\\). Given this simplification, what does Marschak’s Maxim (and common sense) suggest about what parameters need to be estimated here?\nBased on your answer to the above, you simplify the model to the following specification: \\[ h^* = \\arg\\max (\\phi (wh)^{\\rho} + (1-\\phi) (1-h)^{\\rho})^{1/\\rho} \\] and you derive the following relationship: \\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi}{1-\\phi}\\right) + \\frac{1}{1-\\rho}\\log(w) \\] where \\(C=wh^*\\) is total labor income and \\(L=1-h^*\\) is non-market time. Suppose you have a cross-section of data \\((C_{n},L_{n},W_{n})\\) where \\(C_{n}\\) is labor market earnings, \\(L_{n}\\) is non-market time, and \\(W_{n}\\) is the wage-rate for person \\(n\\). This could be taken (for example) from the Outgoing Rotation Group of the CPS monthly survey. Does the model, as written, allow for any randomness in the relationship between \\(C_{n}/L_{n}\\) and \\(W_{n}\\)? Is this likely to be replicated in the data?\nSuppose now you augment the model to acommodate some randomness in how much individuals work by allowing for heterogeneity in preferences (\\(\\phi\\)): \\[ \\log\\left(\\frac{C}{L}\\right) =  \\frac{1}{1-\\rho}\\log\\left(\\frac{\\phi_{n}}{1-\\phi_{n}}\\right) + \\frac{1}{1-\\rho}\\log(w) \\] What assumption do you need for an OLS regression of \\(\\log(C_{n}/L_{n})\\) on \\(\\log(W_{n})\\) to consistently recover the elasticity of labor supply, \\(1/(1-\\rho)\\)? Do you consider this credible? Why/why not?\n\n\n\n\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nHeckman, James, and Edward Vytlacil. 2005. “Structural equations, treatment effects, and econometric policy evaluation.” Econometrica 73 (3): 669–738.\n\n\n———. 2007. “Chapter 70 Econometric Evaluation of Social Programs, Part i: Causal Models, Structural Models and Econometric Policy Evaluation.” In, edited by James J. Heckman and Edward E. Leamer, 6:4779–874. Handbook of Econometrics. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(07)06070-9.\n\n\nLucas Jr, Robert E. 1976. “Econometric Policy Evaluation: A Critique.” In Carnegie-Rochester Conference Series on Public Policy, 1:19–46. North-Holland.\n\n\nMarschak, Jacob. 1953. “Economic Measurements for Policy and Prediction.” In Studies in Econometric Method, edited by W. Hood and C. Koopmans. John Wiley & Sons.\n\n\nMullins, Joseph. 2026. “A Structural Meta-Analysis of Welfare Reform Experiments and Their Impacts on Children.” Journal of Political Economy 134 (1): 435–77. https://doi.org/10.1086/738482.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>How and Why to Use Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aguirregabiria, Victor, and Pedro Mira. 2007. “Sequential\nEstimation of Dynamic Discrete Games.” Econometrica 75\n(1): 1–53.\n\n\nArellano, Manuel, Richard Blundell, and Stephane Bonhomme. 2018.\n“Nonlinear Persistence and Partial Insurance: Income and\nConsumption Dynamics in the PSID.” AEA Papers and\nProceedings 108 (May): 281–86. https://doi.org/10.1257/pandp.20181049.\n\n\nBlundell, Richard, and Ian Walker. 1986. “A Life-Cycle Consistent\nEmpirical Model of Family Labour Supply Using Cross-Section\nData.” The Review of Economic Studies 53 (4): 539–58.\n\n\nDe Nardi, Mariacristina. 2004. “Wealth Inequality and\nIntergenerational Links.” The Review of Economic Studies\n71 (3): 743–68. https://doi.org/10.1111/j.1467-937X.2004.00302.x.\n\n\nDearing, Adam, and Jason R. Blevins. 2024. “Efficient and\nConvergent Sequential Pseudo-Likelihood Estimation of Dynamic Discrete\nGames.” Review of Economic Studies.\n\n\nEricson, Richard, and Ariel Pakes. 1995. “Markov-Perfect Industry\nDynamics: A Framework for Empirical Work.” The Review of\nEconomic Studies 62 (1): 53–82. https://doi.org/10.2307/2297841.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with\nVariation in Treatment Timing.” Journal of Econometrics\n225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nGourinchas, Pierre-Olivier, and Jonathan A. Parker. 2002.\n“Consumption over the Life Cycle.” Econometrica 70\n(1): 47–89. https://doi.org/10.1111/1468-0262.00269.\n\n\nHeckman, James J, and Bo E Honore. 1990. “The Empirical Content of\nthe Roy Model.” Econometrica: Journal of the Econometric\nSociety, 1121–49.\n\n\nHeckman, James, and Edward Vytlacil. 2005. “Structural equations, treatment effects, and econometric\npolicy evaluation.” Econometrica 73 (3): 669–738.\n\n\n———. 2007. “Chapter 70 Econometric Evaluation of Social Programs,\nPart i: Causal Models, Structural Models and Econometric Policy\nEvaluation.” In, edited by James J. Heckman and Edward E. Leamer,\n6:4779–874. Handbook of Econometrics. Elsevier. https://doi.org/https://doi.org/10.1016/S1573-4412(07)06070-9.\n\n\nLucas Jr, Robert E. 1976. “Econometric Policy Evaluation: A\nCritique.” In Carnegie-Rochester Conference Series on Public\nPolicy, 1:19–46. North-Holland.\n\n\nMaCurdy, Thomas E. 1981. “An Empirical Model of Labor Supply in a\nLife-Cycle Setting.” Journal of Political Economy 89\n(6): 1059–85.\n\n\nMarschak, Jacob. 1953. “Economic Measurements for Policy and\nPrediction.” In Studies in Econometric Method, edited by\nW. Hood and C. Koopmans. John Wiley & Sons.\n\n\nMullins, Joseph. 2026. “A Structural Meta-Analysis of Welfare\nReform Experiments and Their Impacts on Children.” Journal of\nPolitical Economy 134 (1): 435–77. https://doi.org/10.1086/738482.\n\n\nRoy, A. D. 1951. “Some Thoughts on the Distribution of\nEarnings.” Oxford Economic Papers 3 (2): 135–46. http://www.jstor.org/stable/2662082.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Appendix A — Data",
    "section": "",
    "text": "A.1 A Disclaimer for IPUMS CPS data\nThese data are a subsample of the IPUMS CPS data available from cps.ipums.org. Any use of these data should be cited as follows:\nSarah Flood, Miriam King, Renae Rodgers, Steven Ruggles, J. Robert Warren, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Megan Schouweiler, and Michael Westberry. IPUMS CPS: Version 11.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D030.V11.0\nThe CPS data file is intended only for exercises as part of ECON8208. Individuals are not to redistribute the data without permission. Contact ipums@umn.edu for redistribution requests. For all other uses of these data, please access data directly via cps.ipums.org.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#a-disclaimer-for-ipums-cps-data",
    "href": "data.html#a-disclaimer-for-ipums-cps-data",
    "title": "Appendix A — Data",
    "section": "",
    "text": "Arellano, Manuel, Richard Blundell, and Stephane Bonhomme. 2018. “Nonlinear Persistence and Partial Insurance: Income and Consumption Dynamics in the PSID.” AEA Papers and Proceedings 108 (May): 281–86. https://doi.org/10.1257/pandp.20181049.\n\n\nDearing, Adam, and Jason R. Blevins. 2024. “Efficient and Convergent Sequential Pseudo-Likelihood Estimation of Dynamic Discrete Games.” Review of Economic Studies.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "autodiff.html",
    "href": "autodiff.html",
    "title": "Appendix B — Automatic Differentiation",
    "section": "",
    "text": "B.1 Why AD Matters for Structural Estimation\nIn structural econometrics, we frequently need gradients for:\nHand-coding derivatives is tedious and error-prone. Finite differences are slow (requiring \\(O(n)\\) function evaluations for an \\(n\\)-dimensional gradient) and can be numerically unstable. AD provides exact gradients efficiently.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#why-ad-matters-for-structural-estimation",
    "href": "autodiff.html#why-ad-matters-for-structural-estimation",
    "title": "Appendix B — Automatic Differentiation",
    "section": "",
    "text": "Optimization (MLE, GMM, minimum distance)\n\nGradient-free methods such as Nelder-Mead are popular, of course, but are less efficient\n\nComputing standard errors\nSolving models with equilibrium conditions (using Newton’s method, for example)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#forward-mode-vs-reverse-mode",
    "href": "autodiff.html#forward-mode-vs-reverse-mode",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.2 Forward Mode vs Reverse Mode",
    "text": "B.2 Forward Mode vs Reverse Mode\nAD comes in two flavors:\nForward mode propagates derivatives forward through the computation. For a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\), computing the full Jacobian requires \\(n\\) forward passes. This is efficient when \\(n \\ll m\\).\nReverse mode propagates derivatives backward (like backpropagation in neural networks). Computing the full Jacobian requires \\(m\\) reverse passes. This is efficient when \\(m \\ll n\\).\nFor most estimation problems, we have a scalar objective (\\(m = 1\\)) and many parameters (\\(n\\) large), so reverse mode is typically preferred. In my experience however, I have had more success writing code that is compatible with forward differencing. You will learn from experience that these tools can be fussy.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#ad-in-julia",
    "href": "autodiff.html#ad-in-julia",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.3 AD in Julia",
    "text": "B.3 AD in Julia\nJulia’s AD ecosystem is excellent. The main packages are:\n\nB.3.1 ForwardDiff.jl\nForward-mode AD. Simple and robust, works out-of-the-box for most pure Julia code.\n\nusing ForwardDiff\n\nf(x) = sum(x.^2)\nx = [1.0, 2.0, 3.0]\n\n# Gradient\nForwardDiff.gradient(f, x)\n\n3-element Vector{Float64}:\n 2.0\n 4.0\n 6.0\n\n\n\n# Hessian\nForwardDiff.hessian(f, x)\n\n3×3 Matrix{Float64}:\n 2.0  0.0  0.0\n 0.0  2.0  0.0\n 0.0  0.0  2.0\n\n\n\n\nB.3.2 Enzyme.jl\nA high-performance AD engine that works at the LLVM level. Supports both forward and reverse mode. Often the fastest option, especially for code with loops and mutations.\n\nusing Enzyme\n\nf(x) = x[1]^2 + sin(x[2])\n\nx = [1.0, 2.0]\ndx = zeros(2)\n\n# Reverse mode gradient\nEnzyme.autodiff(Reverse, f, Active, Duplicated(x, dx))\ndx\n\n2-element Vector{Float64}:\n  2.0\n -0.4161468365471424\n\n\n\n\nB.3.3 Zygote.jl\nA source-to-source reverse-mode AD system. Popular in machine learning (used by Flux.jl). Works well for array-heavy code but may struggle with control flow.\n\nusing Zygote\n\nf(x) = sum(x.^2)\nx = [1.0, 2.0, 3.0]\n\nZygote.gradient(f, x)\n\n([2.0, 4.0, 6.0],)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#practical-recommendations",
    "href": "autodiff.html#practical-recommendations",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.4 Practical Recommendations",
    "text": "B.4 Practical Recommendations\n\nStart with ForwardDiff for problems with few parameters (&lt; 100). It’s the most reliable.\nUse Enzyme for performance-critical code, especially if you have loops or in-place mutations.\nBe aware of limitations: AD systems can fail on code that uses certain constructs (try-catch, foreign function calls, some global variables). When in doubt, test that your gradients match finite differences:\n\n\nusing ForwardDiff, FiniteDiff\n\nf(x) = log(1 + exp(x[1] * x[2])) + x[3]^2\nx = [1.0, 2.0, 3.0]\n\nad_grad = ForwardDiff.gradient(f, x)\nfd_grad = FiniteDiff.finite_difference_gradient(f, x)\n\nmaximum(abs.(ad_grad .- fd_grad))\n\n5.3512749786932545e-12",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#integration-with-optimization",
    "href": "autodiff.html#integration-with-optimization",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.5 Integration with Optimization",
    "text": "B.5 Integration with Optimization\nMost Julia optimization packages accept AD gradients. Here’s an example with Optim.jl:\n\nusing Optim, ForwardDiff\n\nrosenbrock(x) = (1 - x[1])^2 + 100*(x[2] - x[1]^2)^2\nx0 = [0.0, 0.0]\n\n# With automatic gradients via ForwardDiff\nresult = optimize(rosenbrock, x0, LBFGS(); autodiff = :forward)\nresult.minimizer\n\n2-element Vector{Float64}:\n 0.999999999999928\n 0.9999999999998559",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#example-maximum-likelihood-with-optim.jl",
    "href": "autodiff.html#example-maximum-likelihood-with-optim.jl",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.6 Example: Maximum Likelihood with Optim.jl",
    "text": "B.6 Example: Maximum Likelihood with Optim.jl\nConsider a simple probit model:\n\\[ D = \\mathbf{1}\\{X\\beta - \\nu \\geq 0\\},\\qquad \\nu \\sim \\mathcal{N}(0,1) \\]\nHere is code to simulate data for this model:\n\nusing Random, Distributions\n\nfunction sim_data(X ; γ)\n    N = size(X,1)\n    ν = rand(Normal(),N)\n    D = (X * γ .- ν) .&gt; 0\n    return D\nend\n\n# a quick test of the function:\nN = 1000\nX = [ones(N) 2*rand(Normal(),N)]\nγ = [0.1, 0.5]\nD = sim_data(X ; γ);\n\nConsider the problem of estimating \\(\\gamma\\) using maximum likelihood. We will establish the properties of this estimator in class. Here let’s just focus on numerically how to attack the minimization problem. The log-likelihood of the data D given X is:\n\\[ \\mathcal{L}(\\gamma) = \\sum_{n}l(D_{n}; X_{n},\\gamma) = \\sum_{n=1}^{N}D_{n}\\log(\\Phi(X\\gamma)) + (1-D_{n})\\log(1-\\Phi(X\\gamma)) \\]\nLet’s write up this likelihood function.\n\nfunction log_likelihood(D,X,γ)\n    ll = 0.\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            ll += log(cdf(Normal(),xg))\n        else\n            ll += log(1-cdf(Normal(),xg))\n        end\n    end\n    return ll\nend\nlog_likelihood(D,X,[0.,0.])\n\n-693.1471805599322\n\n\n\nB.6.1 Numerical Optimization\nOptimization is most efficient when we have access to the first and second order derivatives of the function. There is a general class of hill-climbing (or descent in the case of minimization) algorithms that find new guesses \\(\\gamma_{k+1}\\) given \\(\\gamma_{k}\\) as:\n\\[ \\gamma_{k+1} = \\gamma_{k} + \\lambda_{k}A_{k}\\frac{\\partial Q}{\\partial \\gamma} \\]\nwhere \\(Q\\) is the function being maximized (or minimized). \\(A_{k}\\) defines a direction in which to search (providing weights on the derivatives) and \\(\\lambda_{k}\\) is a scalar variable known as a step-size which is often calculated optimally in each iteration \\(k\\). For Newton’s method, the matrix \\(A_{k}\\) is the inverse of the Hessian of the objective function \\(Q\\). Since the hessian can sometimes be expensive to calculate, other methods use approximations to the Hessian that are cheaper to compute.\nSince we have a simple model, we can calculate derivatives relatively easily. Below we’ll compare a hard-coded derivative to this automatic differentiation.\n\nusing ForwardDiff\n\nfunction deriv_ll(D,X,γ)\n    dll = zeros(2)\n    for n in eachindex(D)\n        xg = X[n,1] * γ[1] + X[n,2] * γ[2] \n        if D[n]\n            dl = pdf(Normal(),xg) / cdf(Normal(),xg)\n        else\n            dl = - pdf(Normal(),xg) / (1 - cdf(Normal(),xg))\n        end\n        dll[1] += X[n,1] * dl\n        dll[2] += X[n,2] * dl            \n    end\n    return dll\nend\ndx = zeros(2)\n# forward mode\nauto_deriv_ll(D,X,γ) = ForwardDiff.gradient(x-&gt;log_likelihood(D,X,x),γ)\n# reverse mode\nauto_deriv2_ll(D,X,γ,dx) = Enzyme.autodiff(Reverse, x-&gt;log_likelihood(D,X,x), Active, Duplicated(γ, dx))\n\nd1 = deriv_ll(D,X,γ)\nd2 = auto_deriv_ll(D,X,γ)\nauto_deriv2_ll(D,X,γ,dx)\n[d1 d2 dx]\n\n2×3 Matrix{Float64}:\n 11.1675  11.1675  11.1675\n 59.9985  59.9985  59.9985\n\n\nOk so we’re confident that these functions work as intended, but how do they compare in performance?\n\n@time deriv_ll(D,X,γ);\n@time auto_deriv_ll(D,X,γ);\n@time auto_deriv2_ll(D,X,γ,dx);\n\n  0.000028 seconds (2 allocations: 80 bytes)\n  0.000044 seconds (7 allocations: 304 bytes)\n  0.000038 seconds\n\n\nAll are quite quick and you can see that we’re not losing much with automatic differentiation. In my experience, the gap between the two methods can narrow for more complicated functions.\nSo now let’s try implementing the maximum likelihood estimator using two different gradient-based algorithms: Newton’s Method (which uses the Hessian), and the Broyden–Fletcher–Goldfarb–Shannon (BFGS) algorithm (which updates search direction using changes in the first derivative).\nWhile Newton’s method requires calculation of the Hessian (second derivatives), BFGS and related methods only require first derivatives. Typically, this makes each iteration quicker but will take more time to converge. Let’s test them.\n\nusing Optim\nmin_objective(x) = -log_likelihood(D,X,x) #&lt;- Optim assumes that we will minimize a function, hence the negative\nγ_guess = zeros(2)\nprintln(\" ---- Using Newton's Method ------ \")\nres1 = optimize(min_objective,γ_guess,Newton(),autodiff=:forward,Optim.Options(show_trace=true))\nprintln(\" ---- Using BFGS ------ \")\nres2 = optimize(min_objective,γ_guess,BFGS(),autodiff=:forward,Optim.Options(show_trace=true))\n[res1.minimizer res2.minimizer γ]\n\n ---- Using Newton's Method ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.429516e+02\n * time: 0.012720108032226562\n     1     4.873710e+02     1.564794e+02\n * time: 0.43104004859924316\n     2     4.693793e+02     9.133735e+00\n * time: 0.4315500259399414\n     3     4.693308e+02     2.431698e-03\n * time: 0.4318211078643799\n     4     4.693308e+02     4.359872e-09\n * time: 0.43204617500305176\n ---- Using BFGS ------ \nIter     Function value   Gradient norm \n     0     6.931472e+02     9.429516e+02\n * time: 7.987022399902344e-5\n     1     4.849950e+02     1.434033e+02\n * time: 0.00822591781616211\n     2     4.825821e+02     1.387009e+02\n * time: 0.00848388671875\n     3     4.696142e+02     2.158030e+01\n * time: 0.008702993392944336\n     4     4.693308e+02     1.473504e-01\n * time: 0.008884906768798828\n     5     4.693308e+02     7.674024e-03\n * time: 0.0090789794921875\n     6     4.693308e+02     3.727930e-07\n * time: 0.009229898452758789\n     7     4.693308e+02     8.768958e-13\n * time: 0.009376049041748047\n\n\n2×3 Matrix{Float64}:\n 0.129586  0.129586  0.1\n 0.56237   0.56237   0.5",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#further-reading",
    "href": "autodiff.html#further-reading",
    "title": "Appendix B — Automatic Differentiation",
    "section": "B.7 Further Reading",
    "text": "B.7 Further Reading\n\nJuliaDiff documentation\nForwardDiff.jl docs\nEnzyme.jl docs",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Appendix C — Performance Tips",
    "section": "",
    "text": "C.1 The Golden Rules",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "performance.html#the-golden-rules",
    "href": "performance.html#the-golden-rules",
    "title": "Appendix C — Performance Tips",
    "section": "",
    "text": "C.1.1 1. Avoid Global Variables\nGlobal variables with non-constant types force the compiler to generate slow, generic code.\n# Bad\ndata = [1.0, 2.0, 3.0]\nf() = sum(data)  # `data` could change type\n\n# Good: use const\nconst DATA = [1.0, 2.0, 3.0]\nf() = sum(DATA)\n\n# Good: pass as argument\nf(data) = sum(data)\n\n\nC.1.2 2. Write Type-Stable Functions\nA function is type-stable if the output type can be inferred from the input types. Type instability forces runtime dispatch.\n# Bad: returns Int or Float64 depending on value\nfunction unstable(x)\n    if x &gt; 0\n        return 1\n    else\n        return 0.0\n    end\nend\n\n# Good: consistent return type\nfunction stable(x)\n    if x &gt; 0\n        return 1.0\n    else\n        return 0.0\n    end\nend\nUse @code_warntype to check for type instabilities (look for red Any or Union types).\n\n\nC.1.3 3. Pre-allocate Arrays\nAvoid creating arrays inside loops. Pre-allocate and use in-place operations.\n# Bad: allocates on every iteration\nfunction bad_example(n)\n    result = 0.0\n    for i in 1:n\n        v = zeros(100)  # allocation!\n        v .= rand(100)\n        result += sum(v)\n    end\n    result\nend\n\n# Good: pre-allocate\nfunction good_example(n)\n    result = 0.0\n    v = zeros(100)\n    for i in 1:n\n        rand!(v)  # in-place\n        result += sum(v)\n    end\n    result\nend\n\n\nC.1.4 4. Use @views for Array Slices\nArray slices create copies by default. Use @views or view() to avoid allocation.\nA = rand(1000, 1000)\n\n# Bad: creates a copy\nf(A[1:100, :])\n\n# Good: creates a view\nf(@views A[1:100, :])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "performance.html#quick-profiling",
    "href": "performance.html#quick-profiling",
    "title": "Appendix C — Performance Tips",
    "section": "C.2 Quick Profiling",
    "text": "C.2 Quick Profiling\nUse @time for basic timing (run twice—first call includes compilation):\n@time my_function(args)  # compile\n@time my_function(args)  # actual timing\nFor more detailed analysis: - BenchmarkTools.jl: Accurate microbenchmarks with @btime - Profile (stdlib): Sampling profiler - ProfileView.jl: Flame graph visualization",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  },
  {
    "objectID": "performance.html#further-resources",
    "href": "performance.html#further-resources",
    "title": "Appendix C — Performance Tips",
    "section": "C.3 Further Resources",
    "text": "C.3 Further Resources\n\nJulia Performance Tips — the official guide, essential reading\nJulia Academy performance course — free video tutorials\nBenchmarkTools.jl documentation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Performance Tips</span>"
    ]
  }
]