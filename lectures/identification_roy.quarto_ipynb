{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Generalized Roy Model\n",
        "\n",
        "## Selection in General\n",
        "\n",
        "Let us review the empirical content of this model without placing further restrictions on the functional forms. Let's start with the selection equation. Let $P(X,Z) = P[D=1|X,Z]$. For any distribution of $V$, since $F_{V}$ is monotonically increasing (under support conditions on $V$), we can write:\n",
        "$$ D = \\mathbf{1}\\{\\mu_{d}(X,Z) \\geq V\\} = \\mathbf{1}\\{F_{V}(\\mu_{d}(X,Z)) \\geq F_{V}(V)\\}.$$\n",
        "Since $F_{V}(V)$ is a uniform random variable in $[0,1]$, we can always write the selection equation without loss of generality as:\n",
        "\n",
        "$$ D = \\mathbf{1}\\{P(X,Z) - V \\geq 0\\},\\qquad V\\sim U[0,1] $$\n",
        "\n",
        "Now consider the conditional expectations:\n",
        "$$ \\mathbb{E}[Y|X,Z,D=1] = \\mu_{1}(X) + \\underbrace{\\mathbb{E}[U_1 | V \\leq P(X,Z)]}_{h_{1}(P(X,Z))} $$\n",
        "$$ \\mathbb{E}[Y|X,Z,D=0] = \\mu_{0}(X) + \\underbrace{\\mathbb{E}[U_0 | V > P(X,Z)]}_{h_{0}(P(X,Z))} $$\n",
        "\n",
        "Two observations follow:\n",
        "\n",
        "1. These equations illustrate the classic selection problem. If the unobservable that determines $D$ is related to the unobservables that determine the potential outcomes $(Y_0,Y_1)$, then the difference in conditional means is partly contaminated with this *selection effect*. \n",
        "2. The selection model implies *dimension reduction* in the conditional expectation of each $U_{D}$ given $X$ and $Z$: the combined propensity $P(X,Z)$ encodes all the relevant information to control for selection. This is a useful property, but the underlying index model is *not* without loss of generality. We will explore a little more below.\n",
        "\n",
        "\n",
        "## Identification by Functional Form\n",
        "\n",
        "We'll begin by considering identification under some additional functional form restrictions.\n",
        "First, consider the example where the triple $(V,U_0,U_1)$ are jointly normally distributed:\n",
        "\n",
        "$$ \n",
        "\\left[\\begin{array}{c}\n",
        "V \\\\ U_0 \\\\ U_1\n",
        "\\end{array}\\right] = \\mathcal{N}\\left(\\mathbf{0},\\left[\n",
        "    \\begin{array}{ccc}\n",
        "    1 & \\sigma_{V0} & \\sigma_{V1} \\\\\n",
        "    \\sigma_{V0} & \\sigma^2_{0} & \\sigma_{01} \\\\\n",
        "    \\sigma_{V1} & \\sigma_{01} & \\sigma^2_{1}\n",
        "    \\end{array}\\right]\n",
        "    \\right) \n",
        "$$\n",
        "\n",
        "where we have normalized the location of the unobservables by setting the mean to zero, and normalized the scale of $V$ by assuming a unit variance. Additionally, let us specify that each $\\mu_{D}$ is linear in $X$:\n",
        "$$ \\mu_{D}(X) = X\\beta_{D} $$\n",
        "Assume that our data is a single cross-section of observations $(Y_{D},D,X)$. Identification is a statement about population values, so can we take as given that we see the joint distribution $\\mathbb{P}_{Y_{D},D,X}$.\n",
        "\n",
        "**Step 1** Note that the distribution of $D$ given $X$ is a probit model:\n",
        "$$ P[D=1|X] = \\Phi(\\mu_{d}(X)) $$\n",
        "where $\\Phi$ is the cdf of the standard normal. Thus $\\mu_{d}(X)$ is identified as $\\mu_{d}(X) = \\Phi^{-1}(P[D=1|X])$ for any $X$. If we additionally impose that $\\mu_{d}(X) = X\\gamma$, then identification of each $\\gamma$ follows from the usual assumption that $X$ is full-rank with positive probability (just like OLS).\n",
        "\n",
        "**Step 2** Now consider the identification of each $\\beta_{D}$. We have:\n",
        "\n",
        "\\begin{align} \n",
        "\\mathbb{E}[Y_{1}|X,D=1] &= X\\beta_{1} + \\mathbb{E}[U_1 | V<\\mu_{d}(X)] \\\\\n",
        "&= X\\beta_{1} - \\sigma_{V1}\\frac{\\phi(\\mu_{d}(X))}{\\Phi(\\mu_{d}(X))}\n",
        "\\end{align}\n",
        "and similarly:\n",
        "$$\n",
        "\\mathbb{E}[Y_{0}|X,D=0] = X\\beta_1 + \\sigma_{V0}\\frac{\\phi(\\mu_{d}(X))}{1-\\Phi(\\mu_{d}(X))}\n",
        "$$\n",
        "Under the same rank conditions for $X$, both $\\beta_0$ and $\\beta_1$ are identified due to. This also means that $ATE(X) = X(\\beta_1 - \\beta_0)$ is identified for all $X$.\n",
        "\n",
        "Although we cannot identify the full distribution of treatment effects, this also gives us the average treatment effect among individuals with treatment propensity $V$:\n",
        "$$\\mathbb{E}[Y_0-Y_0|X,V] = X(\\beta_1-\\beta_0) + \\underbrace{(\\sigma_{V1}-\\sigma_{V0})V}_{\\mathbb{E}[U_1-U_0|V]}$$\n",
        "We'll come back to this object in the next section.\n",
        "\n",
        "Notice that identification holds here even *without* an excluded variable $Z$. It follows from the assumption of linearity and normality of the error terms, which yield a particular parametric decomposition of the conditional expectation that can be identified. \n",
        "\n",
        "This is an example of **identification by functional form**: identification depends crucially on these particular functional form assumptions. Notice that without linearity in $\\mu_{d}$, it is not possible to separately identify $\\mu_{d}$ from the selection correction. Would you say that this is a very credible approach to identifying the key causal parameters in the model? @Lewbel provides a broad and satisfying discussion of these issues.\n",
        "\n",
        "## Identification with Exclusion Restrictions\n",
        "\n",
        "If we have access to an excluded regressor, we do not have to rely so much on seemingly arbitrary functional form restrictions. Formally, we make two assumptions:\n",
        "\n",
        "1. $\\mu_{D}(X,Z)=\\mu_{D}(X)$ almost everywhere (exclusion)\n",
        "2. $ Z\\perp (V,U_0,U_1) | X $ (independence)\n",
        "\n",
        "This implies that we can write:\n",
        "$$\\mathbb{E}[U_1|X,Z,D=1] = \\mathbb{E}[U_1|V\\leq P(X,Z)] $$\n",
        "and similarly for $\\mathbb{E}[Y|X,Z,D=0]$. So the expectation of $Y$ (unconditional on $D$) is:\n",
        "\\begin{align}\n",
        "\\mathbb{E}[Y|X,P(X,Z)=p] &= \\mu_{0}(X) + p[\\mu_{1}(X)-\\mu_{0}(X)] + p\\mathbb{E}[(U_{1}-U_{0})|V\\leq P(X,Z)] \\\\\n",
        "& \\mu_{0}(X) + P(X,Z)[\\mu_{1}(X)-\\mu_{0}(X)] + \\int_{0}^{p}\\mathbb{E}[(U_1 - U_0)|V=u]du\n",
        "\\end{align}\n",
        "\n",
        "Taking a derivative with respect to $p$ gives:\n",
        "\\begin{align}\n",
        "\\frac{\\partial \\mathbb{E}[Y|X,P(X,Z)=p]}{\\partial p} &= \\mu_{1}(X)-\\mu_{0}(X) + \\mathbb{E}[U_1 - U_0 | V = p] \\\\\n",
        " & \\mathbb{E}[Y_1 - Y_0 | V = p] \\\\\n",
        " & MTE(p)\n",
        "\\end{align}\n",
        "\n",
        "@HeckmanVytlacil2005 call this derivative *local instrumental variables* approach and define the estimand, $MTE(p)$ as the **marginal treatment effect**. It is the average treatment effect among individuals with a propensity $1-p$ to take the treatment. They show that, commonly used estimators (instrumental variables, difference-in-differences) are often weighted averages of this marginal treatment effect, and that the marginal treatment effect is a useful building block for.\n",
        "\n",
        "Note that under support conditions on $Z$, as the support of $P(X,Z)$ approaches the unit interval $[0,1]$, the average treatment effect $ATE(X) = \\mu_{1}(X)-\\mu_{0}(X)$ is also identified. \n",
        "\n",
        "**Credibility** Whether or not you think that this approach to identifying treatment effects is more or less credible than using functional form restrictions may depend on how valid you think the exclusion and independence restrictions are. However, assuming that we have access to a \"good\" instrument, it should be clear that this approach is preferable.\n",
        "\n",
        "In practice, even when we have access to an instrument, the data requirements for fully non-parametric estimation can be overly demanding. A reasonable compromise is to illustrate the conditions under which your instrument provides nonparametric identification, then introduce parametric assumptions that can interpolate this plausible variation. This is a way to establish that identification of your parameters of interest in not purely driven by functional form restrictions. See @Cunha2010 and @Carneiro2011 for two examples of this kind of approach.\n",
        "\n",
        "::: {.callout-note icon=\"false\" collapse=\"true\"}\n",
        "## Example: Parametric and Semiparametric Estimation\n",
        "::: {#exm-roy_estimation}\n",
        "Let's compare estimation methods for the generalized roy model. Our estimation approach will follow the identification argument: we'll estimate the probit model and then run a selection corrected regression to recover the conditional means of the potential outcomes. Let's assume linearity:\n",
        "$$ Y_D = \\beta_{D,1} + \\beta_{D,2}X + U_D $$\n",
        "$$ D = \\mathbf{1}\\{\\gamma_1 + \\gamma_2 X + \\gamma_3 Z - V \\geq 0 \\} $$\n",
        "Here is a function to simulate the data, a function to calculate the log-likelihood, and a function to estimate $\\gamma$ by maximizing the log-likelihood."
      ],
      "id": "da74fde0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Distributions, Optim, Random, Plots\n",
        "function sim_data(γ,β0,β1,N ; ρ_0 = 0.3, ρ_1 = -0.3)\n",
        "    X = rand(Normal(),N)\n",
        "    Z = rand(Normal(),N)\n",
        "    v = rand(Normal(),N)\n",
        "    U0 = rand(Normal(),N) .+ ρ_0.*v\n",
        "    U1 = rand(Normal(),N) .+ ρ_1.*v\n",
        "    D = (γ[1] .+ γ[2]*X .+ γ[3]*Z .- v) .> 0\n",
        "    Y = β0[1] .+ β0[2].*X .+ U0\n",
        "    Y1 = β1[1] .+ β1[2].*X .+ U1\n",
        "    Y[D.==1] .= Y1[D.==1]\n",
        "    return (;X,Z,Y,D)\n",
        "end\n",
        "\n",
        "function log_likelihood(γ,data)\n",
        "    (;D,X,Z) = data\n",
        "    ll = 0.\n",
        "    Fv = Normal()\n",
        "    for n in eachindex(D)\n",
        "        xg = γ[1] + γ[2]*X[n] + γ[3]*Z[n]\n",
        "        if D[n] == 1\n",
        "            ll += log(cdf(Fv,xg))\n",
        "        else\n",
        "            ll += log(1-cdf(Fv,xg))\n",
        "        end\n",
        "    end\n",
        "    return ll\n",
        "end\n",
        "\n",
        "function estimate_probit(data)\n",
        "    res = optimize(x->-log_likelihood(x,data),zeros(3),Newton(),autodiff=:forward)\n",
        "    return res.minimizer\n",
        "end"
      ],
      "id": "852443b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we move on to estimating the other parameters, let's run a quick Monte-Carlo to make sure we did everything correctly.\n"
      ],
      "id": "bb433395"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gamma = [0., 0.5, 0.5]\n",
        "beta0 = [0., 0.3]\n",
        "beta1 = [0., 0.5]\n",
        "gamma_boot = [estimate_probit(sim_data(gamma,beta0,beta1,500))[2] for b in 1:500]\n",
        "histogram(gamma_boot,label = false)\n",
        "#plot!([0.5,0.5],[0.,10.],label=\"true value\")"
      ],
      "id": "10303059",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! Now let's write code to estimate the $\\beta$ parameters via a selection correction. We'll try a non-parametric and a parametric selection correction. The nonparametric selection uses that\n",
        "\n",
        "$$ \\mathbb{E}[Y_{D}|X,P(X,Z)=p] = X\\beta_D + K(p) $$\n",
        "\n",
        "and estimates the correction using a third order polynomial in $p$.\n"
      ],
      "id": "d71892bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "function estimate_parametric(data,γ)\n",
        "    (;Y,X,Z,D) = data\n",
        "    Fv = Normal()\n",
        "    select_index = γ[1] .+ γ[2].*X .+ γ[3].*Z\n",
        "    correction0 = pdf.(Normal(),select_index) ./ (1 .- cdf.(Normal(),select_index))\n",
        "    correction1 = pdf.(Normal(),select_index) ./ cdf.(Normal(),select_index)\n",
        "    Y0 = Y[D.==0]\n",
        "    X0 = [ones(length(Y0)) X[D.==0] correction0[D.==0]]\n",
        "    β0 = inv(X0' * X0) * (X0' * Y0)\n",
        "    Y1 = Y[D.==1]\n",
        "    X1 = [ones(length(Y1)) X[D.==1] correction1[D.==1]]\n",
        "    β1 = inv(X1' * X1) * (X1' * Y1)\n",
        "    return β0, β1\n",
        "end\n",
        "\n",
        "function estimate_semiparametric(data,γ)\n",
        "    (;Y,X,Z,D) = data\n",
        "    Fv = Normal()\n",
        "    select_index = γ[1] .+ γ[2].*X .+ γ[3].*Z\n",
        "    P = cdf.(Normal(),select_index)\n",
        "    P0 = P[D.==0]\n",
        "    P1 = P[D.==1]\n",
        "\n",
        "    Y0 = Y[D.==0]\n",
        "    X0 = [ones(length(Y0)) X[D.==0] P0 P0.^2 P0.^3]\n",
        "    β0 = inv(X0' * X0) * (X0' * Y0)\n",
        "    Y1 = Y[D.==1]\n",
        "    X1 = [ones(length(Y1)) X[D.==1] P1 P1.^2 P1.^3]\n",
        "    β1 = inv(X1' * X1) * (X1' * Y1)\n",
        "    return β0, β1\n",
        "end"
      ],
      "id": "09581ccc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's finish off with a Monte Carlo to see how each estimator performs.\n"
      ],
      "id": "c24fa0ad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ests = mapreduce(vcat,1:500) do b\n",
        "    data = sim_data([0.,1.,1.],[0.,0.3],[0.,0.5],500)\n",
        "    gamma = estimate_probit(data)\n",
        "    B0, B1 = estimate_parametric(data,gamma)\n",
        "    C0, C1 = estimate_semiparametric(data,gamma)\n",
        "    return [B0[2] B1[2] C0[2] C1[2]]\n",
        "end \n",
        "\n",
        "pl = plot((histogram(ests[:,j],label=false) for j in 1:4)...)\n",
        "plot!(pl[1],title = \"β0[2] - Parametric\")\n",
        "plot!(pl[1],[0.3,0.3],[0,150],color=\"red\",label=\"True Value\")\n",
        "plot!(pl[2],title = \"β1[2] - Parametric\")\n",
        "plot!(pl[2],[0.5,0.5],[0,150],color=\"red\",label=\"True Value\")\n",
        "plot!(pl[3],title = \"β0[2] - Semi-Parametric\")\n",
        "plot!(pl[3],[0.3,0.3],[0,150],color=\"red\",label=\"True Value\")\n",
        "plot!(pl[4],title = \"β1[2] - Semi-Parametric\")\n",
        "plot!(pl[4],[0.5,0.5],[0,150],color=\"red\",label=\"True Value\")"
      ],
      "id": "e5b65726",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.callout-tip icon=\"false\"}\n",
        "## Exercise\n",
        "::: {#exr-id_functional_form}\n",
        "Using @exm-roy_estimation, try setting $\\gamma_{3} = 0$ and verify that identification by functional form works in practice: the model can still be estimated using the parametric selection correction.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.callout-tip icon=\"false\"}\n",
        "## Exercise\n",
        "\n",
        "::: {#exr-endog_selection}\n",
        "Consider the following problem that features endogeneity *and* selection. Let $W$ be wages, let $X$ be an endogenous variable of interest, and let $Z$ be an instrument for $X$. Wages are *selected* because we only observe then when the individual chooses to work ($H=1$):\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\log(W) = \\alpha_0 + \\alpha_1 X + \\epsilon + \\rho\\eta \\\\\n",
        "X = \\gamma_0 + \\gamma_1 Z + \\eta \\\\\n",
        "H = \\mathbf{1}\\{\\beta_0 + \\beta_1\\log(W) + \\zeta >0\\}\n",
        "\\end{eqnarray}\n",
        "\n",
        "Here is a function that simulates these data for a fixed set of parameter values:\n"
      ],
      "id": "de3f7aad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Distributions, Random\n",
        "function sim_data(N)\n",
        "    α = [1., 0.5]\n",
        "    γ = [1., 0.5]\n",
        "    ρ = 0.4\n",
        "    β = [-0.25, 0.5]\n",
        "    ϵ = rand(Normal(),N)\n",
        "    η = rand(Normal(),N)\n",
        "    ζ = rand(Normal(),N)\n",
        "    Z = rand(Normal(),N)\n",
        "    X = γ[1] .+ γ[2]*Z .+ η\n",
        "    logW = α[1] .+ α[2]*X .+ ρ*η .+ ϵ\n",
        "    H = (β[1] .+ β[2]*logW .+ ζ) .> 0\n",
        "    logW[H.==0] .= -1 #<- set to missing if H=0\n",
        "    return (;logW,X,Z,H)\n",
        "end"
      ],
      "id": "bb6412e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose that you decide to handle the endogeneity problem by estimating $\\alpha_1$ with 2SLS. Below is a monte-carlo simulation of the performance of this estimator:\n"
      ],
      "id": "be9ea744"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Plots, StatsBase\n",
        "function monte_carlo_trial(N)\n",
        "    data = sim_data(N)\n",
        "    # pull out non-missing data\n",
        "    W = data.logW[data.H.==1]\n",
        "    Z = data.Z[data.H.==1]\n",
        "    X = data.X[data.H.==1]\n",
        "    # run 2SLS\n",
        "    alpha_est = cov(W,Z) / cov(X,Z)\n",
        "    return alpha_est\n",
        "end\n",
        "# run 500 trials\n",
        "alpha_boot = [monte_carlo_trial(10_000) for b in 1:500]\n",
        "histogram(alpha_boot,normalize=:probability,label=false)\n",
        "plot!([0.5,0.5],[0.,0.2],color=\"red\",label = \"True Value\")\n",
        "xlabel!(\"2SLS Estimate\")"
      ],
      "id": "56cec652",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Explain why this estimator appears to be showing asymptotic bias.\n",
        "2. Assume that $\\zeta\\sim\\mathcal{N}(0,1)$ and $\\epsilon\\sim\\mathcal(0,\\sigma^2_\\epsilon)$. Show that the *reduced form* for this model ($H$ and $W$ written in terms of exogenous variables) can be written as:\n",
        "$$ \\log(W) = A_0 + A_1z + A_2\\eta + \\xi_1 $$\n",
        "$$ H = \\mathbf{1}\\{B_0 + B_1z + B_2\\eta + \\xi_2 >0\\} $$\n",
        "$$ X = \\gamma_0 + \\gamma_1 z + \\eta $$\n",
        "where $[\\xi_1,\\ \\xi_2]$ is a bivariate normal with non-zero covariance and the coefficients $A$ and $B$ are combinations of underlying structural parameters.\n",
        "3. Show that the parameters of the reduced form are identified (hint: start by identifying $[\\gamma_0,\\gamma_1]$ and then inverting $\\eta$ from $Z$ and $X$, treating it as observable).\n",
        "4. Show that the structural parameters can be identified from this reduced form.\n",
        "5. Does this approach to identification work if we do not make these functional form assumptions? In other words, could we *nonparametrically* identify the reduced form parameters of the model?\n",
        "6. Introduce a variable to this model that would allow you to nonparametrically estimate the reduced form once again (hint: think back to what allows this in the Generalized Roy Model).\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## Monotonicity and Potential Outcomes\n",
        "\n",
        "Recall that the generalized roy model embeds the **potential outcomes framework** (this is the model you get if you throw away the selection equation). @ImbensAngrist consider what can be estimated from two stage least squares when you have access to an instrument $Z$ and heterogeneous potential outcomes. To formalize their setup, assume that individuals can be indexed in some measurable space $\\omega\\in\\Omega$, so that we write the model as a triple: $(Y_1(\\omega),Y_0(\\omega),D_{Z}(\\omega))$ where $D_{Z}(\\omega)\\in\\{0,1\\}$ indicates the choice of an individual of type $\\omega$ when the instrument takes a value $Z$. \n",
        "\n",
        "They combine four assumptions:\n",
        "\n",
        "1. Exclusion: $Y_{D}(\\omega,Z) = Y_{D}(\\omega)$ for $D\\in\\{0,1\\}$ \n",
        "2. Independence: $Z \\indep \\omega $ \n",
        "3. **Monotonicity**: For any pair $(z,z')$ in the support of $Z$, either $D_{Z}(\\omega)\\geq D_{Z'}(\\omega)$ for all $\\omega\\in\\Omega$ or $D_{Z}(\\omega)\\leq D_{Z'}(\\omega)$ for all $\\omega\\in\\Omega$.\n",
        "4. Relevance: $P[\\omega: D_{1}(\\omega) \\neq D_{0}(\\omega)]>0$.\n",
        "\n",
        "Monotonicity is easiest to understand when $Z\\in\\{0,1\\}$. In this case one can partition $\\Omega$ into \n",
        "\n",
        "1. Always takers: $D_{1}(\\omega)=D_{0}(\\omega)=1$\n",
        "2. Never takers: $D_{1}(\\omega)=D_{0}(\\omega)=0$\n",
        "3. Compliers: $D_{1}(\\omega)=1$, $D_{0}(\\omega)=0$\n",
        "4. Defiers: $D_{0}(\\omega) = 0$, $D_{1}(\\omega)=1$\n",
        "\n",
        "In this case, monotonicity rules out (without loss of generality) the existence of defiers.\n",
        "\n",
        "::: {.callout-tip icon=\"false\"}\n",
        "## Exercise\n",
        "\n",
        "::: {#exr-LATE}\n",
        "Under these assumptions, show that the 2SLS estimand:\n",
        "$$\n",
        "\\frac{\\mathbb{E}[Y|Z=1]-\\mathbb{E}[Y|Z=0]}{P[D=1|Z=1]-P[D=1|Z=0]}\n",
        "$$\n",
        "is equal to the average treatment effect among compliers: $\\mathbb{E}[Y_1(\\omega)-Y_0(\\omega)|\\omega \\in\\text{Compliers}]$.\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "@ImbensAngrist call this object the *Local Average Treatment Effect*. They show that for multi-valued instruments, 2SLS produces a weighted average of these LATES for difference complier groups.\n",
        "\n",
        "This interpretation of IV estimators is immensely widely used, so you should know the definition and the assumptions under which it works.\n",
        "\n",
        "### Relationship to the Generalized Roy Model\n",
        "\n",
        "There is a deeper relationship between the monotonicity assumption and the Generalized Roy Model. @Vytlacil2002 shows that the monotonicity assumption is **equivalent** to a latent index model representation. One direction of this proof is easy, since it is simple to verify that the latent index model:\n",
        "$$D = \\mathbf{1}\\{P(Z)-V \\geq 0\\}$$\n",
        "obeys monotonicity. Going the other way, it is relatively intuitive to see that one can construct a mapping from $\\Omega$ to $[0,1]$ such that:\n",
        "$$ D_{Z}(\\omega) = \\mathbf{1}\\{P(Z)-V(\\omega) \\geq 0\\}. $$\n",
        "\n",
        "\n",
        "::: {.callout-tip icon=\"false\"}\n",
        "## Exercise\n",
        "\n",
        "::: {#exr-roy-late}\n",
        "Use the Generalized Roy Model to show that for some binary instrument $Z\\in\\{0,1\\}$, the 2SLS estimand is equal to:\n",
        "$$\\int_{P(Z=0)}^{P(Z=1)}MTE(u)du $$\n",
        ":::\n",
        "\n",
        ":::\n"
      ],
      "id": "762de49c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.11",
      "language": "julia",
      "display_name": "Julia 1.11.3",
      "path": "/Users/mullinsj/Library/Jupyter/kernels/julia-1.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}