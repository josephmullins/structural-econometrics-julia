{
  "hash": "f389101672dff57ff3abd2686c4df7b0",
  "result": {
    "engine": "jupyter",
    "markdown": "# Introducing the Estimators\n\nBefore diving into statistical theory, we will introduce each estimator and sketch out applications by proposing estimation methods for each of our prototype models. \n\nThe three workhorse methods are:\n\n1. Maximum Likelihood;\n2. The Generalized Method of Moments; and\n3. Minimum Distance.\n\nEach of these approaches is an **extremum estimator**: any estimator that can be characterized as the solution to a maximization or minimization problem.\n\n:::{#def-extremum}\n$\\hat{\\theta}$ is an extremum estimator if:\n\n$$ \\hat{\\theta} = \\arg\\max_{\\theta\\in\\Theta} Q_{N}(\\theta) $$\nwhere $\\Theta\\subset\\mathbb{R}^{p}$.\n:::\n\nJust to clarify where we are going, it helps to reiterate what the key properties are that we would like to establish for each approach.\n\n:::{.callout-important}\n## Key Properties of Estimators\n\nThe key theoretical questions we want to establish for each estimation approach described below are:\n\n1. [**Consistency**] Does our estimate approach the \"true\" parameters of the data generating process as we collect more data?\n2. [**Inference**] How is our estimate distributed around the true parameters? How uncertain are we about our key calculations of interest and can we place reasonable bounds on the correct answer?\n\n:::\n\n## The Generalized Roy Model\n\nOur identification argument suggested a **two-step** estimator for the Generalized Roy Model: \n\n1. Estimate the selection equation by *maximum likelihood*.\n2. Estimate the outcome equations with *OLS* using a *selection correction*. \n\nThis model will give us a little more to chew on when we think about the properties of two-step estimators.\n\n## The Search Model\n\nFor this example, let's assume that we observe wages with some small amount of *known* measurement error:\n\n$$ \\log(W^{o}_{n,t}) = \\log(W_{n,t}) + \\zeta_{n,t}$$\n\nwhere $\\zeta_{n,t} \\sim \\mathcal{N}(0,\\sigma^2_\\zeta)$ and $\\sigma_\\zeta = 0.05$. \n\nRecall that without further variation, we must make a parametric assumption on the wage distribution, and so we assume that $W$ is log-normally distributed with mean $\\mu$ and variance $\\sigma^2_{W}$.\n\nOur strategy here is to estimate the parameters\n\n$$ \\theta = (\\mu,\\sigma^2_{W},h,\\delta,w^*) $$\n\nand invert out $\\lambda$ and $b$ (the latter using the reservation wage equation). Let $X_{n} = (W^o,t_u,E)$ indicate the data. The log-likelihood of a single observation is:\n\n$$ l(X;\\theta) = E \\times \\int f_{W|W>w^*}(\\log(W^{o})-\\zeta)\\phi(\\zeta;\\sigma_\\zeta)d\\zeta + (1-E)\\times[\\log(h) + t_u\\log(1-h)]  $$\n\nwhere, according to our parametric specifications:\n\n$$ f_{W|W>w^*}(w) = \\frac{\\phi(w;\\sigma_{W})}{1-\\Phi(w^*/\\sigma_{W})}.$$\n\n$\\phi(\\cdot;\\sigma)$ is the pdf of a normal with standard deviation $\\sigma$ and $\\Phi$ is the cdf of a standard normal.\n\nThe maximum likelihood estimator is:\n\n$$ \\hat{\\theta} = \\arg\\max_\\theta \\frac{1}{N}\\sum_{n}l(X_n;\\theta) $$\nwhile the MLE estimates of $\\lambda$ and $b$ are:\n\n$$ \\hat{\\lambda} = \\hat{h} / (1 - \\widehat{F}_{W|W>w^*}(\\hat{w}^*) $$\n\n$$ \\hat{b} = w^* - \\frac{\\hat{\\lambda}}{1 - \\beta(1-\\hat{\\delta})}\\int_{\\hat{w}^*}(1-\\widehat{F}_{W|W>w^*}(w))dw $$\n\nWhen we get to the theory we will consider the asymptotic properties of not just $\\hat{\\theta}$ but also the derived estimates $\\hat{b}$ and $\\hat{\\lambda}$. \n\n:::{.callout-note}\n## Example: Coding the Log-Likelihood\n\n:::{#exm-search_likelihood}\n\nFirst, let's load the routines that we previously wrote to solve the model and take numerical integrals with quadrature. These are identical to what we've seen before and are available on the course github.\n\n::: {#3849246e .cell execution_count=1}\n``` {.julia .cell-code}\ninclude(\"../scripts/search_model.jl\")\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nsolve_res_wage (generic function with 1 method)\n```\n:::\n:::\n\n\nBefore writing the likelihood, let's  load the data, clean, and create the data frame. \n\n::: {#eaf0e756 .cell execution_count=2}\n``` {.julia .cell-code}\nusing CSV, DataFrames, DataFramesMeta, Statistics\n\ndata = CSV.read(\"../data/cps_00019.csv\",DataFrame)\ndata = @chain data begin\n    @transform :E = :EMPSTAT.<21\n    @transform @byrow :wage = begin\n        if :PAIDHOUR==0\n            return missing\n        elseif :PAIDHOUR==2\n            if :HOURWAGE<99.99 && :HOURWAGE>0\n                return :HOURWAGE\n            else\n                return missing\n            end\n        elseif :PAIDHOUR==1\n            if :EARNWEEK>0 && :UHRSWORKT<997 && :UHRSWORKT>0\n                return :EARNWEEK / :UHRSWORKT\n            else\n                return missing\n            end\n        end\n    end\n    @subset :MONTH.==1\n    @select :AGE :SEX :RACE :EDUC :wage :E :DURUNEMP\n    @transform begin\n        :bachelors = :EDUC.>=111\n        :nonwhite = :RACE.!=100 \n        :female = :SEX.==2\n        :DURUNEMP = round.(:DURUNEMP .* 12/52)\n    end\nend\n\n# the whole dataset in a named tuple\nwage_missing = ismissing.(data.wage)\nwage = coalesce.(data.wage,1.)\nN = length(data.AGE)\n# create a named tuple with all variables to conveniently pass to the log-likelihood:\ndat = (;logwage = log.(wage),wage_missing,E = data.E,tU = data.DURUNEMP) \n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(logwage = [0.0, 0.0, 0.0, 3.0368742168851663, 2.302585092994046, 3.2188758248682006, 2.2512917986064953, 0.0, 0.0, 0.0  …  0.0, 0.0, 2.4849066497880004, 3.056356895370426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], wage_missing = Bool[1, 1, 1, 0, 0, 0, 0, 1, 1, 1  …  1, 1, 0, 0, 1, 1, 1, 1, 1, 1], E = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1], tU = [231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0  …  231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0, 231.0])\n```\n:::\n:::\n\n\nNow, let's write the log-likelihood as above.\n\n::: {#284b6d5b .cell execution_count=3}\n``` {.julia .cell-code}\nusing Distributions, Optim\n\nϕ(x,μ,σ) = pdf(Normal(μ,σ),x)\nΦ(x,μ,σ) = cdf(Normal(μ,σ),x)\n\n# a function for the log-likelihood of observed wages (integrating out measurement error)\nfunction logwage_likelihood(logwage,F,σζ,wres)\n    f(x) = pdf(F,x) / (1-cdf(F,wres)) * ϕ(logwage,log(x),σζ)\n    ub = quantile(F,0.9999)\n    return integrateGL(f,wres,ub)\nend\n\n# a function to get the log-likelihood of a single observation\n# note this function assumes that data holds vectors \n# E, tU, and logwage\nfunction log_likelihood(n,data,pars)\n    (;h,δ,wres,F,σζ) = pars\n    ll = 0.\n    if data.E[n]\n        ll += log(h) - log(h + δ)\n        if !data.wage_missing[n]\n            ll += logwage_likelihood(data.logwage[n],F,σζ,wres)\n        end\n    else\n        ll += log(δ) - log(h + δ)\n        ll += log(h) + data.tU[n] * log(1-h)\n    end\n    return ll\nend\n\n# a function to iterate over all observations\nfunction log_likelihood_obj(x,pars,data)\n    pars = update(pars,x)\n    ll = 0.\n    for n in eachindex(data.E)\n        ll += log_likelihood(n,data,pars)\n    end\n    return ll / length(data.E)\nend\n\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nlog_likelihood_obj (generic function with 1 method)\n```\n:::\n:::\n\n\nFinally, since routines like `Optim` optimize over vectors, we want to write an update routine that takes a vector `x` and maps it to new parameters. Here we are going to use transformation functions to ensure that parameters obey their bound constraints. There are other ways to ensure this, but this is one way that works.\n\n::: {#4f449535 .cell execution_count=4}\n``` {.julia .cell-code}\nlogit(x) = exp(x) / (1+exp(x))\nlogit_inv(x) = log(x/(1-x))\n\nfunction update(pars,x)\n    h = logit(x[1])\n    δ = logit(x[2])\n    μ = x[3]\n    σ = exp(x[4])\n    wres = exp(x[5])\n    F = LogNormal(μ,σ)\n    return (;pars...,h,δ,μ,σ,wres,F)\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nupdate (generic function with 1 method)\n```\n:::\n:::\n\n\nNow we can finally test our likelihood to see how it runs:\n\n::: {#babfdecc .cell execution_count=5}\n``` {.julia .cell-code}\nx0 = [logit_inv(0.5),logit_inv(0.03),2.,log(1.),log(5.)]\npars = (;σζ = 0.05, β = 0.995)\nlog_likelihood_obj(x0,pars,dat) #<- test.\nres = optimize(x->-log_likelihood_obj(x,pars,dat),x0,Newton(),Optim.Options(show_trace=true))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIter     Function value   Gradient norm \n     0     2.428210e-01     2.348428e-01\n * time: 0.011512041091918945\n     1     2.051862e-01     1.072566e-01\n * time: 2.10417103767395\n     2     1.827395e-01     4.626301e-01\n * time: 3.4270410537719727\n     3     1.758193e-01     4.441528e-01\n * time: 4.615013122558594\n     4     1.692587e-01     3.168138e-02\n * time: 5.978161096572876\n     5     1.668869e-01     1.483080e-01\n * time: 7.157219171524048\n     6     1.647839e-01     2.507384e-01\n * time: 8.517966032028198\n     7     1.638445e-01     1.648957e-01\n * time: 9.883538007736206\n     8     1.633820e-01     2.338710e-02\n * time: 11.071093082427979\n     9     1.633356e-01     2.095668e-02\n * time: 12.436002016067505\n    10     1.633224e-01     1.774668e-03\n * time: 13.805508136749268\n    11     1.633213e-01     3.628638e-03\n * time: 15.163128137588501\n    12     1.633211e-01     1.280346e-04\n * time: 16.52274513244629\n    13     1.633210e-01     3.719531e-04\n * time: 17.880887031555176\n    14     1.633210e-01     5.299485e-06\n * time: 19.24722909927368\n    15     1.633210e-01     3.008006e-05\n * time: 20.786643981933594\n    16     1.633210e-01     8.243085e-08\n * time: 22.155712127685547\n    17     1.633210e-01     6.359010e-08\n * time: 23.34080410003662\n    18     1.633210e-01     2.488281e-07\n * time: 24.704849004745483\n    19     1.633210e-01     2.588340e-08\n * time: 25.889103174209595\n    20     1.633210e-01     1.472700e-08\n * time: 27.0694420337677\n    21     1.633210e-01     2.956400e-09\n * time: 28.502158164978027\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n * Status: success (objective increased between iterations)\n\n * Candidate solution\n    Final objective value:     1.633210e-01\n\n * Found with\n    Algorithm:     Newton's Method\n\n * Convergence measures\n    |x - x'|               = 6.08e-06 ≰ 0.0e+00\n    |x - x'|/|x'|          = 2.83e-07 ≰ 0.0e+00\n    |f(x) - f(x')|         = 1.60e-13 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 9.82e-13 ≰ 0.0e+00\n    |g(x)|                 = 2.96e-09 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   29  (vs limit Inf)\n    Iterations:    21\n    f(x) calls:    66\n    ∇f(x) calls:   66\n    ∇²f(x) calls:  21\n```\n:::\n:::\n\n\nHere we tell `Optim` to make use of automatic differentiation with `ForwardDiff`. Let's take a peek at the parameter estimates:\n\n::: {#7e63d9c7 .cell execution_count=6}\n``` {.julia .cell-code}\nDataFrame(;update(pars,res.minimizer)...)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div><div style = \"float: left;\"><span>1×8 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">σζ</th><th style = \"text-align: left;\">β</th><th style = \"text-align: left;\">h</th><th style = \"text-align: left;\">δ</th><th style = \"text-align: left;\">μ</th><th style = \"text-align: left;\">σ</th><th style = \"text-align: left;\">wres</th><th style = \"text-align: left;\">F</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"LogNormal{Float64}\" style = \"text-align: left;\">LogNorma…</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">0.05</td><td style = \"text-align: right;\">0.995</td><td style = \"text-align: right;\">0.184915</td><td style = \"text-align: right;\">0.00764741</td><td style = \"text-align: right;\">3.47993</td><td style = \"text-align: right;\">1.09258</td><td style = \"text-align: right;\">4.80711e-10</td><td style = \"text-align: left;\">LogNormal{Float64}(μ=3.47993, σ=1.09258)</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n:::\n:::\n\n:::{.callout-important}\n## Performance Note\n\nNote that in @exm-search_likelihood we created a `NamedTuple` called `dat` from the data frame, which cements the type of each vector of data into `dat`.\n\nThis is important for performance! Working with `DataFrame` types directly can dramatically slow down your code because the columns of these data frames are not typed concretely. See the [performance tips](../appendices/performance.qmd) for more discussion.\n\n:::\n\n:::{.callout-tip icon=\"false\"}\n## Exercise\n\n:::{#exr-estimation_search}\nExtent the code above to\n1. Additionally estimate the parameters $b$ and $\\lambda$.\n2. Estimate the search model separately for men with and without a bachelor's degree. \n3. Report and comment on your estimates.\n:::\n\n:::\n\n## The Labor Supply Model\n\n## The Savings Model\n\n## The Entry-Exit Model\n\n",
    "supporting": [
      "extremum_intro_examples_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}