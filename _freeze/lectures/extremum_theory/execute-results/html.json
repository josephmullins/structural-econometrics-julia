{
  "hash": "9d46372523949528442c5a6eebcfd4b8",
  "result": {
    "engine": "jupyter",
    "markdown": "# Asymptotic Theory\n\nIn the previous chapter, we introduced three classes of extremum estimators --- maximum likelihood, GMM, and minimum distance --- with examples from each of our prototype models. Now we turn to the statistical theory that governs these estimators. Recall the two key properties we set out to establish:\n\n1. **Consistency**: Does $\\hat{\\theta}\\rightarrow\\theta_{0}$ as the sample grows?\n2. **Inference**: What is the sampling distribution of $\\hat{\\theta}$ around $\\theta_{0}$?\n\nThe results in this chapter apply broadly to all extremum estimators, and then we specialize to maximum likelihood, minimum distance, and GMM in turn. Throughout, we use our prototype models to illustrate how the theory translates into practice.\n\nThe results in this section follow @newey1994large very closely, and you can find a more thorough and precise treatment of the theory in that text.\n\n## Definitions\n\nWe begin by formally defining the classes of estimators we will study. The broadest class is the **extremum estimator**.\n\n:::{#def-extremum}\n## Extremum Estimator\n$\\hat{\\theta}$ is an **extremum estimator** if:\n$$\\hat{\\theta} = \\arg\\max_{\\theta\\in\\Theta}Q_{N}(\\theta)$$\nwhere $\\Theta\\subset\\mathbb{R}^{p}$ and $Q_{N}(\\cdot)$ is some objective function that depends on the data.\n:::\n\nThis is a very broad definition. All of the estimators we encountered in the previous chapter fall into this class. What distinguishes them is the structure of $Q_{N}$.\n\n### M-estimators\n\nAn important subclass of extremum estimators arises when the objective function is an average over the sample:\n\n:::{#def-m-estimator}\n## M-estimator\n$\\hat{\\theta}$ is an **M-estimator** if:\n$$Q_{N}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N}m(\\mathbf{w}_{n},\\theta)$$\nfor some known function $m$.\n:::\n\nThe \"M\" stands for \"maximum\" (or \"minimum\"). Two of our workhorse estimators are M-estimators:\n\n- **Maximum Likelihood**: $m(\\mathbf{w}_{n},\\theta) = \\log f(y_{n}|\\mathbf{x}_{n},\\theta)$. This is the log-likelihood contribution of observation $n$.\n- **Nonlinear Least Squares**: $m(\\mathbf{w}_{n},\\theta) = -(y_{n}-\\varphi(\\mathbf{x}_{n},\\theta))^{2}$. Here $\\varphi(\\mathbf{x},\\theta)$ is a regression function and the objective penalizes deviations of $y$ from its conditional mean.\n\n### GMM Estimator\n\nThe GMM estimator is defined by a set of moment conditions $\\mathbb{E}[g(\\mathbf{w},\\theta_{0})]=\\mathbf{0}$:\n\n:::{#def-gmm}\n## GMM Estimator\n$$Q_{N}(\\theta) = -\\frac{1}{2}\\mathbf{g}_{N}(\\theta)'\\hat{\\mathbf{W}}\\mathbf{g}_{N}(\\theta),\\qquad\\mathbf{g}_{N}(\\theta)=\\frac{1}{N}\\sum_{n}g(\\mathbf{w}_{n},\\theta)$$\nwhere $\\hat{\\mathbf{W}}$ is a positive definite weighting matrix.\n:::\n\nNote that GMM is itself an M-estimator with $m(\\mathbf{w}_{n},\\theta)=-\\frac{1}{2}g(\\mathbf{w}_{n},\\theta)'\\hat{\\mathbf{W}}g(\\mathbf{w}_{n},\\theta)$ (after expanding the quadratic form). We will return to the specific properties of GMM in a [later section](#the-generalized-method-of-moments).\n\n### Minimum Distance Estimator\n\nThe minimum distance estimator works with a first-stage reduced-form estimate $\\hat{\\pi}$ and model restrictions $\\psi(\\pi,\\theta)$:\n\n:::{#def-md}\n## Minimum Distance Estimator\n$$Q_{N}(\\theta) = -\\frac{1}{2}\\psi(\\hat{\\pi}_{N},\\theta)'\\hat{\\mathbf{W}}\\psi(\\hat{\\pi}_{N},\\theta)$$\nwhere $\\psi(\\pi_{0},\\theta_{0})=\\mathbf{0}$ and $\\sqrt{N}(\\hat{\\pi}_{N}-\\pi_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Omega)$.\n:::\n\nThe minimum distance estimator differs from GMM in that the objective depends on the data only through the first-stage statistic $\\hat{\\pi}$, rather than through the individual observations directly. We will study its asymptotic properties in a [dedicated section](#minimum-distance-estimators).\n\n\n## Consistency\n\nAn extremum estimator solves $\\hat{\\theta} = \\arg\\max_{\\theta\\in\\Theta}Q_{N}(\\theta)$. Let $Q_{0}(\\theta)$ denote the *population* analogue: the probability limit of $Q_{N}(\\theta)$.\n\nWhen can we guarantee that $\\hat{\\theta}\\rightarrow_{p}\\theta_{0}$? Intuitively, two conditions are needed:\n\n1. **Identification**: The population objective $Q_{0}(\\theta)$ must be *uniquely* maximized at $\\theta_{0}$. If there were multiple maximizers, convergence of $Q_{N}$ to $Q_{0}$ would not pin down which one $\\hat{\\theta}$ approaches.\n2. **Convergence**: $Q_{N}(\\theta)$ must converge to $Q_{0}(\\theta)$ in a sufficiently strong sense that the maximizer of $Q_{N}$ tracks the maximizer of $Q_{0}$.\n\nThese two conditions are the backbone of every consistency argument. The precise form of the convergence condition depends on the structure of the problem.\n\n:::{#thm-consistency-compact}\n## Consistency with Compactness\nSuppose the following conditions hold:\n\n1. $\\Theta$ is a compact subset of $\\mathbb{R}^{p}$\n2. $Q_{N}(\\theta)$ is continuous in $\\theta$ for all realizations of the data\n3. $Q_{N}(\\theta)$ is a measurable function of the data for all $\\theta\\in\\Theta$\n\nand additionally:\n\na. **Identification**: $Q_{0}(\\theta)$ is uniquely maximized at $\\theta_{0}\\in\\Theta$\nb. **Uniform Convergence**: $\\sup_{\\theta\\in\\Theta}|Q_{N}(\\theta)-Q_{0}(\\theta)|\\rightarrow_{p}0$\n\nThen $\\hat{\\theta}\\rightarrow_{p}\\theta_{0}$.\n:::\n\n:::{.callout-note icon=\"false\" collapse=\"true\"}\n## Proof Sketch\nThe idea is straightforward. Pick any open neighborhood $\\mathcal{N}$ around $\\theta_{0}$. We want to show that $\\hat{\\theta}\\in\\mathcal{N}$ with probability approaching 1.\n\nSince $\\theta_{0}$ uniquely maximizes $Q_{0}$ and $\\Theta\\setminus\\mathcal{N}$ is compact (closed subset of a compact set), there exists a gap:\n$$\\varepsilon = Q_{0}(\\theta_{0}) - \\sup_{\\theta\\in\\Theta\\setminus\\mathcal{N}}Q_{0}(\\theta) > 0$$\n\nNow, by uniform convergence:\n$$Q_{N}(\\hat{\\theta}) \\geq Q_{N}(\\theta_{0}) \\geq Q_{0}(\\theta_{0}) - \\varepsilon/2$$\nwith probability approaching 1. At the same time, for any $\\theta\\in\\Theta\\setminus\\mathcal{N}$:\n$$Q_{N}(\\theta) \\leq Q_{0}(\\theta) + \\varepsilon/2 \\leq Q_{0}(\\theta_{0}) - \\varepsilon/2$$\nalso with probability approaching 1. Thus $\\hat{\\theta}$ cannot lie outside $\\mathcal{N}$, and since $\\mathcal{N}$ was arbitrary, $\\hat{\\theta}\\rightarrow_{p}\\theta_{0}$.\n:::\n\nCompactness is a strong assumption. Many parameter spaces of interest are not bounded (e.g. regression coefficients). The following result relaxes compactness at the cost of requiring concavity.\n\n:::{#thm-consistency-concave}\n## Consistency without Compactness\nSuppose the following conditions hold:\n\n1. $\\theta_{0}\\in\\text{int}(\\Theta)$\n2. $Q_{N}(\\theta)$ is concave in $\\theta$ for all realizations of the data\n3. $Q_{N}(\\theta)$ is a measurable function of the data for all $\\theta\\in\\Theta$\n\nand additionally:\n\na. **Identification**: $Q_{0}(\\theta)$ is uniquely maximized at $\\theta_{0}\\in\\Theta$\nb. **Pointwise Convergence**: $Q_{N}(\\theta)\\rightarrow_{p}Q_{0}(\\theta)$ for all $\\theta\\in\\Theta$\n\nThen $\\hat{\\theta}\\rightarrow_{p}\\theta_{0}$.\n:::\n\nThe key insight is that concavity turns *pointwise* convergence into *uniform* convergence on compact subsets (this follows from a result in convex analysis due to Rockafellar, 1970). Combined with the fact that $\\theta_{0}$ is an interior point, one can construct a compact set around $\\theta_{0}$ that traps $\\hat{\\theta}$ with probability approaching 1 and then apply the logic of @thm-consistency-compact.\n\n### Uniform Convergence for M-Estimators\n\nCondition (b) of @thm-consistency-compact requires *uniform* convergence of $Q_{N}$ to $Q_{0}$. This is stronger than pointwise convergence and deserves some attention. For M-estimators of the form $Q_{N}(\\theta)=\\frac{1}{N}\\sum_{n=1}^{N}m(\\mathbf{w}_{n},\\theta)$, the question reduces to asking for a *uniform law of large numbers*. The following result provides simple sufficient conditions.\n\n:::{#thm-ulln}\n## Uniform Law of Large Numbers\nSuppose that $\\{\\mathbf{w}_{n}\\}_{n=1}^{N}$ is an ergodic stationary sequence and:\n\n1. $\\Theta$ is compact\n2. $m(\\mathbf{w},\\theta)$ is continuous in $\\theta$ for all $\\mathbf{w}$\n3. $m(\\mathbf{w},\\theta)$ is measurable in $\\mathbf{w}$ for all $\\theta$\n4. There exists $d(\\mathbf{w})$ with $|m(\\mathbf{w},\\theta)|\\leq d(\\mathbf{w})$ for all $\\theta\\in\\Theta$ and $\\mathbb{E}[d(\\mathbf{w})]<\\infty$\n\nThen:\n\na. $\\sup_{\\theta\\in\\Theta}|Q_{N}(\\theta)-Q_{0}(\\theta)|\\rightarrow_{p}0$; and\nb. $Q_{0}(\\theta) = \\mathbb{E}[m(\\mathbf{w},\\theta)]$ is continuous in $\\theta$.\n:::\n\nIn practice, the dominance condition (4) is verified by checking $\\mathbb{E}[\\sup_{\\theta\\in\\Theta}|m(\\mathbf{w},\\theta)|]<\\infty$. This is straightforward for many common estimators. See @newey1994large for a comprehensive treatment of these results.\n\n### Consistency of Maximum Likelihood\n\nFor maximum likelihood, the objective is $Q_{N}(\\theta)=\\frac{1}{N}\\sum_{n}^{N}\\log f(\\mathbf{w}_{n};\\theta)$, and the population analogue is $Q_{0}(\\theta) = \\mathbb{E}_{\\theta_{0}}[\\log f(\\mathbf{w};\\theta)]$. Identification for MLE has an elegant justification through the **Kullback-Leibler inequality**: for any two densities $g$ and $h$,\n\n$$\\mathbb{E}_{g}\\left[\\log\\frac{g(\\mathbf{w})}{h(\\mathbf{w})}\\right]\\geq 0$$\n\nwith equality if and only if $g=h$ almost everywhere. Applying this with $g(\\cdot) = f(\\cdot;\\theta_{0})$ and $h(\\cdot) = f(\\cdot;\\theta)$ gives:\n\n$$\\mathbb{E}_{\\theta_{0}}[\\log f(\\mathbf{w};\\theta_{0})] \\geq \\mathbb{E}_{\\theta_{0}}[\\log f(\\mathbf{w};\\theta)]$$\n\nwith equality if and only if $f(\\cdot;\\theta)=f(\\cdot;\\theta_{0})$ almost everywhere. Thus, as long as different values of $\\theta$ imply different densities (a natural notion of identification for parametric models), the population log-likelihood is uniquely maximized at $\\theta_{0}$.\n\n:::{#thm-consistency-mle}\n## Consistency of Maximum Likelihood\nSuppose that $\\{\\mathbf{w}_{n}\\}$ is ergodic stationary with density $f(\\mathbf{w};\\theta_{0})$ and that $\\theta_{0}\\in\\Theta$. If:\n\n1. $\\Theta$ is compact\n2. $\\log f(\\mathbf{w};\\theta)$ is continuous in $\\theta$\n3. $f(\\mathbf{w};\\theta_{0})\\neq f(\\mathbf{w};\\theta)$ with positive probability for all $\\theta\\neq\\theta_{0}$ (**identification**)\n4. $\\mathbb{E}[\\sup_{\\theta\\in\\Theta}|\\log f(\\mathbf{w};\\theta)|]<\\infty$ (**dominance**)\n\nThen $\\hat{\\theta}_{ML}\\rightarrow_{p}\\theta_{0}$.\n:::\n\nNotice that identification here takes a model-specific form: we need different parameter values to imply different distributions for the data. This is a consequence of the fact that MLE relies on a fully specified parametric model.\n\nAn analogous result holds without compactness when the log-likelihood is concave in $\\theta$ (as is often the case for exponential family models), replacing (1) with $\\theta_{0}\\in\\text{int}(\\Theta)$ and (4) with pointwise moment conditions.\n\n\n## Asymptotic Normality for M-Estimators\n\nHaving established when $\\hat{\\theta}\\rightarrow_{p}\\theta_{0}$, we now turn to characterizing the *rate* and *distribution* of $\\hat{\\theta}$ around $\\theta_{0}$. The answer will justify the standard errors and confidence intervals that we routinely compute in applied work.\n\nConsider an M-estimator: $Q_{N}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N}m(\\mathbf{w}_{n},\\theta)$. Define the **score** (gradient) and **Hessian** of $m$:\n\n$$\\mathbf{s}(\\mathbf{w},\\theta) = \\frac{\\partial m(\\mathbf{w},\\theta)}{\\partial\\theta}\\qquad(p\\times 1)$$\n\n$$\\mathbf{H}(\\mathbf{w},\\theta) = \\frac{\\partial^{2}m(\\mathbf{w},\\theta)}{\\partial\\theta\\partial\\theta'}\\qquad(p\\times p)$$\n\n### Derivation via the Mean Value Theorem\n\nSince $\\hat{\\theta}$ maximizes $Q_{N}$, the first-order condition gives:\n$$\\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{s}(\\mathbf{w}_{n},\\hat{\\theta}) = \\mathbf{0}$$\n\nA mean value expansion around $\\theta_{0}$ yields:\n$$\\mathbf{0} = \\frac{1}{N}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\theta_{0}) + \\left[\\frac{1}{N}\\sum_{n}\\mathbf{H}(\\mathbf{w}_{n},\\bar{\\theta})\\right](\\hat{\\theta}-\\theta_{0})$$\n\nwhere $\\bar{\\theta}$ lies between $\\hat{\\theta}$ and $\\theta_{0}$ (applied row-by-row). Rearranging:\n$$\\sqrt{N}(\\hat{\\theta}-\\theta_{0}) = -\\left[\\frac{1}{N}\\sum_{n}\\mathbf{H}(\\mathbf{w}_{n},\\bar{\\theta})\\right]^{-1}\\frac{1}{\\sqrt{N}}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\theta_{0})$$\n\nNow apply two standard arguments:\n\n1. By the **Central Limit Theorem**: $\\frac{1}{\\sqrt{N}}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Sigma)$ where $\\Sigma = \\mathbb{E}[\\mathbf{s}(\\mathbf{w},\\theta_{0})\\mathbf{s}(\\mathbf{w},\\theta_{0})']$.\n2. By a **Law of Large Numbers** and continuity: $\\frac{1}{N}\\sum_{n}\\mathbf{H}(\\mathbf{w}_{n},\\bar{\\theta})\\rightarrow_{p}\\mathbb{E}[\\mathbf{H}(\\mathbf{w},\\theta_{0})]$, using that $\\bar{\\theta}\\rightarrow_{p}\\theta_{0}$.\n\nCombining these via Slutsky's theorem gives the result.\n\n:::{#thm-asymptotic-normality}\n## Asymptotic Normality for M-estimators\nSuppose that the consistency conditions hold and additionally:\n\n1. $\\theta_{0}\\in\\text{int}(\\Theta)$\n2. $m(\\mathbf{w},\\theta)$ is twice continuously differentiable in $\\theta$\n3. $\\frac{1}{\\sqrt{N}}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Sigma)$ with $\\Sigma$ positive definite\n4. $\\mathbb{E}[\\sup_{\\theta\\in\\mathcal{N}(\\theta_{0})}\\|\\mathbf{H}(\\mathbf{w},\\theta)\\|]<\\infty$ for some neighborhood $\\mathcal{N}(\\theta_{0})$\n5. $\\mathbb{E}[\\mathbf{H}(\\mathbf{w},\\theta_{0})]$ is nonsingular\n\nThen:\n$$\\sqrt{N}(\\hat{\\theta}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}\\left(\\mathbf{0},\\ \\mathbb{E}[\\mathbf{H}]^{-1}\\Sigma\\mathbb{E}[\\mathbf{H}]^{-1}\\right)$$\n\nwhere $\\mathbb{E}[\\mathbf{H}]=\\mathbb{E}[\\mathbf{H}(\\mathbf{w},\\theta_{0})]$ and $\\Sigma = \\mathbb{E}[\\mathbf{s}(\\mathbf{w},\\theta_{0})\\mathbf{s}(\\mathbf{w},\\theta_{0})']$.\n:::\n\nThe asymptotic variance $\\mathbb{E}[\\mathbf{H}]^{-1}\\Sigma\\mathbb{E}[\\mathbf{H}]^{-1}$ is often called the **sandwich formula**. In practice, we replace the population expectations with sample analogues:\n$$\\widehat{\\mathbb{V}}[\\hat{\\theta}] = \\hat{H}^{-1}\\hat{\\Sigma}\\hat{H}^{-1}/N$$\nwhere $\\hat{H} = \\frac{1}{N}\\sum_{n}\\mathbf{H}(\\mathbf{w}_{n},\\hat{\\theta})$ and $\\hat{\\Sigma} = \\frac{1}{N}\\sum_{n}\\mathbf{s}(\\mathbf{w}_{n},\\hat{\\theta})\\mathbf{s}(\\mathbf{w}_{n},\\hat{\\theta})'$.\n\n### The Information Matrix Equality\n\nFor maximum likelihood, $m(\\mathbf{w},\\theta)=\\log f(\\mathbf{w};\\theta)$, and a remarkable simplification occurs. Under standard regularity conditions, the **information matrix equality** holds:\n\n$$\\mathcal{I}(\\theta_{0}) \\equiv \\mathbb{E}\\left[\\mathbf{s}(\\mathbf{w},\\theta_{0})\\mathbf{s}(\\mathbf{w},\\theta_{0})'\\right] = -\\mathbb{E}\\left[\\mathbf{H}(\\mathbf{w},\\theta_{0})\\right]$$\n\n:::{.callout-note icon=\"false\" collapse=\"true\"}\n## Why does this hold?\n\nSince $\\int f(\\mathbf{w};\\theta)d\\mathbf{w}=1$ for all $\\theta$, differentiating under the integral sign with respect to $\\theta$ gives:\n$$\\int\\frac{\\partial f(\\mathbf{w};\\theta)}{\\partial\\theta}d\\mathbf{w} = 0$$\nwhich is $\\mathbb{E}_{\\theta}[\\mathbf{s}(\\mathbf{w},\\theta)]=0$. Differentiating again:\n$$\\int\\frac{\\partial^{2}\\log f}{\\partial\\theta\\partial\\theta'}f\\ d\\mathbf{w} + \\int\\frac{\\partial\\log f}{\\partial\\theta}\\frac{\\partial\\log f}{\\partial\\theta'}f\\ d\\mathbf{w} = 0$$\nwhich yields $\\mathbb{E}[\\mathbf{H}]+\\mathbb{E}[\\mathbf{s}\\mathbf{s}'] = 0$, i.e. $\\Sigma = -\\mathbb{E}[\\mathbf{H}]$.\n:::\n\nThis means that for MLE, the sandwich formula collapses to:\n$$\\sqrt{N}(\\hat{\\theta}_{ML}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}\\left(\\mathbf{0},\\ \\mathcal{I}(\\theta_{0})^{-1}\\right)$$\n\nThe matrix $\\mathcal{I}(\\theta)$ is called the **Fisher information matrix**. The MLE variance can be estimated using *either* the Hessian or the outer product of the score --- or indeed the sandwich (which is robust to certain forms of misspecification).\n\n:::{.callout-note icon=\"false\" collapse=\"true\"}\n## Example: Probit Standard Errors\n:::{#exm-probit_se}\n\nLet's illustrate the asymptotic variance formula for the probit model from the Generalized Roy Model (@exm-roy_estimation). The probit log-likelihood for a single observation is:\n\n$$m(\\mathbf{w}_{n},\\gamma) = D_{n}\\log\\Phi(\\mathbf{w}_{n}\\gamma) + (1-D_{n})\\log(1-\\Phi(\\mathbf{w}_{n}\\gamma))$$\n\nwhere $\\mathbf{w}_{n} = [1,X_{n},Z_{n}]$. Rather than deriving the Hessian and score analytically, we can use automatic differentiation --- a tool covered in more detail in the [appendix](../appendices/autodiff.qmd).\n\n::: {#3d23f89d .cell execution_count=1}\n``` {.julia .cell-code}\nusing Distributions, Optim, Random, ForwardDiff, LinearAlgebra, Plots\nfunction sim_data(γ,β0,β1,N ; ρ_0 = 0.3, ρ_1 = -0.3)\n    X = rand(Normal(),N)\n    Z = rand(Normal(),N)\n    v = rand(Normal(),N)\n    U0 = rand(Normal(),N) .+ ρ_0.*v\n    U1 = rand(Normal(),N) .+ ρ_1.*v\n    D = (γ[1] .+ γ[2]*X .+ γ[3]*Z .- v) .> 0\n    Y = β0[1] .+ β0[2].*X .+ U0\n    Y1 = β1[1] .+ β1[2].*X .+ U1\n    Y[D.==1] .= Y1[D.==1]\n    return (;X,Z,Y,D)\nend\n\nfunction log_likelihood(γ,data)\n    (;D,X,Z) = data\n    ll = 0.\n    Fv = Normal()\n    for n in eachindex(D)\n        xg = γ[1] + γ[2]*X[n] + γ[3]*Z[n]\n        if D[n] == 1\n            ll += log(cdf(Fv,xg))\n        else\n            ll += log(1-cdf(Fv,xg))\n        end\n    end\n    return ll / length(D)\nend\n\nfunction estimate_probit(data)\n    res = optimize(x->-log_likelihood(x,data),zeros(3),Newton(),autodiff=:forward)\n    return res.minimizer\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nestimate_probit (generic function with 1 method)\n```\n:::\n:::\n\n\nNow let's estimate the probit model on a single dataset and compute standard errors using the information matrix. We use `ForwardDiff` to compute the score and Hessian numerically.\n\n::: {#0359aa9b .cell execution_count=2}\n``` {.julia .cell-code}\ngamma = [0., 0.5, 0.1]\nbeta0 = [0., 0.3]\nbeta1 = [0., 0.5]\nRandom.seed!(123)\ndata = sim_data(gamma, beta0, beta1, 5000)\nγ_hat = estimate_probit(data)\n\n# Compute score for each observation\nfunction score_n(n, γ, data)\n    function ll_n(g)\n        xg = g[1] + g[2]*data.X[n] + g[3]*data.Z[n]\n        if data.D[n] == 1\n            return log(cdf(Normal(),xg))\n        else\n            return log(1-cdf(Normal(),xg))\n        end\n    end\n    return ForwardDiff.gradient(ll_n, γ)\nend\n\nN = length(data.D)\n\n# Outer product of scores (estimate of Fisher information)\nΣ_hat = zeros(3,3)\nfor n in 1:N\n    s = score_n(n, γ_hat, data)\n    Σ_hat += s * s'\nend\nΣ_hat ./= N\n\n# Hessian of average log-likelihood\nH_hat = ForwardDiff.hessian(g -> log_likelihood(g, data), γ_hat)\n\n# Asymptotic variance (three ways)\nV_sandwich = inv(H_hat) * Σ_hat * inv(H_hat) / N\nV_hessian = -inv(H_hat) / N\nV_opg = inv(Σ_hat) / N\n\nse_sandwich = sqrt.(diag(V_sandwich))\nse_hessian = sqrt.(diag(V_hessian))\nse_opg = sqrt.(diag(V_opg))\nprintln(\"Estimates: $(round.(γ_hat,digits=3))\")\nprintln(\"SE (sandwich): $(round.(se_sandwich,digits=4))\")\nprintln(\"SE (Hessian):  $(round.(se_hessian,digits=4))\")\nprintln(\"SE (OPG):      $(round.(se_opg,digits=4))\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEstimates: [-0.005, 0.481, 0.103]\nSE (sandwich): [0.0184, 0.0203, 0.0185]\nSE (Hessian):  [0.0185, 0.0203, 0.0187]\nSE (OPG):      [0.0185, 0.0202, 0.0188]\n```\n:::\n:::\n\n\nLet's verify these asymptotic standard errors against a Monte Carlo simulation. If the asymptotic theory is working, the standard deviation of the Monte Carlo estimates should be close to the asymptotic SE.\n\n::: {#fe76db55 .cell execution_count=3}\n``` {.julia .cell-code}\ngamma_mc = mapreduce(vcat, 1:500) do b\n    d = sim_data(gamma, beta0, beta1, 5000)\n    estimate_probit(d)'\nend\n\nse_mc = std.(eachcol(gamma_mc))\nprintln(\"SE (Monte Carlo): $(round.(se_mc,digits=4))\")\nprintln(\"SE (asymptotic):  $(round.(se_hessian,digits=4))\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSE (Monte Carlo): [0.0186, 0.0197, 0.0185]\nSE (asymptotic):  [0.0185, 0.0203, 0.0187]\n```\n:::\n:::\n\n\nWe can also plot the Monte Carlo distribution against the asymptotic normal approximation.\n\n::: {#8fc4d040 .cell execution_count=4}\n``` {.julia .cell-code}\npl = [begin\n    histogram(gamma_mc[:,j], normalize=:pdf, label=false, alpha=0.5)\n    xgrid = range(extrema(gamma_mc[:,j])..., length=100)\n    plot!(xgrid, pdf.(Normal(gamma[j], se_hessian[j]), xgrid),\n          linewidth=2, label=\"Asymptotic Normal\", color=\"red\")\n    plot!(title = \"γ_$(j)\")\nend for j in 1:3]\nplot(pl..., layout=(1,3), size=(900,300))\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<img src=\"data:image/png;base64,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\" />\n```\n:::\n:::\n\n\n:::\n:::\n\n### The Delta Method\n\nIn many settings, the parameters of direct interest are not $\\theta$ itself but some smooth function $g(\\theta)$. For instance, in @exm-search_likelihood the search model is parameterized by $\\theta=(h,\\delta,\\mu,\\sigma,w^{*})$ but the economically meaningful objects include $\\lambda$ and $b$, which are nonlinear functions of $\\theta$. The **delta method** provides the asymptotic distribution of such transformed parameters.\n\n:::{#thm-delta-method}\n## Delta Method\nIf $\\sqrt{N}(\\hat{\\theta}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},V)$ and $g:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}^{k}$ is continuously differentiable at $\\theta_{0}$ with Jacobian $\\nabla g(\\theta_{0})$ of full row rank, then:\n\n$$\\sqrt{N}(g(\\hat{\\theta})-g(\\theta_{0}))\\rightarrow_{d}\\mathcal{N}\\left(\\mathbf{0},\\ \\nabla g(\\theta_{0})\\ V\\ \\nabla g(\\theta_{0})'\\right)$$\n:::\n\nThe proof is a direct application of the continuous mapping theorem to the first-order Taylor expansion $g(\\hat{\\theta})\\approx g(\\theta_{0})+\\nabla g(\\theta_{0})(\\hat{\\theta}-\\theta_{0})$.\n\nIn practice, the Jacobian $\\nabla g$ can be computed analytically or by automatic differentiation. This is convenient when $g$ is a complex function --- for instance, when it involves solving the model as in the case of the reservation wage $w^{*}$ in the search model.\n\n\n## Minimum Distance Estimators\n\nThe minimum distance estimator takes a different approach from the M-estimators discussed above. Instead of directly optimizing an objective over the raw data, it works in two stages: first estimate a reduced-form object $\\pi$, then find the structural parameters $\\theta$ that best fit the model's implications for $\\pi$.\n\n### Setup\n\nLet $\\psi(\\pi,\\theta)$ be a vector of $J$ model restrictions satisfying $\\psi(\\pi_{0},\\theta_{0})=\\mathbf{0}$. For example, in the savings model from @exm-md_income, $\\pi$ consists of the variances of log income at each age, and $\\psi(\\pi,\\theta)=\\pi - \\mathbf{v}(\\theta)$ measures the gap between observed and model-implied moments.\n\nSuppose we have a first-stage estimator $\\hat{\\pi}$ with:\n$$\\sqrt{N}(\\hat{\\pi}-\\pi_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Omega)$$\n\nThe minimum distance estimator is:\n$$\\hat{\\theta} = \\arg\\min_{\\theta}\\psi(\\hat{\\pi},\\theta)'\\mathbf{W}_{N}\\psi(\\hat{\\pi},\\theta)$$\nwhere $\\mathbf{W}_{N}$ is a positive definite weighting matrix.\n\n### Asymptotic Distribution\n\n:::{#thm-md-asymptotics}\n## Asymptotics for Minimum Distance\nSuppose:\n\n1. $\\psi(\\pi_{0},\\theta_{0})=\\mathbf{0}$ and $\\psi(\\pi_{0},\\theta)\\neq\\mathbf{0}$ for all $\\theta\\neq\\theta_{0}$ (**identification**)\n2. $\\sqrt{N}(\\hat{\\pi}-\\pi_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\Omega)$\n3. $\\mathbf{W}_{N}\\rightarrow_{p}\\mathbf{W}$, symmetric and nonsingular\n4. $\\psi$ is differentiable with $\\text{rank}(\\nabla_{\\theta}\\psi_{0})=p$\n\nDefine $\\nabla_{\\theta}\\psi_{0} = \\frac{\\partial\\psi(\\pi_{0},\\theta_{0})'}{\\partial\\theta}$ and $\\nabla_{\\pi}\\psi_{0}=\\frac{\\partial\\psi(\\pi_{0},\\theta_{0})'}{\\partial\\pi}$. Then:\n\n$$\\sqrt{N}(\\hat{\\theta}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}(\\mathbf{0},\\ V_{MD})$$\nwhere:\n$$V_{MD} = \\left(\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\theta}\\psi_{0}'\\right)^{-1}\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\pi}\\psi_{0}'\\Omega\\nabla_{\\pi}\\psi_{0}\\mathbf{W}\\nabla_{\\theta}\\psi_{0}'\\left(\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\theta}\\psi_{0}'\\right)^{-1}$$\n:::\n\n:::{.callout-note icon=\"false\" collapse=\"true\"}\n## Derivation Sketch\n\nThe first-order condition of the minimum distance problem is:\n$$\\nabla_{\\theta}\\psi(\\hat{\\pi},\\hat{\\theta})\\mathbf{W}_{N}\\psi(\\hat{\\pi},\\hat{\\theta}) = \\mathbf{0}$$\n\nExpanding $\\psi(\\hat{\\pi},\\hat{\\theta})$ around $(\\pi_{0},\\theta_{0})$:\n$$\\psi(\\hat{\\pi},\\hat{\\theta})\\approx \\nabla_{\\pi}\\psi_{0}'(\\hat{\\pi}-\\pi_{0}) + \\nabla_{\\theta}\\psi_{0}'(\\hat{\\theta}-\\theta_{0})$$\n\nSubstituting and solving:\n$$\\sqrt{N}(\\hat{\\theta}-\\theta_{0})\\approx -(\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\theta}\\psi_{0}')^{-1}\\nabla_{\\theta}\\psi_{0}\\mathbf{W}\\nabla_{\\pi}\\psi_{0}'\\sqrt{N}(\\hat{\\pi}-\\pi_{0})$$\n\nThe result follows from the asymptotic distribution of $\\hat{\\pi}$.\n:::\n\n### The Optimal Weighting Matrix\n\nThe variance $V_{MD}$ depends on the choice of $\\mathbf{W}$. The **optimal** weighting matrix minimizes $V_{MD}$ (in the positive semi-definite sense) and is given by:\n\n$$\\mathbf{W}^{*} = \\left(\\nabla_{\\pi}\\psi_{0}'\\Omega\\nabla_{\\pi}\\psi_{0}\\right)^{-1}$$\n\nUnder this choice, the variance simplifies to:\n$$V_{MD}^{*} = \\left(\\nabla_{\\theta}\\psi_{0}\\left(\\nabla_{\\pi}\\psi_{0}'\\Omega\\nabla_{\\pi}\\psi_{0}\\right)^{-1}\\nabla_{\\theta}\\psi_{0}'\\right)^{-1}$$\n\nIn the common case where $\\psi(\\pi,\\theta) = \\pi-h(\\theta)$ for some function $h$, the derivatives simplify: $\\nabla_{\\pi}\\psi_{0} = I$ and $\\nabla_{\\theta}\\psi_{0} = -\\nabla_{\\theta}h(\\theta_{0})$. Then $\\mathbf{W}^{*}=\\Omega^{-1}$ and:\n\n$$V_{MD}^{*} = \\left(\\nabla_{\\theta}h_{0}\\Omega^{-1}\\nabla_{\\theta}h_{0}'\\right)^{-1}$$\n\n:::{.callout-note icon=\"false\"}\n## Efficiency of Optimal Minimum Distance\nSuppose that $\\hat{\\pi}$ are parameters estimated by maximum likelihood.  and let $\\dim(\\psi)=\\dim(\\pi)$. The implicit function theorem says:\n$$ \\nabla_\\theta \\pi = -\\nabla_\\theta\\psi (\\nabla_\\pi\\psi')^{-1} $$\n\nSo:\n\\begin{align}\n\\Sigma^* &= (\\nabla_\\theta\\psi_0(\\nabla_\\pi\\psi_0' \\mathbb{E}[s_\\pi s_\\pi']^{-1} \\nabla_\\pi \\psi_0)^{-1}\\nabla_\\theta \\psi_0')^{-1} \\\\\n        &= (\\nabla_\\theta \\psi_0\\nabla_\\pi\\psi_0'^{-1}\\mathbb{E}[s_\\pi s_\\pi'] \\nabla_\\pi \\psi_0^{-1}\\nabla_\\theta \\psi_0')^{-1} \\\\\n        &= (\\nabla_\\theta \\pi \\mathbb{E}[s_\\pi s_\\pi']\\nabla_\\theta \\pi')^{-1} \\\\\n        &= \\mathbb{E}[s_\\theta s_\\theta']^{-1} = \\mathcal{I}_\\theta\n\\end{align}\n\nThus, the optimally weighted minimum distance estimator $\\hat{\\theta}$ also obtains its respective CRLB. \n:::\n\nAn important special case arises when the model is **just-identified**: $\\text{dim}(\\psi)=\\text{dim}(\\theta)$. In this case, one can show using the implicit function theorem that the optimally weighted minimum distance estimator achieves the same asymptotic variance as MLE. Over-identification (more moments than parameters) necessarily introduces some loss relative to MLE but gains robustness.\n\n:::{.callout-note icon=\"false\" collapse=\"true\"}\n## Example: Standard Errors for the Income Process\n\n:::{#exm-md_income_se}\n\nLet's extend the minimum distance estimation from @exm-md_income to compute standard errors for the income process parameters. Recall that we matched the variance of log income at each age to the model-implied variances.\n\nSince our restrictions take the form $\\psi(\\pi,\\theta) = \\pi - \\mathbf{v}(\\theta)$, we have $\\nabla_{\\pi}\\psi=I$ and $\\nabla_{\\theta}\\psi = -\\nabla_{\\theta}\\mathbf{v}$. Using the identity weighting matrix, the asymptotic variance is:\n\n$$V_{MD} = (\\nabla_{\\theta}\\mathbf{v}'\\nabla_{\\theta}\\mathbf{v})^{-1}\\nabla_{\\theta}\\mathbf{v}'\\Omega\\nabla_{\\theta}\\mathbf{v}(\\nabla_{\\theta}\\mathbf{v}'\\nabla_{\\theta}\\mathbf{v})^{-1}$$\n\nwhere $\\Omega$ is the variance-covariance matrix of the sample variance estimates $\\hat{\\pi}$.\n\n::: {#1dded011 .cell execution_count=5}\n``` {.julia .cell-code}\nusing CSV, DataFrames, DataFramesMeta, Statistics, Optim, ForwardDiff, LinearAlgebra, Plots\n\n# Load and prepare data\ndata_psid = @chain begin\n    CSV.read(\"../data/abb_aea_data.csv\",DataFrame,missingstring = \"NA\")\n    @select :person :y :tot_assets1 :asset :age :year\n    @subset :age.>=25 :age.<=64\nend\n\n# Calculate sample variances by age\nmoments_df = @chain data_psid begin\n    groupby(:age)\n    @combine begin\n        :var_logy = var(log.(:y))\n        :n = length(:y)\n    end\n    @orderby :age\nend\nm_hat = moments_df.var_logy\nn_age = moments_df.n\n\n# Model-implied moments\nfunction model_moments(θ, T)\n    ρ, σ2_α, σ2_η = θ\n    m = [σ2_α + (1-ρ^(2(t-1)))/(1-ρ^2) * σ2_η for t in 1:T]\n    return m\nend\n\n# Transform parameters to enforce constraints\nfunction transform(x)\n    ρ = tanh(x[1])\n    σ2_α = exp(x[2])\n    σ2_η = exp(x[3])\n    return [ρ, σ2_α, σ2_η]\nend\n\n# Minimum distance objective (identity weight)\nfunction md_objective(x, m_hat)\n    θ = transform(x)\n    T = length(m_hat)\n    m_model = model_moments(θ, T)\n    diff = m_hat .- m_model\n    return diff' * diff\nend\n\n# Estimate\nx0 = [0.5, log(0.1), log(0.05)]\nres = optimize(x -> md_objective(x, m_hat), x0, Newton(), autodiff = :forward)\nx_hat = res.minimizer\nθ_hat = transform(x_hat)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n3-element Vector{Float64}:\n 0.9180604516854505\n 0.27854304267515606\n 0.08522314351469629\n```\n:::\n:::\n\n\nNow we compute standard errors. We need $\\nabla_{\\theta}\\mathbf{v}$ (the Jacobian of model moments with respect to parameters) and $\\Omega$ (the variance of sample moments). We compute the Jacobian with `ForwardDiff` and use the delta method to account for the parameter transformation.\n\n::: {#53f0d125 .cell execution_count=6}\n``` {.julia .cell-code}\nT = length(m_hat)\n\n# Jacobian of model moments w.r.t. θ = (ρ, σ²_α, σ²_η)\n∇v = ForwardDiff.jacobian(θ -> model_moments(θ, T), θ_hat)\n\n# Jacobian of transform (for delta method through the transformation)\n∇t = ForwardDiff.jacobian(transform, x_hat)\n\n# Estimate Ω: for variances, Var(σ̂²) ≈ 2σ⁴/(n-1) under normality\n# A simple approximation using sample sizes at each age\nΩ = Diagonal(2 .* m_hat.^2 ./ (n_age .- 1))\n\n# Total sample size (use average n per age as approximation)\nN_eff = Int(round(mean(n_age)))\n\n# Asymptotic variance of θ̂ (identity weighting)\nG = ∇v  # J × p Jacobian\nV_md = inv(G' * G) * G' * Ω * G * inv(G' * G) / N_eff\n\n# Standard errors\nse = sqrt.(diag(V_md))\n\nprintln(\"Minimum Distance Estimates with Standard Errors:\")\nprintln(\"  ρ     = $(round(θ_hat[1], digits=3))  ($(round(se[1], digits=4)))\")\nprintln(\"  σ²_α  = $(round(θ_hat[2], digits=3))  ($(round(se[2], digits=4)))\")\nprintln(\"  σ²_η  = $(round(θ_hat[3], digits=3))  ($(round(se[3], digits=4)))\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMinimum Distance Estimates with Standard Errors:\n  ρ     = 0.918  (0.0005)\n  σ²_α  = 0.279  (0.0009)\n  σ²_η  = 0.085  (0.0005)\n```\n:::\n:::\n\n\nWe can also compute the standard errors under the *optimal* weighting matrix and compare:\n\n::: {#12674064 .cell execution_count=7}\n``` {.julia .cell-code}\n# Optimal weighting\nW_opt = inv(Ω)\nV_md_opt = inv(G' * W_opt * G) / N_eff\nse_opt = sqrt.(diag(V_md_opt))\n\n# Re-estimate with optimal weighting\nfunction md_objective_opt(x, m_hat, W)\n    θ = transform(x)\n    T = length(m_hat)\n    m_model = model_moments(θ, T)\n    diff = m_hat .- m_model\n    return diff' * W * diff\nend\n\nres_opt = optimize(x -> md_objective_opt(x, m_hat, W_opt), x0, Newton(), autodiff = :forward)\nθ_hat_opt = transform(res_opt.minimizer)\n\n# Recompute Jacobian at new estimates\n∇v_opt = ForwardDiff.jacobian(θ -> model_moments(θ, T), θ_hat_opt)\nV_md_opt2 = inv(∇v_opt' * W_opt * ∇v_opt) / N_eff\nse_opt2 = sqrt.(diag(V_md_opt2))\n\nprintln(\"\\nOptimally Weighted Estimates:\")\nprintln(\"  ρ     = $(round(θ_hat_opt[1], digits=3))  ($(round(se_opt2[1], digits=4)))\")\nprintln(\"  σ²_α  = $(round(θ_hat_opt[2], digits=3))  ($(round(se_opt2[2], digits=4)))\")\nprintln(\"  σ²_η  = $(round(θ_hat_opt[3], digits=3))  ($(round(se_opt2[3], digits=4)))\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nOptimally Weighted Estimates:\n  ρ     = 0.922  (0.0004)\n  σ²_α  = 0.271  (0.0008)\n  σ²_η  = 0.062  (0.0003)\n```\n:::\n:::\n\n\nThe optimally weighted estimator should be at least as precise, and is often substantially more so when the moment conditions have very different scales or variances.\n\n:::\n:::\n\n:::{.callout-important icon=\"false\"}\n## Estimating the Variance of Moments\n\nNote that in this example we estimated the variance of the moments assuming the model-implied assumption that the errors take a normal distribution. We also assumed that the off-diagonal members of the matrix (the covariances) were zero. This assumption is **explicitly wrong** since the *same individuals appear at different ages in the sample*. How can we properly calculate these variances and covariances without strict (and wrong) assumptions? The [next chapter](simulation-methods.qmd) will illustrate how using a **block bootstrap** routine.\n\n:::\n\n\n## The Generalized Method of Moments\n\nGMM is an extremum estimator with objective:\n$$Q_{N}(\\theta) = -\\frac{1}{2}\\mathbf{g}_{N}(\\theta)'\\mathbf{W}_{N}\\mathbf{g}_{N}(\\theta),\\qquad\\mathbf{g}_{N}(\\theta)=\\frac{1}{N}\\sum_{n}g(\\mathbf{w}_{n},\\theta)$$\n\nwhere $\\mathbb{E}[g(\\mathbf{w},\\theta_{0})]=\\mathbf{0}$ are the moment conditions. The asymptotic distribution follows from @thm-asymptotic-normality as a special case, but the structure of the problem leads to a particularly clean expression.\n\n:::{#thm-gmm-asymptotics}\n## Asymptotic Distribution of GMM\nSuppose that the standard regularity conditions hold and $\\mathbf{W}_{N}\\rightarrow_{p}\\mathbf{W}$. Let $G=\\mathbb{E}[\\nabla_{\\theta}g(\\mathbf{w},\\theta_{0})']$ and $S=\\mathbb{E}[g(\\mathbf{w},\\theta_{0})g(\\mathbf{w},\\theta_{0})']$. Then:\n\n$$\\sqrt{N}(\\hat{\\theta}_{GMM}-\\theta_{0})\\rightarrow_{d}\\mathcal{N}\\left(\\mathbf{0},\\ (G'\\mathbf{W}G)^{-1}G'\\mathbf{W}S\\mathbf{W}G(G'\\mathbf{W}G)^{-1}\\right)$$\n:::\n\n### The Optimal Weighting Matrix\n\nAs with minimum distance, the asymptotic variance depends on the choice of $\\mathbf{W}$. The optimal weighting matrix is:\n\n$$\\mathbf{W}^{*}=S^{-1} = \\left(\\mathbb{E}[g(\\mathbf{w},\\theta_{0})g(\\mathbf{w},\\theta_{0})']\\right)^{-1}$$\n\nUnder this choice, the variance simplifies to:\n\n$$V_{GMM}^{*} = (G'S^{-1}G)^{-1}$$\n\nWhen the model is **just-identified** (number of moments equals number of parameters), the GMM estimator does not depend on $\\mathbf{W}$ at all. This is because the sample moments $\\mathbf{g}_{N}(\\hat{\\theta})=\\mathbf{0}$ are set exactly to zero, regardless of the weighting.\n\n### Feasible Efficient GMM\n\nIn practice, $S$ depends on $\\theta_{0}$ and must be estimated. A common approach is **two-step GMM**:\n\n1. Estimate $\\hat{\\theta}_{1}$ using some initial weighting matrix (e.g. $\\mathbf{W}=I$).\n2. Compute $\\hat{S}=\\frac{1}{N}\\sum_{n}g(\\mathbf{w}_{n},\\hat{\\theta}_{1})g(\\mathbf{w}_{n},\\hat{\\theta}_{1})'$.\n3. Re-estimate: $\\hat{\\theta}_{2} = \\arg\\min_{\\theta}\\mathbf{g}_{N}(\\theta)'\\hat{S}^{-1}\\mathbf{g}_{N}(\\theta)$.\n\nThe resulting estimator $\\hat{\\theta}_{2}$ is asymptotically efficient. The first-stage estimation of $\\hat{S}$ does not affect the asymptotic variance because $\\hat{S}\\rightarrow_{p}S$ under standard conditions, and the weighting matrix appears in the asymptotic variance only through its probability limit.\n\n\n## Efficiency\n\nGiven two consistent, asymptotically normal estimators $\\hat{\\theta}_{1}$ and $\\hat{\\theta}_{2}$, we say $\\hat{\\theta}_{1}$ is **asymptotically efficient** relative to $\\hat{\\theta}_{2}$ if $V_{2}-V_{1}$ is positive semi-definite, where $V_{j}$ is the asymptotic variance of $\\hat{\\theta}_{j}$. A natural question is: among the class of estimators we have discussed, is there a \"best\" one?\n\n### Efficiency of Maximum Likelihood\n\nThe answer is yes, under the assumption that the model is correctly specified. The MLE achieves the **Cramér-Rao lower bound**: for any consistent, asymptotically normal estimator $\\hat{\\theta}$ based on the likelihood,\n\n$$V[\\hat{\\theta}] - \\mathcal{I}(\\theta_{0})^{-1}\\geq 0$$\n\nin the positive semi-definite sense. Since the MLE has asymptotic variance $\\mathcal{I}(\\theta_{0})^{-1}$, no other estimator in this class can do better.\n\nMore concretely, one can show that MLE is efficient relative to *any* GMM estimator that uses moment conditions implied by the model. The argument proceeds by showing that for any GMM estimator with moments $g(\\mathbf{w},\\theta)$, the difference in asymptotic variances is:\n\n$$V_{GMM} - V_{MLE} = \\mathbb{E}[\\mathbf{m}\\mathbf{s}']^{-1}\\mathbb{E}[\\mathbf{U}\\mathbf{U}']\\mathbb{E}[\\mathbf{s}\\mathbf{m}']^{-1}\\geq 0$$\n\nwhere $\\mathbf{m}$ is the influence function of the GMM estimator and $\\mathbf{U} = \\mathbf{m}-\\mathbb{E}[\\mathbf{m}\\mathbf{s}']\\mathbb{E}[\\mathbf{s}\\mathbf{s}']^{-1}\\mathbf{s}$ is the projection residual of $\\mathbf{m}$ on $\\mathbf{s}$. This is non-negative by construction, and equals zero only when $\\mathbf{m}$ is a linear function of $\\mathbf{s}$ --- i.e. when the GMM estimator fully exploits the likelihood.\n\n:::{.callout-warning icon=\"false\"}\n## Discussion: Efficiency vs. Robustness\n\nThe efficiency of MLE comes at a price: it requires the entire parametric model to be correctly specified. If the density $f(\\mathbf{w};\\theta)$ is wrong --- even slightly --- the MLE may still converge, but it will converge to a *pseudo-true* value that minimizes the Kullback-Leibler divergence to the truth, and the information matrix equality will fail. In contrast, GMM only requires the *moment conditions* to be correct, making it more robust to partial misspecification. The sandwich variance estimator $\\hat{H}^{-1}\\hat{\\Sigma}\\hat{H}^{-1}$ remains valid for MLE even under misspecification, which is why it is sometimes preferred in practice.\n\n:::\n\n:::{.callout-warning icon=\"false\"}\n## Discussion: Pros and Cons of MLE\n\nThe single greatest strength of MLE is that it **uses every single piece of information in the data**. If you know that your model is identified by the population distribution, you do not have to think about which features of the data to match in order for estimation to work well in practice. This is particularly useful when you want to deal with **unobserved heterogeneity** in the model, as we will see later on. MLE will extract the most amount of information from panel data and use it optimally.\n\nThe greatest weakness of MLE is that it **uses every single piece of information in the data**. You do not get to control which features of the data the model fits and which features it misses. The likelihood will decide that. Given that you know your model is wrong, and it is not designed to fit everything perfectly well, MLE takes away your ability to govern this tradeoff. Often, as a result, models that are estimated by MLE in practice often feature. This relates to the **whether vs how** of identification. The way you map the data to the model *most credibly* (say, by fitting how the data respond to plausibly exogenous variation) may not coincide with the MLE estimator.\n\n:::\n\n\n## Two-Step Estimators\n\nMany structural estimators proceed in stages. In @exm-roy_estimation, we first estimated the selection equation by MLE and then used these estimates in a second-stage OLS regression. In the search model, we might first estimate the wage distribution and then back out the reservation wage. The theory of *two-step estimators* formalizes the effect of first-stage estimation uncertainty on second-stage inference.\n\n### Setup\n\nSuppose the estimator is defined by two sets of moment conditions:\n\n1. **First step**: Estimate $\\hat{\\gamma}$ via $\\frac{1}{N}\\sum_{n}g_{1}(\\mathbf{w}_{n},\\hat{\\gamma})=\\mathbf{0}$\n2. **Second step**: Estimate $\\hat{\\beta}$ via $\\frac{1}{N}\\sum_{n}g_{2}(\\mathbf{w}_{n},\\hat{\\gamma},\\hat{\\beta})=\\mathbf{0}$\n\nThe key feature is that the second step depends on the first-step estimates.\n\n### Asymptotic Distribution\n\nTo derive the joint distribution, stack the moment conditions. Let $\\alpha=(\\gamma',\\beta')'$ and write the full system as:\n$$\\frac{1}{N}\\sum_{n}\\begin{bmatrix}g_{1}(\\mathbf{w}_{n},\\gamma)\\\\ g_{2}(\\mathbf{w}_{n},\\gamma,\\beta)\\end{bmatrix} = \\mathbf{0}$$\n\nThe Jacobian of this system has a **block-triangular** structure:\n$$\\Gamma = \\begin{bmatrix}\\Gamma_{1\\gamma} & 0 \\\\ \\Gamma_{2\\gamma} & \\Gamma_{2\\beta}\\end{bmatrix}$$\n\nwhere $\\Gamma_{1\\gamma}=\\mathbb{E}[\\nabla_{\\gamma}g_{1}']$, $\\Gamma_{2\\gamma}=\\mathbb{E}[\\nabla_{\\gamma}g_{2}']$, and $\\Gamma_{2\\beta}=\\mathbb{E}[\\nabla_{\\beta}g_{2}']$. The zero in the upper-right block reflects the fact that the first step does not depend on $\\beta$.\n\nApplying the standard GMM formula, the asymptotic variance of $\\hat{\\beta}$ is:\n\n$$V_{\\beta} = \\Gamma_{2\\beta}^{-1}\\mathbb{E}[(g_{2}-\\Gamma_{2\\gamma}\\Gamma_{1\\gamma}^{-1}g_{1})(g_{2}-\\Gamma_{2\\gamma}\\Gamma_{1\\gamma}^{-1}g_{1})']\\Gamma_{2\\beta}^{-1\\prime}$$\n\nThe term $\\Gamma_{2\\gamma}\\Gamma_{1\\gamma}^{-1}g_{1}$ captures the *correction* for first-stage estimation error. If we ignored this term and computed standard errors using only the second-stage moment conditions, we would generally get incorrect inference.\n\n:::{.callout-important}\n## When Does the First Stage Not Matter?\n\nThere is an important special case: if $\\Gamma_{2\\gamma}=\\mathbb{E}[\\nabla_{\\gamma}g_{2}']=0$, then the correction vanishes and the first-stage estimation has no effect on the second-stage asymptotic variance. Intuitively, this happens when the second-step moment conditions are *locally insensitive* to the first-step parameters at the true values.\n\nA classic example is the **two-stage IV** estimator where the first stage is a probit. Here, the second-stage moment condition takes the form $\\mathbb{E}[\\Phi(\\mathbf{x}'\\gamma_{0})\\cdot u]=0$. One can show that $\\mathbb{E}[\\nabla_{\\gamma}g_{2}']=0$ because the projection of $u$ onto functions of the instruments is zero by construction.\n:::\n\n<!-- :::{.callout-note icon=\"false\" collapse=\"true\"}\n## Example: Two-Step Roy Model with Standard Errors\n:::{#exm-two_step_roy_se}\n\nLet's revisit the two-step estimator for the Generalized Roy Model, now accounting for first-stage estimation uncertainty. In the first step we estimate the probit, and in the second step we run the selection-corrected OLS regression.\n\nThe second-step regression for the treated group ($D=1$) is:\n$$Y_{n} = \\beta_{1,0} + \\beta_{1,1}X_{n} + \\beta_{1,2}\\underbrace{\\frac{\\phi(\\mathbf{w}_{n}'\\gamma)}{\\Phi(\\mathbf{w}_{n}'\\gamma)}}_{\\lambda_{n}(\\gamma)} + \\varepsilon_{n}$$\n\nwhere $\\mathbf{w}_{n}=[1,X_{n},Z_{n}]'$. To apply the two-step variance formula, we write the stacked moment conditions over *all* $N$ observations:\n$$g_{1,n}(\\gamma) = \\text{score of probit log-likelihood for observation } n$$\n$$g_{2,n}(\\gamma,\\beta) = D_{n}\\cdot\\mathbf{x}_{n}(\\gamma)\\left(Y_{n} - \\mathbf{x}_{n}(\\gamma)'\\beta_{1}\\right)$$\nwhere $\\mathbf{x}_{n}(\\gamma)=[1,X_{n},\\lambda_{n}(\\gamma)]'$. Note that $g_{2,n}=\\mathbf{0}$ for untreated observations ($D_{n}=0$), but they still contribute to the corrected variance through the first-stage score $g_{1,n}$.\n\nThe corrected asymptotic variance of $\\hat{\\beta}_{1}$ is:\n$$\\text{Avar}(\\hat{\\beta}) = \\Gamma_{2\\beta}^{-1}\\mathbb{E}\\left[\\tilde{g}_{2,n}\\tilde{g}_{2,n}'\\right]\\Gamma_{2\\beta}^{-1\\prime}$$\nwhere $\\tilde{g}_{2,n} = g_{2,n} - \\Gamma_{2\\gamma}\\Gamma_{1\\gamma}^{-1}g_{1,n}$ and the expectation is over *all* observations (treated and untreated). Rather than computing the individual blocks of $\\Gamma$ by hand, we compute the full Jacobian of the stacked system $G_{n}(\\alpha)=[g_{1,n};g_{2,n}]$ (where $\\alpha=(\\gamma',\\beta')'$) using `ForwardDiff`, apply the sandwich formula to the full $6\\times 6$ system, and extract the $\\hat{\\beta}$ block.\n\n::: {#ea908bfe .cell execution_count=8}\n``` {.julia .cell-code}\n# Reuse sim_data and estimate_probit from above\n\nfunction two_step_with_se(data)\n    (;X,Z,Y,D) = data\n    N = length(D)\n\n    # === Step 1: Probit MLE ===\n    γ_hat = estimate_probit(data)\n\n    # === Step 2: Selection-corrected OLS for treated ===\n    treated = findall(D .== 1)\n\n    function build_x(γ, n)\n        xg = γ[1] + γ[2]*X[n] + γ[3]*Z[n]\n        λ = pdf(Normal(), xg) / cdf(Normal(), xg)\n        return [one(eltype(γ)), X[n], λ]\n    end\n\n    X1 = reduce(hcat, [build_x(γ_hat, n) for n in treated])'\n    Y1 = Y[treated]\n    β_hat = X1 \\ Y1\n\n    # === Stacked moment: Gₙ(α) = [g₁ₙ(γ); g₂ₙ(γ,β)] where α = [γ;β] ===\n    function G_n(α, n)\n        γ = α[1:3]; β = α[4:6]\n        xg = γ[1] + γ[2]*X[n] + γ[3]*Z[n]\n        ϕ = pdf(Normal(), xg)\n        Φ = cdf(Normal(), xg)\n        w = [one(eltype(α)), X[n], Z[n]]\n        # g₁: probit score\n        g1 = D[n] == 1 ? (ϕ / Φ) .* w : (-ϕ / (1 - Φ)) .* w\n        # g₂: selection-corrected OLS moment (zero for untreated)\n        if D[n] == 1\n            x = [one(eltype(α)), X[n], ϕ / Φ]\n            g2 = x .* (Y[n] - dot(x, β))\n        else\n            g2 = zeros(eltype(α), 3)\n        end\n        return vcat(g1, g2)\n    end\n\n    α_hat = vcat(γ_hat, β_hat)\n\n    # Jacobian of average stacked moment\n    avg_G(α) = sum(n -> G_n(α, n), 1:N) / N\n    Γ = ForwardDiff.jacobian(avg_G, α_hat)\n\n    # Meat: Σ = (1/N) Σₙ Gₙ Gₙ'\n    Σ = zeros(6, 6)\n    for n in 1:N\n        G = G_n(α_hat, n)\n        Σ += G * G'\n    end\n    Σ ./= N\n\n    # Full sandwich: V(α̂) = Γ⁻¹ Σ Γ⁻¹' / N\n    V_full = inv(Γ) * Σ * inv(Γ)' / N\n    se_corrected = sqrt.(diag(V_full[4:6, 4:6]))\n\n    # Naive SEs (HC0 OLS, ignoring first-stage uncertainty)\n    resid = Y1 .- X1 * β_hat\n    V_naive = inv(X1'X1) * (X1' * Diagonal(resid.^2) * X1) * inv(X1'X1)\n    se_naive = sqrt.(diag(V_naive))\n\n    return (;γ_hat, β_hat, se_naive, se_corrected)\nend\n\ngamma = [0., 0.5, 0.1]\nbeta0 = [0., 0.3]\nbeta1 = [0., 0.5]\nRandom.seed!(42)\ndata = sim_data(gamma, beta0, beta1, 500; ρ_1 = -0.8)\nresults = two_step_with_se(data)\nprintln(\"Second-stage estimates: $(round.(results.β_hat, digits=3))\")\nprintln(\"SE (naive):     $(round.(results.se_naive, digits=4))\")\nprintln(\"SE (corrected): $(round.(results.se_corrected, digits=4))\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSecond-stage estimates: [0.381, 0.494, 0.397]\nSE (naive):     [1.0196, 0.3468, 1.2154]\nSE (corrected): [1.267, 0.4286, 1.5135]\n```\n:::\n:::\n\n\nLet's verify the corrected standard errors against a Monte Carlo simulation.\n\n::: {#8b7cc25c .cell execution_count=9}\n``` {.julia .cell-code}\nN_mc = 500\nests = mapreduce(vcat, 1:500) do b\n    d = sim_data(gamma, beta0, beta1, N_mc; ρ_1 = -0.8)\n    γ_hat = estimate_probit(d)\n    W = hcat(ones(N_mc), d.X, d.Z)\n    idx = W * γ_hat\n    corr = pdf.(Normal(), idx) ./ cdf.(Normal(), idx)\n    Y1 = d.Y[d.D .== 1]\n    X1 = hcat(ones(sum(d.D)), d.X[d.D .== 1], corr[d.D .== 1])\n    β = X1 \\ Y1\n    return β'\nend\n\nse_mc = std.(eachcol(ests))\n\n# Also compute corrected SEs at N = N_mc for a fair comparison\nRandom.seed!(42)\ndata_mc = sim_data(gamma, beta0, beta1, N_mc; ρ_1 = -0.8)\nresults_mc = two_step_with_se(data_mc)\n\nprintln(\"SE (Monte Carlo):  $(round.(se_mc, digits=4))\")\nprintln(\"SE (corrected):    $(round.(results_mc.se_corrected, digits=4))\")\nprintln(\"SE (naive):        $(round.(results_mc.se_naive, digits=4))\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSE (Monte Carlo):  [1.0113, 0.3574, 1.2381]\nSE (corrected):    [1.267, 0.4286, 1.5135]\nSE (naive):        [1.0196, 0.3468, 1.2154]\n```\n:::\n:::\n\n\nThe corrected standard errors should be closer to the Monte Carlo standard deviations than the naive ones. The stacked-moment approach with `ForwardDiff` avoids error-prone manual computation of individual Jacobian blocks --- the same approach works for any two-step estimator.\n:::\n::: -->\n\n:::{.callout-warning icon=\"false\"}\n## Discussion: Bootstrap vs. Analytical Standard Errors\n\nComputing analytical standard errors for two-step estimators can be tedious, as the example above illustrates. An attractive alternative is the **bootstrap**: resample the data, re-run the entire two-step procedure on each bootstrap sample, and compute standard errors from the distribution of bootstrap estimates. This automatically accounts for first-stage estimation uncertainty without requiring explicit computation of correction terms. We will discuss the bootstrap formally in the chapter on [simulation methods](simulation-methods.qmd).\n\n:::\n\n## Exercises\n\n:::{.callout-tip icon=\"false\"}\n## Exercise\n:::{#exr-search_se}\nExtend @exm-search_likelihood to:\n\n1. Compute asymptotic standard errors for the estimated parameters $\\hat{\\theta}=(h,\\delta,\\mu,\\sigma,w^{*})$ using the information matrix.\n2. Use the delta method to compute standard errors for the derived estimates $\\hat{\\lambda}$ and $\\hat{b}$.\n3. Estimate the search model separately for men with and without a bachelor's degree.\n4. Report and compare your estimates. Are the differences across education groups economically meaningful? What does the model imply about the sources of wage differences between these groups?\n\n:::\n:::\n\n\n:::{.callout-tip icon=\"false\"}\n## Exercise\n:::{#exr-bpp}\n@BPP2008 estimate income and consumption dynamics using a model where:\n$$\\Delta y_{n,t} = \\zeta_{n,t} + \\Delta\\xi_{n,t}$$\nwhere $\\zeta_{n,t}$ is a permanent shock and $\\xi_{n,t}$ is a transitory shock. Consumption responds differently to each:\n$$\\Delta c_{n,t} = \\phi\\zeta_{n,t} + \\psi\\xi_{n,t} + \\epsilon_{n,t}$$\nwhere $\\phi$ and $\\psi$ capture the \"insurance coefficients\" --- the degree to which consumption responds to permanent and transitory income shocks, respectively.\n\n1. Show that from the second moments of $(\\Delta y, \\Delta c)$, one can identify $\\sigma^{2}_{\\zeta}$, $\\sigma^{2}_{\\xi}$, $\\phi$, and $\\psi$. *Hint*: consider $\\mathbb{V}[\\Delta y_{t}]$, $\\mathbb{C}(\\Delta y_{t},\\Delta y_{t-1})$, $\\mathbb{C}(\\Delta c_{t},\\Delta y_{t})$, and $\\mathbb{C}(\\Delta c_{t},\\Delta y_{t-1})$.\n2. Using the data from @exm-psid, implement a minimum distance estimator for $(\\sigma^{2}_{\\zeta},\\sigma^{2}_{\\xi},\\phi,\\psi)$ and report standard errors.\n3. What do the estimates of $\\phi$ and $\\psi$ tell us about how well households insure against permanent versus transitory income shocks?\n\n:::\n:::\n\n\n:::{.callout-tip icon=\"false\"}\n## Exercise\n:::{#exr-entry_exit_md}\nConsider estimation of the entry-exit model by minimum distance, as described in the [introduction](extremum_intro_examples.qmd). Recall that for each state $(x,a,a')$, the model implies a choice probability $p(x,a,a';\\phi,\\beta)$ that depends on the payoff parameters through the equilibrium of the game. Data consist of entry decisions across many independent markets.\n\n1. Simulate data from the entry-exit model for $M=500$ markets at 5 equally spaced values of $x$ and use the empirical choice frequencies as your target moments $\\hat{\\mathbf{p}}$.\n2. Implement the minimum distance estimator:\n$$\\hat{\\phi} = \\arg\\min_{\\phi}(\\hat{\\mathbf{p}}-\\mathbf{p}(\\phi,\\beta))'\\mathbf{W}(\\hat{\\mathbf{p}}-\\mathbf{p}(\\phi,\\beta))$$\nusing the identity weighting matrix. *Note*: for each candidate $\\phi$, you must solve for the equilibrium to compute $\\mathbf{p}(\\phi,\\beta)$.\n3. Compute standard errors for $\\hat{\\phi}$ using the minimum distance variance formula from @thm-md-asymptotics.\n4. Re-estimate with the optimal weighting matrix and compare your standard errors.\n\n:::\n:::\n\n",
    "supporting": [
      "extremum_theory_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}